@article{Kra,
abstract = {null},
annote = {null
T�picos: Distributed system
T�picos: research article
T�picos: other testing models
T�picos: VISOR tool},
author = {Kra, M Furkan and Szer, Hasan and Aktemur, Bar},
journal = {Journal of Systems and Software},
title = {{VISOR}}
}
@article{Aichernig2017,
abstract = {We propose a framework for requirement-driven test generation that combines contract-based interface theories with model-based testing. We design a specification language, requirement interfaces, for formalizing different views (aspects) of synchronous data-flow systems from informal requirements. Various views of a system, modeled as requirement interfaces, are naturally combined by conjunction. We develop an incremental test generation procedure with several advantages. The test generation is driven by a single requirement interface at a time. It follows that each test assesses a specific aspect or feature of the system, specified by its associated requirement interface. Since we do not explicitly compute the conjunction of all requirement interfaces of the system, we avoid state space explosion while generating tests. However, we incrementally complete a test for a specific feature with the constraints defined by other requirement interfaces. This allows catching violations of any other requirement during test execution, and not only of the one used to generate the test. This framework defines a natural association between informal requirements, their formal specifications, and the generated tests, thus facilitating traceability. Finally, we introduce a fault-based test-case generation technique, called model-based mutation testing, to requirement interfaces. It generates a test suite that covers a set of fault models, guaranteeing the detection of any corresponding faults in deterministic systems under test. We implemented a prototype test generation tool and demonstrate its applicability in two industrial use cases.},
annote = {model-based testing
contract-based interface
test case generation
model-based mutation testing
fault-based test-case generation technique
framework
requirement-driven test generation
research article
no type of testing specified
no type of system specified



Proponemos un marco para la generaci{\'{o}}n de pruebas basada en requisitos que combina teor{\'{i}}as de interfaz basadas en contratos con pruebas basadas en modelos. Dise{\~{n}}amos un lenguaje de especificaci{\'{o}}n, interfaces de requisitos, para formalizar diferentes vistas (aspectos) de sistemas de flujo de datos s{\'{i}}ncronos a partir de requisitos informales. Varias vistas de un sistema, modeladas como interfaces de requisitos, se combinan naturalmente por conjunci{\'{o}}n. Desarrollamos un procedimiento de generaci{\'{o}}n de pruebas incremental con varias ventajas. La generaci{\'{o}}n de prueba es impulsada por una sola interfaz de requisito a la vez. De ello se deduce que cada prueba eval{\'{u}}a un aspecto o caracter{\'{i}}stica espec{\'{i}}fico del sistema, especificado por su interfaz de requisitos asociada. Puesto que no calculamos expl{\'{i}}citamente la conjunci{\'{o}}n de todas las interfaces de requisitos del sistema, evitamos la explosi{\'{o}}n del espacio de estado al generar pruebas. Sin embargo, completamos incrementalmente una prueba para una caracter{\'{i}}stica espec{\'{i}}fica con las restricciones definidas por otras interfaces de requisitos. Esto permite detectar infracciones de cualquier otro requisito durante la ejecuci{\'{o}}n de la prueba, y no solo del que se usa para generar la prueba. Este marco define una asociaci{\'{o}}n natural entre los requisitos informales, sus especificaciones formales y las pruebas generadas, facilitando as{\'{i}} la trazabilidad. Por {\'{u}}ltimo, introducimos una t{\'{e}}cnica de generaci{\'{o}}n de casos de prueba basada en errores, denominada pruebas de mutaci{\'{o}}n basadas en modelos, para las interfaces de requisitos. Genera un conjunto de pruebas que cubre un conjunto de modelos de fallas, garantizando la detecci{\'{o}}n de cualquier fallo correspondiente en sistemas deterministas bajo prueba. Implementamos una herramienta de generaci{\'{o}}n de pruebas prototipo y demostramos su aplicabilidad en dos casos de uso industrial.
T�picos: research article
T�picos: Distributed system
T�picos: mutation testing
T�picos: model-based testing},
author = {Aichernig, Bernhard K. and H{\"{o}}rmaier, Klaus and Lorber, Florian and Ni{\v{c}}kovi{\'{c}}, Dejan and Tiran, Stefan},
doi = {10.1007/s10009-016-0444-z},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Consistency checking,Formal specification,Incremental test-case generation,Model-based mutation testing,Model-based testing,Requirement interfaces,Requirements engineering,Synchronous systems,Test-case generation,Traceability},
number = {4},
pages = {409--426},
title = {{Require, test, and trace IT}},
volume = {19},
year = {2017}
}
@article{Bulej2017,
abstract = {Unit testing is an attractive quality management tool in the software development process, however, practical obstacles make it difficult to use unit tests for performance testing. We present Stochastic Performance Logic, a formalism for expressing performance requirements, together with interpretations that facilitate performance evaluation in the unit test context. The formalism and the interpretations are implemented in a performance testing framework and evaluated in multiple experiments, demonstrating the ability to identify performance differences in realistic unit test scenarios.},
annote = {unit testing
performance testing
unit performance testing
stochastic performance logic
java
no model of testing specified
no type of testing specified
research article


Las pruebas unitarias son una herramienta de gesti{\'{o}}n de calidad atractiva en el proceso de desarrollo de software, sin embargo, los obst{\'{a}}culos pr{\'{a}}cticos dificultan el uso de pruebas unitarias para las pruebas de rendimiento. Presentamos Stochastic Performance Logic, un formalismo para expresar los requisitos de rendimiento, junto con interpretaciones que facilitan la evaluaci{\'{o}}n del rendimiento en el contexto de la prueba unitaria. El formalismo y las interpretaciones se implementan en un marco de pruebas de rendimiento y se eval{\'{u}}an en varios experimentos, lo que demuestra la capacidad de identificar diferencias de rendimiento en escenarios de pruebas unitarias realistas.
T�picos: research article
T�picos: Distributed system
T�picos: non functional testing
T�picos: functional testing
T�picos: other testing models},
author = {Bulej, Lubom{\'{i}}r and Bure{\v{s}}, Tom{\'{a}}{\v{s}} and Hork{\'{y}}, Vojt{\v{e}}ch and Kotr{\v{c}}, Jaroslav and Marek, Luk{\'{a}}{\v{s}} and Troj{\'{a}}nek, Tom{\'{a}}{\v{s}} and Tůma, Petr},
doi = {10.1007/s10515-015-0188-0},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {Java,Performance evaluation,Unit testing},
number = {1},
pages = {139--187},
title = {{Unit testing performance with Stochastic Performance Logic}},
volume = {24},
year = {2017}
}
@article{Esfandyari2015,
author = {Esfandyari, Azadeh},
keywords = {2,of web service,overview and a classification,web service security testing,wsdl},
number = {4},
pages = {46--50},
title = {{A comparative study and classification on web service security testing approaches}},
volume = {4},
year = {2015}
}
@article{Aiguier2012,
abstract = {The contribution of this paper is twofold: first, it defines a unified framework for modelling abstract components, as well as a formalization of integration rules to combine their behaviour. This is based on a coalgebraic definition of components, which is a categorical representation allowing the unification of a large family of formalisms for specifying state-based systems. Second, it studies compositional conformance testing i.e. checking whether an implementation made of correct interacting components combined with integration operators conforms to its specification. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Aiguier, Marc and Boulanger, Fr{\'{e}}d{\'{e}}ric and Kanso, Bilal},
doi = {10.1016/j.tcs.2011.12.072},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Coalgebra,Component based system,Compositional testing,Conformance testing,Integration operators,Monad,Trace semantics,Transfer function},
pages = {66--97},
publisher = {Elsevier},
title = {{A formal abstract framework for modelling and testing complex software systems}},
volume = {455},
year = {2012}
}
@article{Ni2016,
abstract = {Risk assessment at the early stage of software development can effectively reduce potential security flaws in the software, thus reduce the cost of testing and maintenance. However, there are very few standardized risk assessment methods toward the design models of security-critical RTESs (real-time embedded systems). This paper defines a formal model called OMR (Object-Message-Role) using Z notation for the security-critical RTESs. Comparing with the existing models for RTESs, OMR is able to specify both the functional and security aspects of the system as an integrated model, which directly provides the input for risk assessment. A risk assessment method RAMES (risk assessment method for embedded systems) based on OMR is then proposed. RAMES is complianced with the risk management process standardized by ISO 31000. To perform the risk analysis in RAMES, an algorithm RAOMR is designed based on the analysis of the message flows and security constraints in OMR. The illustration of a case study shows that RAMES is able to evaluate the risk level of the system model, and locate the high-risky objects and messages.},
author = {Ni, Siru and Zhuang, Yi and Gu, Jingjing and Huo, Ying},
doi = {10.1016/j.cose.2016.01.005},
issn = {01674048},
journal = {Computers and Security},
keywords = {Formal method,Real-time embedded systems,Risk assessment,Security,Z notation},
pages = {199--215},
publisher = {Elsevier},
title = {{A formal model and risk assessment method for security-critical real-time embedded systems}},
volume = {58},
year = {2016}
}
@article{Jabbar2020,
abstract = {The emergence of embedded and connected smart technologies, systems, and devices has enabled the concept of smart cities by connecting every "thing" to the Internet and in particular in transportation through the Internet of Vehicles (IoV). The main purpose of IoV is to prevent fatal crashes by resolving traffic and road safety problems. Nevertheless, it is paramount to ensure secure and accurate transmission and recording of data in "Vehicle-to-Vehicle" (V2V) and "Vehicle-to-Infrastructure" (V2I) communication. To improve "Vehicle-to- Everything" (V2X) communication, this work uses Blockchain technology for developing a Blockchain-based IoT system aimed at establishing secure communication and developing a fully decentralized cloud computing platform. Moreover, the authors propose a model-based framework to validate the proposed approach. This framework is mainly based on the use of the Attack Trees (AT) and timed automaton (TA) formalisms in order to test the functional, load and security aspects. An optimization phase for testers placement inspired by fog computing is proposed as well.},
author = {Jabbar, Rateb and Krichen, Moez and Kharbeche, Mohamed and Fetais, Noora and Barkaoui, Kamel},
doi = {10.5220/0009594305950602},
isbn = {9789897584213},
journal = {ENASE 2020 - Proceedings of the 15th International Conference on Evaluation of Novel Approaches to Software Engineering},
keywords = {Attack Trees,Automotive Communication,Blockchain,Internet of Vehicles,Model-Based Testing,Security,Timed Automaton},
pages = {595--602},
title = {{A formal model-based testing framework for validating an IoT solution for blockchain-based vehicles communication}},
year = {2020}
}
@article{Charaf2014,
abstract = {The paper presents some technical issues for testing distributed frameworks with rules based Agent System. The proposed approach consists firstly on exploring the benefits of rule based systems to design communication between different components of the distributed test application and then it deals with the distribution of rules over the testers to ensure their coordination without the need of a centralized manager system. The presented work describes our algorithm allowing the generation of the local rules to be respected by each tester and how we can use JESS in a multi-agent system. We compared also our results to those obtained in the centralized approach, where the antecedents of a rule can be in different testers, thus the tester can't decide when to enable the rule and that we will need a shared knowledge base to manage such situations.},
author = {Charaf, My El Hassan and Benattou, Mohammed and Azzouzi, Salma},
doi = {10.6688/JISE.2014.30.5.18},
issn = {10162364},
journal = {Journal of Information Science and Engineering},
keywords = {Agents,Controllability and observability problems,Distributed test,Rules,Synchronization},
number = {5},
pages = {1619--1634},
title = {{A JESS AGENT based architecture for testing distributed systems}},
volume = {30},
year = {2014}
}
@article{Bizerra2012,
abstract = {In this paper proposes a method for the generation of tests instances from business rules of a system, expressed using OCL. The aim of this method is to give one a better support for test activities like, for example, the generation of tests instances to test models in the initial phases of the software development lifecycle, showing the inconsistencies and ambiguities in earlier in the project. A differential of this method, compared to others with similar intention, is that, beyond the generation of the test cases, the generated tests could be validated and be followed in an automatized way through the USE tool. To help in the execution of the proposed method, the TestiMonium tool was implemented and integrated to the environment. {\textcopyright} 2012 IEEE.},
author = {Bizerra, Edilson Mendes and Silveira, Denis Silva and Cruz, Maria Lencastre Pinheiro Menezes and Wanderley, Fernando Jose Araujo},
doi = {10.1109/TLA.2012.6362355},
issn = {15480992},
journal = {IEEE Latin America Transactions},
keywords = {Model Based Test Generation,OCL,Software Testing,UML},
number = {5},
pages = {2105--2111},
publisher = {IEEE},
title = {{A method for generation of tests instances of models from business rules expressed in OCL}},
volume = {10},
year = {2012}
}
@article{Mouw2013,
abstract = {Background: Information Security is important for e-Science research groups and other small organisations that design and operate science gateways and virtual research environments, especially when such environments are being used for (bio)medical research. We propose a novel method to do risk assessments: MISRAM, the Model-based Information Security Risk Assessment Method. It uses an information architecture model, a method to assign values to information assets and IT components, and a method to calculate risks. The output of MISRAM is a ranked list of risks and a list of actionable tasks to solve the main issues. METHODS: MISRAM was applied as a test case to an e-Science research group at a Dutch research hospital. Meetings and surveys were used to create and evaluate lists of information assets and IT components. One meeting was used to create a list of practical task recommendations. RESULTS: Good insight into the information architecture and security problems of the IT infrastructure was gained. Also the participating group members confirmed that the identified security issues were realistic. CONCLUSIONS: Our approach raises awareness about security among the developers and operators of e-Science environments. It also gives insight in how the technical architecture affects information security. Traditional questionnaires are an important part of any risk assessment, and MISRAM's inclusion of such generic questionnaires is an important aspect to create an integrated information security risk assessment.},
author = {Mouw, Evert and {Van 't Noordende}, Guido and Louter, Baas and Olabarriaga, Silvia Delgado},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {DCRA,E-Science,IT&T,Information security,MISRAM,Risk assessment,Science gateway},
pages = {46},
publisher = {Citeseer},
title = {{A model-based information security risk assessment method for science gateways}},
volume = {993},
year = {2013}
}
@article{Felderer2014,
abstract = {In many development projects, testing has to be conducted under severe pressure due to limited resources and a challenging time schedule. Risk-based testing, which utilizes identified risks of the system for testing purposes, has a high potential to improve testing as it helps to optimize the allocation of resources and provides decision support for management. But for many organizations, the integration of a risk-based approach into established testing activities is a challenging task, and there are several options to do so. In this article, we analyze how risk is defined, assessed, and applied to support and improve testing activities in projects, products, and processes. We investigate these questions empirically by a multiple case study of currently applied risk-based testing activities in industry. The case study is based on three cases from different backgrounds, i.e., a test project in context of the extension of a large Web-based information system, product testing of a measurement and diagnostic equipment for the electrical power industry, as well as a test process of a system integrator of telecommunication solutions. By analyzing and comparing these different industrial cases, we draw conclusions on the state of risk-based testing and discuss possible improvements. {\textcopyright} 2014 Springer-Verlag Berlin Heidelberg.},
author = {Felderer, Michael and Ramler, Rudolf},
doi = {10.1007/s10009-014-0328-z},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Case study research,Multiple case study,Risk management,Risk-based testing,Software quality,Software testing,Test management,Test process improvement},
number = {5},
pages = {609--625},
publisher = {Springer},
title = {{A multiple case study on risk-based testing in industry}},
volume = {16},
year = {2014}
}
@article{Athamena2012,
abstract = {In Multi-Agent System (MAS), developers concentrate on creating design models and evolving them, from higher level models to lower level models, in several steps. Considerable part of MAS implementations is automatically produced from the design models. If a design model contains faults, they are passed to the generated implementations. Practical model validation techniques are required to discover and delete faults in abstract design models. In this paper, we introduce a formal approach for MAS design testing. It specifies a testing process that complements Multi-agent Systems Engineering (MaSE) methodology and strengthens the mutual relationship between UML and MAS. Besides, it defines a structured and comprehensive testing process for engineering software agents at the design level by providing a systematic way of converting the MAS design models to UML design diagram. Then a Petri Net (PN) diagram is generated from the UML models to simulate the behavior of the MAS system. Finally, because Petri Nets (PNs) are formal models, their analysis techniques can be applied to automatic MAS behavioral testing.},
author = {Athamena, Belkacem and Houhamdi, Zina},
doi = {10.5539/mas.v6n3p46},
issn = {19131844},
journal = {Modern Applied Science},
keywords = {Mase,Multi-Agent System,Petri Net,Sequence diagram,Software testing},
number = {3},
pages = {46--57},
publisher = {Canadian Center of Science and Education},
title = {{A petri net based Multi-Agent System behavioral testing}},
volume = {6},
year = {2012}
}
@article{,
abstract = {Modern software applications are getting more complex in order to provide better service and quality. This complexity has given birth too many challenges for software testing such as functional discontinuation and detection of system level defects. Existing testing techniques which are based on test cases are time consuming and unable to offer higher confidence on quality of products for these complex applications. Distributed testing frameworks could be used to test complex software but these frameworks do not provide a global picture of testing activities and application status. This poor visibility results in poor control over testing activities. In this study we have proposed a distributed testing model (DisTest) using scenario-based testing technique. DisTest could be used with any COTS test automation tool and could be employed at any testing level. Result shows DisTest provides better visibility and control on testing activities and view of application status.},
author = { and  and  and  and Mehmood, Mirza Aamir and Mahmood, Azhar and {Ahmed Khan}, Muhammad Naeem and Khatoon, Shaheen},
doi = {10.21833/ijaas.2016.10.011},
issn = {2313626X},
journal = {International Journal of ADVANCED AND APPLIED SCIENCES},
number = {10},
pages = {64--71},
title = {{A scenario-based distributed testing model for software applications}},
volume = {3},
year = {2016}
}
@article{Kloos2010,
abstract = {This paper considers the problem of model-based testing of a class of safety-critical systems. These systems are built up from components that are connected a network-like structure. The number of possible structures is usually large. In particular, we consider the following issue: For many of these systems, each instance needs its own set of models for testing. On the other hand, the instances that should be tested will have to be chosen so that the reliability statements are generally applicable. Thus, they must be chosen by a domain expert. The approach in this paper addresses both of these points. The structure of the instance of system under test is described using a domain-specific language, so that a domain expert can easily describe a system instance for testing. At the same time, the components and composition operators are formalized. Using a structure description written in the DSL, corresponding test models can be automatically generated, allowing for automated testing by the domain expert. We show some evidence about the feasibility of our approach and about the effort required for modelling an example, supporting our belief that our approach improves both on the efficiency and the expressivity of current compositional test model construction techniques. {\textcopyright} 2010 elsevier b.v. all rights reserved.},
author = {Kloos, Johannes and Eschbach, Robert},
doi = {10.1016/j.entcs.2010.05.009},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {DSL,Model-based testing,Safety-critical system},
pages = {145--160},
publisher = {Elsevier},
title = {{A systematic approach to construct compositional behaviour models for network-structured safety-critical systems}},
volume = {263},
year = {2010}
}
@article{Zein2016,
abstract = {The importance of mobile application specific testing techniques and methods has been attracting much attention of software engineers over the past few years. This is due to the fact that mobile applications are different than traditional web and desktop applications, and more and more they are moving to being used in critical domains. Mobile applications require a different approach to application quality and dependability and require an effective testing approach to build high quality and more reliable software. We performed a systematic mapping study to categorize and to structure the research evidence that has been published in the area of mobile application testing techniques and challenges that they have reported. Seventy nine (79) empirical studies are mapped to a classification schema. Several research gaps are identified and specific key testing issues for practitioners are identified: there is a need for eliciting testing requirements early during development process; the need to conduct research in real-world development environments; specific testing techniques targeting application life-cycle conformance and mobile services testing; and comparative studies for security and usability testing.},
author = {Zein, Samer and Salleh, Norsaremah and Grundy, John},
doi = {10.1016/j.jss.2016.03.065},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Mobile application testing,Software testing,Systematic mapping},
pages = {334--356},
publisher = {Elsevier},
title = {{A systematic mapping study of mobile application testing techniques}},
volume = {117},
year = {2016}
}
@article{Arora2016,
abstract = {Concurrent programs are replacing the sequential programs as they utilize the true capabilities of multicore architecture. The extensive use of multicore systems and multithreaded paradigms warrants more attention to the testing of the concurrent programs. The testing concurrent program is not a new field as it has been more than 40 years because the first problem related to the testing concurrent program was addressed by the researchers. The field covers various domains, which include concurrency problems, testing approaches, techniques, graphical representations, tools, and subject systems. This paper aims at providing an overview of research in the domain of testing concurrent programs by classifying it into eight categories: (a) reachability testing, (b) structural testing, (c) model-based testing, (d) mutation-based testing, (e) slicing-based testing, (f) formal methods, (g) random testing, and (h) search-based testing. The survey is focused on the techniques applied, methodologies followed, and tools used in these aforementioned approaches. Furthermore, the gaps are also identified in different approaches. The paper concludes with the consolidation of various testing parameters along with the future directions.},
author = {Arora, Vinay and Bhatia, Rajesh and Singh, Maninder},
doi = {10.1002/cpe.3711},
issn = {15320634},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {concurrency,concurrent programs,multithreaded program,systematic review,testing concurrent programs,testing methodologies},
number = {5},
pages = {1572--1611},
publisher = {Wiley Online Library},
title = {{A systematic review of approaches for testing concurrent programs}},
volume = {28},
year = {2016}
}
@article{Shafique2010,
abstract = {Model-based testing (MBT) is about testing a software system by using a model of its behaviour. To benefit fully from MBT, automation support is required. This paper presents a systematic review of prominent MBT tool support where we focus on tools that rely on state-based models. The systematic review protocol precisely describes the scope of the search and the steps involved in tool selection. Precisely defined criteria are used to compare selected tools and comprise support for test coverage criteria, level of automation for various testing activities, and support for the construction of test scaffolding. The results of this review should be of interest to a wide range of stakeholders: software companies interested in selecting the most appropriate MBT tool for their needs; organizations willing to invest into creating MBT tool support; researchers interested in setting research directions.},
author = {Shafique, Muhammad and Labiche, Yvan},
keywords = {state-based testing,systematic review,transition-based testing},
number = {May},
pages = {1--21},
title = {{A Systematic Review of Model Based Testing Tool Support}},
year = {2010}
}
@article{Utting2012,
abstract = {Model-based testing (MBT) relies on models of a system under test and/or its environment to derive test cases for the system. This paper discusses the process of MBT and defines a taxonomy that covers the key aspects of MBT approaches. It is intended to help with understanding the characteristics, similarities and differences of those approaches, and with classifying the approach used in a particular MBT tool. To illustrate the taxonomy, a description of how three different examples of MBT tools fit into the taxonomy is provided. Copyright {\textcopyright} 2011 John Wiley & Sons, Ltd.},
author = {Utting, Mark and Pretschner, Alexander and Legeard, Bruno},
doi = {10.1002/stvr.456},
issn = {09600833},
journal = {Software Testing Verification and Reliability},
keywords = {model-based testing approaches,survey,taxonomy},
number = {5},
pages = {297--312},
publisher = {Wiley Online Library},
title = {{A taxonomy of model-based testing approaches}},
volume = {22},
year = {2012}
}
@misc{felderer2019taxonomy,
archivePrefix = {arXiv},
arxivId = {cs.SE/1912.11519},
author = {Felderer, Michael and Schieferdecker, Ina},
eprint = {1912.11519},
primaryClass = {cs.SE},
title = {{A taxonomy of risk-based testing}},
year = {2019}
}
@article{Grossmann2020,
abstract = {This article provides a taxonomy for risk-based testing that serves as a tool to define, tailor, or assess such approaches. In this setting, the taxonomy is used to systematically identify deviations between the requirements from public standards and the individual testing approaches.},
author = {Grossmann, Juergen and Felderer, Michael and Viehmann, Johannes and Schieferdecker, Ina},
doi = {10.1109/MS.2019.2915297},
issn = {19374194},
journal = {IEEE Software},
keywords = {Risk management,Security and Privacy Protection,Test management,Testing strategies},
number = {1},
pages = {40--49},
publisher = {IEEE},
title = {{A Taxonomy to Assess and Tailor Risk-Based Testing in Recent Testing Standards}},
volume = {37},
year = {2020}
}
@article{Simons2020,
abstract = {The Stream X-Machine (SXM) testing method provides strong and repeatable guarantees of functional correctness, up to a specification. These qualities make the method attractive for software certification, especially in the domain of brokered cloud services, where arbitrage seeks to substitute functionally equivalent services from alternative providers. However, practical obstacles include the difficulty in providing a correct specification, the translation of abstract paths into feasible concrete tests and the large size of generated test suites. We describe a novel SXM verification and testing method, which automatically checks specifications for completeness and determinism, prior to generating complete test suites with full grounding information. Three optimization steps achieve up to a 10-fold reduction in the size of the test suite, removing infeasible and redundant tests. The method is backed by a set of tools to validate and verify the SXM specification, generate technology-agnostic test suites and ground these in SOAP, REST or rich-client service implementations. The method was initially validated using seven specifications, three cloud platforms and five grounding strategies.},
author = {Simons, Anthony J.H. and Lefticaru, Raluca},
doi = {10.1002/stvr.1729},
issn = {10991689},
journal = {Software Testing Verification and Reliability},
keywords = {X-machines,cloud computing,cloud service broker,functional testing,service certification,specification,state-based testing,test grounding,verification},
number = {3},
pages = {e1729},
publisher = {Wiley Online Library},
title = {{A verified and optimized Stream X-Machine testing method, with application to cloud service certification}},
volume = {30},
year = {2020}
}
@article{Fraser2015,
abstract = {Without complete formal specification, automatically generated software tests need to be manually checked in order to detect faults. This makes it desirable to produce the strongest possible test set while keeping the number of tests as small as possible. As commonly applied coverage criteria like branch coverage are potentially weak, mutation testing has been proposed as a stronger criterion. However, mutation based test generation is hampered because usually there are simply too many mutants, and too many of these are either trivially killed or equivalent. On such mutants, any effort spent on test generation would per definition be wasted. To overcome this problem, our search-based EvoSuite test generation tool integrates two novel optimizations: First, we avoid redundant test executions on mutants by monitoring state infection conditions, and second we use whole test suite generation to optimize test suites towards killing the highest number of mutants, rather than selecting individual mutants. These optimizations allowed us to apply EvoSuite to a random sample of 100 open source projects, consisting of a total of 8,963 classes and more than two million lines of code, leading to a total of 1,380,302 mutants. The experiment demonstrates that our approach scales well, making mutation testing a viable test criterion for automated test case generation tools, and allowing us to analyze the relationship of branch coverage and mutation testing in detail.},
author = {Fraser, Gordon and Arcuri, Andrea},
doi = {10.1007/s10664-013-9299-z},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Mutation testing,Search-based testing,Test case generation,Testing classes,Unit testing},
number = {3},
pages = {783--812},
publisher = {Springer},
title = {{Achieving scalable mutation-based generation of whole test suites}},
volume = {20},
year = {2015}
}
@article{Chen2010,
abstract = {Random testing is not only a useful testing technique in itself, but also plays a core role in many other testing methods. Hence, any significant improvement to random testing has an impact throughout the software testing community. Recently, Adaptive Random Testing (ART) was proposed as an effective alternative to random testing. This paper presents a synthesis of the most important research results related to ART. In the course of our research and through further reflection, we have realised how the techniques and concepts of ART can be applied in a much broader context, which we present here. We believe such ideas can be applied in a variety of areas of software testing, and even beyond software testing. Amongst these ideas, we particularly note the fundamental role of diversity in test case selection strategies. We hope this paper serves to provoke further discussions and investigations of these ideas. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Chen, Tsong Yueh and Kuo, Fei Ching and Merkel, Robert G. and Tse, T. H.},
doi = {10.1016/j.jss.2009.02.022},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Adaptive random sequence,Adaptive random testing,Failure pattern,Failure-based testing,Random testing,Software testing},
number = {1},
pages = {60--66},
publisher = {Elsevier},
title = {{Adaptive Random Testing: The ART of test case diversity}},
volume = {83},
year = {2010}
}
@article{Jabbar2020a,
abstract = {The Electronic Health Records (EHR) sharing system is the modern tool for delivering efficient healthcare to patients. Its functions include tracking of therapies, monitoring of the treatment effectiveness, prediction of outcomes throughout the patient's lifespan, and detection of human errors. For all the stakeholders, integrity and interoperability of the care continuum are paramount. Yet, its implementation is challenging due to the heterogeneity of healthcare information systems, security threats, and the enormousness of EHR data. To overcome these challenges, this work proposes BiiMED: a Blockchain framework for Enhancing Data Interoperability and Integrity regarding EHR-sharing. This solution is innovative as it contains an access management system allowing the exchange of EHRs between different medical providers and a decentralized Trusted Third Party Auditor (TTPA) for ensuring data integrity. This paper also discusses two validation techniques for enhancing the quality and correctness of the proposed solution: Formal Verification and Model-Based Techniques. The first one checks the correctness of a mathematical model describing the behavior of the given system prior to the implementation. The second technique derives test suites from the adopted model, performs them, and assesses the correctness.},
author = {Jabbar, Rateb and Krichen, Moez and Fetais, Noora and Barkaoui, Kamel},
doi = {10.5220/0009592102610268},
isbn = {9789897584237},
journal = {ICEIS 2020 - Proceedings of the 22nd International Conference on Enterprise Information Systems},
keywords = {BiiMED,Blockchain,Etherum,Formal Verification,Health Records,Model-Based Testing,Sharing System},
pages = {261--268},
title = {{Adopting formal verification and model-based testing techniques for validating a blockchain-based healthcare records sharing system}},
volume = {1},
year = {2020}
}
@article{Riungu-Kalliosaari2016,
abstract = {This qualitative study addresses the adoption, utilization and effects of cloud-based testing in different organizational contexts. We approached the research problem by conducting thirty-five interviews with professionals in 20 organizations and applied grounded theory as the research method. The results indicate that cloud-based testing provides viable solutions to meet the testing needs within organizations. Cloud-based resources can be applied in performing various testing activities such as performance and multiplatform testing as well supporting practitioners in involving users during iterative development and testing. Cloud-based testing also adds value to practitioners by enabling easier management of the cloud-based testing resources and helping to produce improved end products. We use the results of the study to propose a strategy that can be used to assist practitioners in their decision-making processes towards adoption of cloud-based testing.},
author = {Riungu-Kalliosaari, Leah and Taipale, Ossi and Smolander, Kari and Richardson, Ita},
doi = {10.1007/s11219-014-9256-0},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Cloud computing,Cloud-based testing,Cloud-based testing resources,Testing},
number = {2},
pages = {337--364},
publisher = {Springer},
title = {{Adoption and use of cloud-based testing in practice}},
volume = {24},
year = {2016}
}
@article{Genc2020,
abstract = {Test oracles differentiate between the correct and incorrect system behavior. Automation of test oracles for visual output systems mainly involves image comparison, where a snapshot of the output is compared with respect to a reference image. Hereby, the captured snapshot can be subject to variations such as scaling and shifting. These variations lead to incorrect evaluations. Existing approaches employ computer vision techniques to address a specific set of variations. In this article, we introduce ADVISOR, an adjustable framework for test oracle automation of visual output systems. It allows the use of a flexible combination and configuration of computer vision techniques. We evaluated a set of valid configurations with a benchmark dataset collected during the tests of commercial digital TV systems. Some of these configurations achieved up to 3% better overall accuracy with respect to state-of-the-art tools. Further, we observed that there is no configuration that reaches the best accuracy for all types of image variations. We also empirically investigated the impact of significant parameters. One of them is a threshold regarding image matching score that determines the final verdict. This parameter is automatically tuned by offline training. We evaluated runtime performance as well. Results showed that differences among the ADVISOR configurations and state-of-the-art tools are in the order of seconds per image comparison.},
author = {Genc, Ahmet Esat and Sozer, Hasan and {Furkan Kirac}, M. and Aktemur, Baris},
doi = {10.1109/TR.2019.2957507},
issn = {15581721},
journal = {IEEE Transactions on Reliability},
keywords = {Adjustable framework,black-box testing,com-puter vision,test automation,test oracle},
number = {3},
pages = {1050--1063},
publisher = {IEEE},
title = {{ADVISOR: An Adjustable Framework for Test Oracle Automation of Visual Output Systems}},
volume = {69},
year = {2020}
}
@article{Jia2011,
abstract = {Mutation Testing is a fault-based software testing technique that has been widely studied for over three decades. The literature on Mutation Testing has contributed a set of approaches, tools, developments, and empirical results. This paper provides a comprehensive analysis and survey of Mutation Testing. The paper also presents the results of several development trend analyses. These analyses provide evidence that Mutation Testing techniques and tools are reaching a state of maturity and applicability, while the topic of Mutation Testing itself is the subject of increasing interest. {\textcopyright} 2011 IEEE.},
author = {Jia, Yue and Harman, Mark},
doi = {10.1109/TSE.2010.62},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Mutation testing,survey},
number = {5},
pages = {649--678},
publisher = {IEEE},
title = {{An analysis and survey of the development of mutation testing}},
volume = {37},
year = {2011}
}
@article{,
abstract = {Testing is a crucial step in designing and implementing software in the distributed environment. Testing in the distributed applications is not only difficult, but also a costly method. This Research briefly discusses the performance testing in the distributed software environment along with different other testing techniques proposed in the literature that correspond to distributed applications. Additionally, we discuss the key testing challenges faced during the whole process of testing the distributed applications. Much of the focus of this paper is on intelligent agent-based testing. Agent based testing is provide better coordination mechanism between multiple testers and exert more controllability and observablility on fault detection. In this study we have critically analyzed testing methodologies being practiced in the distributed environment. We have studied the merits and limitations of these methodologies proposed in the contemporary literature and have identified the possible improvements in those methodologies to make them more robust.},
author = { and {Fraz Malik}, Muhammad and {A. Khan}, M. N.},
doi = {10.5815/ijmecs.2016.07.06},
issn = {20750161},
journal = {International Journal of Modern Education and Computer Science},
number = {7},
pages = {53--60},
publisher = {Modern Education and Computer Science Press},
title = {{An Analysis of Performance Testing in Distributed Software Applications}},
volume = {8},
year = {2016}
}
@article{El-Fakih2017,
abstract = {Extended finite state machines (EFSMs) provide a rigorous model for the derivation of functional tests for software systems and protocols. Various types of data-flow, control-flow, graph-based, and state machine based test selection criteria can be used for deriving tests from a given EFSM specification. Also, traditional types of state machine based notions of faults, such as transfer and output parameter faults, and common types of assignment faults can be used to describe the fault domains of EFSMs. We present an assessment of the most known types of EFSM test selection criteria such as test suites that cover single transfer faults, double transfer faults, single output parameter faults, and many types of single assignment faults of a given EFSM specification. Also, test suites that cover edge-pair, prime path, prime path with side trip, and all-uses criterion are derived from the graph and flow-graph representations of the specification. We also consider transition tour and random test suites. The assessment ranks the considered test suites in terms of their length and their coverage of single transfer, double transfer, and different type of single assignment faults. Dispersion of the obtained results is assessed and results are summarized.},
author = {El-Fakih, Khaled and Simao, Adenilso and Jadoon, Noshad and Maldonado, Jose Carlos},
doi = {10.1016/j.jss.2016.09.044},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Extended finite state machines,Fault coverage assessment,Model based testing,Mutation testing,Test derivation},
pages = {106--118},
publisher = {Elsevier},
title = {{An assessment of extended finite state machine test selection criteria}},
volume = {123},
year = {2017}
}
@article{Usman2020,
abstract = {Mobile devices have limited resources, including memory and processing speed. The performance of mobile applications is an important concern. There are a large number of mobile platforms available with varying operating systems and hardware. Native applications are usually developed and maintained separately for these platforms. The overall performance of native applications may significantly vary across platforms. The current industrial practice is to manually test the performance for each variant, which is not a scalable or efficient approach. We tackled the problem of generating native application variants in our previous work. This paper proposes an automated model-based approach for performance test generation for native application variants at unit level. We propose a performance profile that allows modeling of domain-specific performance parameters on UML models, which are used for automated performance test generation for each native variant. The results of applying the approach on two real-world applications show that the approach evaluates the performance of application variants for two different versions of Android successfully and have potential to reduce the effort and time. A questionnaire-based study is conducted to evaluate the usefulness of the approach.},
author = {Usman, Muhammad and Iqbal, Muhammad Zohaib and Khan, Muhammad Uzair},
doi = {10.1002/smr.2215},
issn = {20477481},
journal = {Journal of Software: Evolution and Process},
keywords = {aspect,mobile application,model-based,performance profile,performance testing,state machine},
number = {1},
pages = {e2215},
publisher = {Wiley Online Library},
title = {{An automated model-based approach for unit-level performance test generation of mobile applications}},
volume = {32},
year = {2020}
}
@article{MacedoRodrigues2015,
abstract = {A variety of testing tools has been developed to support and automate the software testing activity. Some of them may use different techniques such as Model-based Testing (MBT) or Capture and Replay (CR). Model-based Testing is a technique for automatic generation of testing artifacts based on software models. One of the main benefits of using MBT is related to the easiness of maintaining models over code; hence, it is likely that using models as a source for automatic generation of scripts requires less effort and reduces the number of faults. Otherwise, CR-based tools basically record the user interaction with the System Under Test (SUT) and then playback the recorded test. This paper presents our effort on setting up and running an experimental study performed in order to evaluate the effort to use MBT and CR-based tools to generate performance scripts. Thus, we apply an MBT and a CR approaches for the purpose of evaluation with respect to the effort to generate scripts and scenarios from the perspective of the performance testers and the performance test engineers in the context of undergraduates, M.Sc. and Ph.D. students, performance testers and performance test engineers for the generation of performance test scripts and scenarios. Our results indicate that, for simple testing tasks, the effort of using a CR-based tool was lower than using an MBT tool, but as the complexity or size of the activities of the testing tasks increases, the advantage of using MBT increased significantly.},
author = {{Macedo Rodrigues}, Elder and {Moreira de Oliveira}, Fl{\'{a}}vio and {Teodoro Costa}, Leandro and Bernardino, Maicon and Zorzo, Avelino Francisco and {do Rocio Senger Souza}, Simone and Saad, Rodrigo},
doi = {10.1007/s10664-014-9337-5},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Empirical study,Experimental study,Model-based testing,Performance testing,Script generation,Testing automation},
number = {6},
pages = {1831--1860},
publisher = {Springer},
title = {{An empirical comparison of model-based and capture and replay approaches for performance testing}},
volume = {20},
year = {2015}
}
@article{Charaf2013,
abstract = {In the distributed test context, where a set of parallel testers exchange some I/O messages to perform the test, the implementation process must consider the mechanisms and functions required to support interaction as long as the communication and the coordination between distributed testing components. The typical reactions of such systems are the generation of errors such as time outs, locks, channels and network failures. Nowadays, rule based systems provide an interesting approach for representing and interpreting such kind of messages exchange using the artificial intelligence features. In this paper, we suggest two algorithms allowing the generation of rules to be respected by the test system to avoid the coordination/synchronization problems. Then, we explain the advantages of the study and how the proposed architecture based on expert systems can avoid the use of the coordination messages and resolve the coordination problems. Thus, the testers will exchange only the observation messages which will reduce significantly the use of external messages and I/O operations. {\textcopyright} 2013 ISSN 1349-4198.},
author = {Charaf, My El Hassan and Benattou, Mohammed and Azzouzi, Salma},
issn = {13494198},
journal = {International Journal of Innovative Computing, Information and Control},
keywords = {Controllability and observability problems,Distributed test,Expert system,Rules,Synchronization},
number = {9},
pages = {3779--3797},
title = {{An expert system based architecture for testing distributed systems}},
volume = {9},
year = {2013}
}
@article{Anand2013,
abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and McMinn, Phil},
doi = {10.1016/j.jss.2013.02.061},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Adaptive random testing,Combinatorial testing,Model-based testing,Orchestrated survey,Search-based software testing,Software testing,Symbolic execution,Test automation,Test case generation},
number = {8},
pages = {1978--2001},
publisher = {Elsevier},
title = {{An orchestrated survey of methodologies for automated software test case generation}},
volume = {86},
year = {2013}
}
@article{Erdogan2014,
abstract = {Risk analysis and testing are conducted for different purposes. Risk analysis and testing nevertheless involve processes that may be combined to the benefit of both. We may use testing to support risk analysis and risk analysis to support testing. This paper surveys literature on the combined use of risk analysis and testing. First, the existing approaches are identified through a systematic literature review. The identified approaches are then classified and discussed with respect to main goal, context of use and maturity level. The survey highlights the need for more structure and rigor in the definition and presentation of approaches. Evaluations are missing in most cases. The paper may serve as a basis for examining approaches for the combined use of risk analysis and testing, or as a resource for identifying the adequate approach to use. {\textcopyright} 2014 Springer-Verlag Berlin Heidelberg.},
author = {Erdogan, Gencer and Li, Yan and Runde, Ragnhild Kobro and Seehusen, Fredrik and St{\o}len, Ketil},
doi = {10.1007/s10009-014-0330-5},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Literature survey,Risk-based testing,Test-based risk analysis},
number = {5},
pages = {627--642},
publisher = {Springer},
title = {{Approaches for the combined use of risk analysis and testing: A systematic literature review}},
volume = {16},
year = {2014}
}
@article{Mouchawrab2011,
abstract = {A large number of research works have addressed the importance of models in software engineering. However, the adoption of model-based techniques in software organizations is limited since these models are perceived to be expensive and not necessarily cost-effective. Focusing on model-based testing, this paper reports on a series of controlled experiments. It investigates the impact of state machine testing on fault detection in class clusters and its cost when compared with structural testing. Based on previous work showing this is a good compromise in terms of cost and effectiveness, this paper focuses on a specific state-based technique: the round-trip paths coverage criterion. Round-trip paths testing is compared to structural testing, and it is investigated whether they are complementary. Results show that even when a state machine models the behavior of the cluster under test as accurately as possible, no significant difference between the fault detection effectiveness of the two test strategies is observed, while the two test strategies are significantly more effective when combined by augmenting state machine testing with structural testing. A qualitative analysis also investigates the reasons why test techniques do not detect certain faults and how the cost of state machine testing can be brought down. {\textcopyright} 2006 IEEE.},
author = {Mouchawrab, Samar and Briand, Lionel C. and Labiche, Yvan and {Di Penta}, Massimiliano},
doi = {10.1109/TSE.2010.32},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {State-based software testing,controlled experiments,state machines,structural testing},
number = {2},
pages = {161--187},
publisher = {IEEE},
title = {{Assessing, comparing, and combining state machine-based testing and structural testing: A series of experiments}},
volume = {37},
year = {2011}
}
@article{Arcuri2021,
abstract = {RESTful APIs are very popular in industry, especially when developing enterprise systems using a microservice architecture. Testing such APIs is challenging, as tests will be composed of not only HTTP calls, but also settings of the environment, like databases. Different blackbox testing techniques have been shown to easily find real faults in many RESTful APIs, with very little human effort from software engineers. However, whitebox techniques could lead to much better results, although having an up-front cost for the engineers. In this paper, we report on the use of the open-source tool EvoMaster, on eight RESTful APIs. We show how EvoMaster can be used to automatically generate test cases that can find several bugs, even when using a naive blackbox approach. When enhancing the search with whitebox information, significantly better results are achieved. However, there are several challenges that need to be taken into account when an engineer wants to use a tool such as EvoMaster to test their projects.},
author = {Arcuri, Andrea},
doi = {10.1109/MS.2020.3013820},
issn = {19374194},
journal = {IEEE Software},
keywords = {Databases,Open source software,Payloads,Servers,Testing,Tools,Web services},
number = {3},
pages = {72--78},
publisher = {IEEE},
title = {{Automated Black- And White-Box Testing of RESTful APIs with EvoMaster}},
volume = {38},
year = {2021}
}
@article{Adamoli2011,
abstract = {A significant body of prior work has devised approaches for automating the functional testing of interactive applications. However, little work exists for automatically testing their performance. Performance testing imposes additional requirements upon GUI test automation tools: the tools have to be able to replay complex interactive sessions, and they have to avoid perturbing the application's performance. We study the feasibility of using five Java GUI capture and replay tools for GUI performance test automation. Besides confirming the severity of the previously known GUI element identification problem, we also describe a related problem, the temporal synchronization problem, which is of increasing importance for GUI applications that use timer-driven activity. We find that most of the tools we study have severe limitations when used for recording and replaying realistic sessions of real-world Java applications and that all of them suffer from the temporal synchronization problem. However, we find that the most reliable tool, Pounder, causes only limited perturbation and thus can be used to automate performance testing. Based on an investigation of Pounder's approach, we further improve its robustness and reduce its perturbation. Finally, we demonstrate in a set of case studies that the conclusions about perceptible performance drawn from manual tests still hold when using automated tests driven by Pounder. Besides the significance of our findings to GUI performance testing, the results are also relevant to capture and replay-based functional GUI test automation approaches. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {Adamoli, Andrea and Zaparanuks, Dmitrijs and Jovic, Milan and Hauswirth, Matthias},
doi = {10.1007/s11219-011-9135-x},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Graphical user interfaces,Perfomance analysis,Performance testing,Test automation},
number = {4},
pages = {801--839},
publisher = {Springer},
title = {{Automated GUI performance testing}},
volume = {19},
year = {2011}
}
@article{Wang2019,
abstract = {Implementing test suites for distributed software systems is a complex and time-consuming task due to the number of test cases that need to be considered in order to obtain high coverage. We show how a formal Coloured Petri Net model can be used to automatically generate a suite of test cases for the Paxos distributed consensus protocol. The test cases cover both normal operation of the protocol as well as failure injection. To evaluate our model-based testing approach, we have implemented the Paxos protocol in the Go programming language using the quorum abstractions provided by the Gorums framework. Our experimental evaluation shows that we obtain high code coverage for our Paxos implementation using the automatically generated test cases.},
author = {Wang, Rui and Kristensen, Lars Michael and Meling, Hein and Stolz, Volker},
doi = {10.1016/j.jlamp.2019.02.004},
issn = {23522216},
journal = {Journal of Logical and Algebraic Methods in Programming},
keywords = {Coloured Petri Nets,Distributed systems,Model-based testing},
pages = {254--273},
publisher = {Elsevier},
title = {{Automated test case generation for the Paxos single-decree protocol using a Coloured Petri Net model}},
volume = {104},
year = {2019}
}
@article{Enoiu2016,
abstract = {In software development, testers often focus on functional testing to validate implemented programs against their specifications. In safety-critical software development, testers are also required to show that tests exercise, or cover, the structure and logic of the implementation. To achieve different types of logic coverage, various program artifacts such as decisions and conditions are required to be exercised during testing. Use of model checking for structural test generation has been proposed by several researchers. The limited application to models used in practice and the state space explosion can, however, impact model checking and hence the process of deriving tests for logic coverage. Thus, there is a need to validate these approaches against relevant industrial systems such that more knowledge is built on how to efficiently use them in practice. In this paper, we present a tool-supported approach to handle software written in the Function Block Diagram language such that logic coverage criteria can be formalized and used by a model checker to automatically generate tests. To this end, we conducted a study based on industrial use-case scenarios from Bombardier Transportation AB, showing how our toolbox CompleteTest can be applied to generate tests in software systems used in the safety-critical domain. To evaluate the approach, we applied the toolbox to 157 programs and found that it is efficient in terms of time required to generate tests that satisfy logic coverage and scales well for most of the programs.},
author = {Enoiu, Eduard P. and {\v{C}}au{\v{s}}evi{\'{c}}, Adnan and Ostrand, Thomas J. and Weyuker, Elaine J. and Sundmark, Daniel and Pettersson, Paul},
doi = {10.1007/s10009-014-0355-9},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Automated test generation,FBD,Function Block Diagram,IEC 1131-3,Logic coverage safety-critical systems,Model checking,Model-based testing,PLC,Programmable Logic Controllers,Software testing,Structured Text,Uppaal},
number = {3},
pages = {335--353},
publisher = {Springer},
title = {{Automated test generation using model checking: an industrial evaluation}},
volume = {18},
year = {2016}
}
@article{KahSeng2018,
abstract = {Automated web application penetration testing has emerged as a trend. The computer was assigned the task of penetrating web application security with penetration testing technique. Relevant computer program reduces time, cost, and resources required for assessing a web application security. At the same time, scaling down tester reliance on human knowledge. Web application security scanner is such kind of program that is designed to assess web application security automatically with penetration testing technique. The downside is that computer is not well-formed as human. Consequently, web application security scanner often found generating the false alarms, especially in a testing environment, which web application source codes are unreachable. Thus, in this paper, the state-of-the-art of black box web application security scanner is systematically reviewed, to investigate the approaches for detecting web application vulnerability in an ambiguous testing environment.  This survey is critical in providing insights on how to design efficient algorithms for assessing web application security with penetration testing technique in the ambiguous environment.},
author = {{Kah Seng}, Lim and Ithnin, Norafida and {Mohd Shaid}, Syed Zainudeen},
doi = {10.11113/ijic.v8n3.180},
issn = {2180-4370},
journal = {International Journal of Innovative Computing},
number = {3},
title = {{Automating Penetration Testing Within Ambiguous Testing Environment}},
volume = {8},
year = {2018}
}
@article{Carrera2014,
abstract = {This paper presents a testing methodology to apply Behaviour Driven Development (BDD) techniques while developing Multi-Agent Systems (MASs), termed BEhavioural Agent Simple Testing (BEAST) Methodology. This methodology is supported by the open source framework (BEAST Tool) which automatically generates test cases skeletons from BDD scenarios specifications. The developed framework allows the testing of MASs based on JADE or JADEX platforms. In addition, this framework offers a set of configurable Mock Agents with the aim of being able to execute tests while the MAS is under development. The BEAST Methodology presents transparent traceability from user requirements to test cases. Thus, the stakeholders can be aware of the project status. The methodology and the associated tool have been validated in the development of a MAS for fault diagnosis in FTTH (Fiber To The Home) networks. The results have been measured in quantifiable way obtaining a reduction of the tests implementation time. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Carrera, {\'{A}}lvaro and Iglesias, Carlos A. and Garijo, Mercedes},
doi = {10.1007/s10796-013-9438-5},
issn = {13873326},
journal = {Information Systems Frontiers},
keywords = {Agile,Behaviour-driven development,Methodology,Mock-agents,Multi-agent systems,Test},
number = {2},
pages = {169--182},
publisher = {Springer},
title = {{Beast methodology: An agile testing methodology for multi-agent systems based on behaviour driven development}},
volume = {16},
year = {2014}
}
@article{Bartolini2011,
abstract = {The attractive feature of Service Oriented Architecture (SOA) is that pieces of software conceived and developed by independent organizations can be dynamically composed to provide richer functionality. The same reasons that enable flexible compositions, however, also prevent the application of some traditional testing approaches, making SOA validation challenging and costly. Web services usually expose just an interface, enough to invoke them and develop some general (black-box) tests, but insufficient for a tester to develop an adequate understanding of the integration quality between the application and the independent web services. To address this lack we propose an approach that makes web services more transparent to testers through the addition of an intermediary service that provides coverage information. The approach, named Service Oriented Coverage Testing (SOCT), provides testers with feedback about how much a service is exercised by their tests without revealing the service internals. In SOCT, testing feedback is offered itself as a service, thus preserving SOA founding principles of loose coupling and implementation neutrality. In this paper we motivate and define the SOCT approach, and implement an instance of it. We also perform a study to asses SOCT feasibility and provide a preliminary evaluation of its viability and value. {\textcopyright} 2010 Elsevier Inc. All rights reserved.},
author = {Bartolini, Cesare and Bertolino, Antonia and Elbaum, Sebastian and Marchetti, Eda},
doi = {10.1016/j.jss.2010.10.024},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Coverage adequacy criteria,Service-Oriented Architecture,Testing web services,White-box testing},
number = {4},
pages = {655--668},
publisher = {Elsevier},
title = {{Bringing white-box testing to Service Oriented Architectures through a Service Oriented Approach}},
volume = {84},
year = {2011}
}
@article{Shrivastva2014,
abstract = {Cloud computing is the recently emerged technology which has gained popularity among organizations and corporates. For better services of the cloud, there is need for some kind of testing. Cloud testing then came into existence which referred as a form of testing in which cloud computing environment is used by web applications to simulate real world user traffic. Testing somehow saves the cost of maintenance which is helpful for the customers. This paper provides us with various cloud testing techniques, challenges, issues and benefits in testing areas. It also elaborates all the fundamental concepts regarding features and requirements in cloud testing. Furthermore, various cloud testing platforms are also discussed briefly. Cloud testing has been explained widely in this paper that would help to understand various aspects of cloud testing in a much better way.},
author = {Shrivastva, Akash and Gupta, Shubham and Tiwari, Rinki},
doi = {10.5120/18200-9122},
issn = {0975-8887},
journal = {International Journal of Computer Applications},
number = {5},
pages = {24--29},
publisher = {Citeseer},
title = {{Cloud based Testing Techniques (CTT)}},
volume = {104},
year = {2014}
}
@article{Cunha2017,
abstract = {As the number of infrastructure-as-a-service (IaaS) cloud offers in the market increases, selecting an appropriate configuration of cloud resources for a given application becomes a non-trivial task even for experienced developers. Because cloud resources are relatively cheap, usually charged by the hour, developers could systematically evaluate the performance of their application using different resource types from different cloud providers, thus allowing them to accurately identify the best providers and resource types for their application. However, conducting systematic performance tests in multiple IaaS clouds may require a significant amount of planning and configuration effort from application developers. This paper presents Cloud Crawler, a declarative environment for specifying and conducting application performance tests in IaaS clouds. The environment includes a novel declarative domain-specific language, Crawl, by means of which cloud users can describe, at a high abstraction level, a large variety of performance evaluation scenarios for a given application, and a scenario execution engine, Crawler, which automatically configures, executes, and collects the results of the scenarios described in Crawl. The paper also reports on how Cloud Crawler has been successfully used to systematically test the performance of two open-source web applications in public IaaS clouds. Copyright {\textcopyright} 2016 John Wiley & Sons, Ltd.},
author = {Cunha, M. and Mendon{\c{c}}a, N. C. and Sampaio, A.},
doi = {10.1002/cpe.3825},
issn = {15320634},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {cloud computing,declarative environment,performance evaluation},
number = {1},
pages = {e3825},
publisher = {Wiley Online Library},
title = {{Cloud Crawler: a declarative performance evaluation environment for infrastructure-as-a-service clouds}},
volume = {29},
year = {2017}
}
@article{Tao2015,
abstract = {With respect to security, the use of various terminals in the mobile Internet environment is problematic. Traditional terminal testing methods cannot simulate actual testing environments; thus, the test results do not accurately reflect the security of terminals. To address this problem, we designed and developed a cloud platform based automated testing system for the mobile Internet. In this system, virtualization and automation technology are utilized to integrate mobile terminals into the cloud platform as a resource, to achieve a novel cloud service called Testing as a Service (TaaS). The system consists of three functional modules: web front-end module, testing environment module, and automated testing module. We adopted the permeable automated testing tool Metasploit to perform security testing. In our test experiments, we selected 100 apps with diverse vulnerability levels, ranging from secure to vulnerable, to perform a series of functional tests. The experimental results show that this system can correctly test both the number of vulnerable apps and their corresponding vulnerability levels. As such, the designed system can flexibly configure various testing environments for different testing cases or projects, and thereby perform security testing automatically.},
author = {Tao, Dan and Lin, Zhaowen and Lu, Cheng},
doi = {10.1109/TST.2015.7349926},
issn = {10070214},
journal = {Tsinghua Science and Technology},
keywords = {Automated security testing,Cloud platform,Metasploit,Virtualization},
number = {6},
pages = {537--544},
publisher = {TUP},
title = {{Cloud platform based automated security testing system for mobile internet}},
volume = {20},
year = {2015}
}
@article{Bertolino2020,
abstract = {While great emphasis is given in the current literature about the potential of leveraging the cloud for testing purposes, the authors have scarce factual evidence from real-world industrial contexts about the motivations, drawbacks and benefits related to the adoption of automated cloud testing technology. In this study, the authors present an empirical study undertaken within the ongoing European Project ElasTest, which has developed an open source platform for end-to-end testing of large distributed systems. This study aims at validating the ElasTest solution, and consists of the assessment of four demonstrators belonging to different application domains, namely e-commerce, 5G networking, WebRTC and Internet of Things. For each demonstrator, they collected differing requirements, and achieved varying results, both positive and negative, showing that cloud testing needs careful assessment before adoption.},
author = {Bertolino, Antonia and Calabr{\`{o}}, Antonello and Marchetti, Eda and Sala, Anton Cervantes and de Hita, Guiomar Tu{\~{n}}{\'{o}}n and Pop, Ilie Daniel Gheorghe and Gowtham, Varun},
doi = {10.1049/iet-sen.2019.0140},
issn = {17518806},
journal = {IET Software},
number = {5},
pages = {553--562},
publisher = {IET},
title = {{Cloud testing automation: Industrial needs and elas test response}},
volume = {14},
year = {2020}
}
@article{Gao2011,
abstract = {Cloud computing not only changes the way of obtaining computing resources (such as computers, infrastructures, data storage, and application services), but also changes the way of managing and delivering computing services, technologies, and solutions. Cloud computing leads an opportunity in offering testing as a service (TaaS) for SaaS and clouds. Meanwhile, it causes new issues, challenges and needs in software testing, particular in testing clouds and cloud-based applications. This paper provides a comprehensive tutorial on cloud testing and cloud-based application testing. It answers the common questions raised by engineers and managers, and it provides clear concepts, discusses the special objectives, features, requirements, and needs in cloud testing. It offers a clear comparative view between web-based software testing and cloud-based application testing. In addition, it examines the major issues, challenges, and needs in testing cloud-based software applications. Furthermore, it also summarizes and compares different commercial products and solutions supporting cloud testing as services.},
author = {Gao, Jerry and Bai, Xiaoying and Tsai, Wei-Tek},
journal = {Software Engineering: An International Journal},
number = {1},
pages = {9--23},
title = {{Cloud testing-issues, challenges, needs and practice}},
volume = {1},
year = {2011}
}
@article{Vilkomir2012,
author = {Vilkomir, Sergiy},
issn = {1311-1493},
journal = {Information & Security},
number = {2},
pages = {213},
publisher = {ProCon Ltd.},
title = {{Cloud testing: A state-of-the-art review}},
volume = {28},
year = {2012}
}
@article{Husni2017,
author = {Husni, Hind and Saifan, Ahmad A},
journal = {New Trends in Information Technology (NTIT)–2017},
pages = {34},
title = {{Cloud testing: Steps, tools, challenges}},
year = {2017}
}
@article{Siddiqui2015,
author = {Siddiqui, Tamanna and Ahmad, Riaz},
journal = {International Research Journal of Engineering and Technology (IRJET)},
number = {03},
pages = {397--406},
title = {{Cloud Testing–A Systematic Review}},
volume = {2},
year = {2015}
}
@article{Hilken2015,
author = {Hilken, Christoph and H{\"{u}}bner, Felix},
keywords = {cyber-physical sys-,etcs,hybrid systems,model-based testing,sysml,tems},
number = {December},
title = {{Combination of Behavioral and Parametric Diagrams for Model-based Testing Technical Report December 2015}},
year = {2015}
}
@article{Huang2017,
abstract = {The main objective of this article is to present a complete finite black-box testing theory for non-deterministic Kripke structures with possibly infinite input domains, but finite domains for internal state variables and outputs. To this end, an abstraction from Kripke structures of this sub-domain to finite state machines is developed. It is shown that every complete black-box testing theory for (deterministic or nondeterministic) finite state machines in the range of this abstraction induces a complete black-box input equivalence class partition testing (IECPT) theory for the Kripke structures under consideration. Additionally, it is shown that each of these IECPT theories can be combined with random testing, such that a random value is selected from an input equivalence class, whenever a representative from this class is required in a test step. Experiments have shown that this combination increases the test strength of equivalence class tests for systems under test (SUT) outside the fault domain, while we show here that this randomisation preserves the completeness property for SUT inside the domain. The investigations lead to several complete IECPT strategies which, to our best knowledge, were not known before for this sub-domain of Kripke structures. The elaboration and presentation of results is performed on a semantic level, so that the testing theories under consideration can be applied to models presented in any concrete formalism, whose behaviour is reflected by a member of our semantic category.},
author = {ling Huang, Wen and Peleska, Jan},
doi = {10.1007/s00165-016-0402-2},
issn = {1433299X},
journal = {Formal Aspects of Computing},
keywords = {Complete testing theories,Equivalence class partition testing,Kripke structures,Model-based testing,Nondeterminism,Random testing},
number = {2},
pages = {335--364},
publisher = {Springer},
title = {{Complete model-based equivalence class testing for nondeterministic systems}},
volume = {29},
year = {2017}
}
@article{Ahmed2017,
abstract = {Interaction testing can be used to effectively detect faults that are otherwise difficult to find by other testing techniques. However, in practice, the input configurations of software systems are subjected to constraints, especially in the case of highly configurable systems. Handling constraints effectively and efficiently in combinatorial interaction testing is a challenging problem. Nevertheless, researchers have attacked this challenge through different techniques, and much progress has been achieved in the past decade. Thus, it is useful to reflect on the current achievements and shortcomings and to identify potential areas of improvements. This paper presents the first comprehensive and systematic literature study to structure and categorize the research contributions for constrained interaction testing. Following the guidelines of conducting a literature study, the relevant data are extracted from a set of 103 research papers belonging to constrained interaction testing. The topics addressed in constrained interaction testing research are classified into four categories of constraint test generation, application, generation and application, and model validation studies. The papers within each of these categories are extensively reviewed. Apart from answering several other research questions, this paper also discusses the applications of constrained interaction testing in several domains, such as software product lines, fault detection and characterization, test selection, security, and graphical user interface testing. This paper ends with a discussion of limitations, challenges, and future work in the area.},
author = {Ahmed, Bestoun S. and Zamli, Kamal Z. and Afzal, Wasif and Bures, Miroslav},
doi = {10.1109/ACCESS.2017.2771562},
issn = {21693536},
journal = {IEEE Access},
keywords = {Constrained combinatorial testing,Constrained interaction testing,Software testing,Test case design techniques,Test generation tools},
pages = {25706--25730},
publisher = {IEEE},
title = {{Constrained interaction testing: A systematic literature study}},
volume = {5},
year = {2017}
}
@article{Zhang2020,
abstract = {Fully black-box robotic testing is needed given the popularity of mobile applications. A critical constraining issue for generating graphical user interface (GUI) models is identifying isomorphic GUIs. We present a deep learningbased end-to-end trainable model to determine the similarity between GUIs and identify isomorphic GUIs.},
author = {Zhang, Tao and Liu, Ying and Gao, Jerry and Gao, Li Peng and Cheng, Jing},
doi = {10.1109/MS.2020.2987044},
issn = {19374194},
journal = {IEEE Software},
keywords = {Isomorphic GUI identification,automatic robotic black-box testing,element recognition,feature extraction,relative entropy},
number = {4},
pages = {67--74},
publisher = {IEEE},
title = {{Deep Learning-Based Mobile Application Isomorphic GUI Identification for Automated Robotic Testing}},
volume = {37},
year = {2020}
}
@inproceedings{Khamis2013,
abstract = {Software agents are the basic building blocks in many software systems especially those based on artificial intelligence methods, e.g., reinforcement learning based multi-agent systems (MASs). However, testing software agents is considered a challenging problem. This is due to the special characteristics of agents which include its autonomy, distributed nature, intelligence, and heterogeneous communication protocols. Following the test-driven development (TDD) paradigm, we present a framework that allows MAS developers to write test scenarios that test each agent individually. The framework relies on the concepts of building mock agents and testing common agent interaction design patterns. We analyze the most common agent interaction patterns including pair and mediation patterns in order to provide stereotype implementation for their corresponding test cases. These implementations serve as test building blocks and are provided as a set of ready-for-reuse components in our repository. This way, the developer can concentrate on testing the business logic itself and spare him/her the burden of implementing tests for the underlying agent interaction patterns. Our framework is based on standard components such as the JADE agent platform, the JUnit framework, and the eclipse plug-in architecture. In this paper, we present in details the design and function of the framework. We demonstrate how we can use the proposed framework to define more stereotypes in the code repository and provide a detailed analysis of the code coverage for our designed stereotype test code implementations. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Khamis, Mohamed A. and Nagi, Khaled},
booktitle = {Engineering Applications of Artificial Intelligence},
doi = {10.1016/j.engappai.2013.04.009},
issn = {09521976},
keywords = {Agent social design patterns,Code coverage,Code generation,Mock agent,Multi-agent unit tests,Test-driven development},
number = {9},
pages = {2128--2142},
title = {{Designing multi-agent unit tests using systematic test design patterns-(extended version)}},
volume = {26},
year = {2013}
}
@article{Carver2010,
abstract = {Reachability testing is an approach to verifying concurrent programs. During reachability testing, every partially ordered synchronization sequence of a program with a given input is exercised exactly once. In this paper, we present the design and implementation of a distributed reachability testing algorithm for a cluster of workstations. This algorithm allows different test sequences to be exercised concurrently by different workstations without any synchronization, and without any duplication of sequences among workstations. Dynamic load balancing is performed using a work-stealing scheme. A novel aspect of this scheme is that work-stealing requests progress in rounds. This round-based structure identifies overloaded workstations to target for work stealing. Empirical studies show good speedup for four benchmark Java programs and one Lotos specification. {\textcopyright} 2010 John Wiley & Sons, Ltd.},
author = {Carver, Richard H. and Lei, Yu},
doi = {10.1002/cpe.1573},
issn = {15320634},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {Concurrent programming,Reachability testing,Software testing},
number = {18},
pages = {2445--2466},
publisher = {Wiley Online Library},
title = {{Distributed reachability testing of concurrent programs}},
volume = {22},
year = {2010}
}
@article{Priyanka2012,
abstract = {Software Testing is a challenging activity for many software engineering projects, especially for large scale systems. The amount of tests cases can range from a few hundred to several thousands, requiring significant computing resources and lengthy execution times. Cloud computing offers the potential to address both of these issues: it offers resources such as virtualized hardware, effectively unlimited storage, and software services that can aid in reducing the execution time of large test suites in a cost-effective manner. In this paper we report on a systematic review of cloud based testing techniques published in major software engineering journals and conferences conducted by other researchers. Research papers were gathered from various scholarly databases using provided search engines within a given period of time. A total of 82 research papers are analyzed in this systematic review and we classified it into four categories according to issues addressed by them. We identified majority of the research papers focused on Cloud based Testing and Issues (38 papers) and 23 papers focused on Cloud based Testing Frameworks. By looking at the areas focused by existing researchers, gaps and untouched areas of cloud based testing can be discovered},
author = {Priyanka and Chana, Inderveer and Rana, Ajay},
isbn = {0163-5948},
issn = {0163-5948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {cloud testing,cloud-based testing,metamorphic testing,performance testing,privacy-aware testing,security testing,software testing,symbolic execution,testing cloud services},
number = {3},
pages = {1--9},
publisher = {ACM New York, NY, USA},
title = {{Empirical evaluation of cloud-based testing techniques: a systematic review}},
url = {http://dl.acm.org.eaccess.ub.tum.de/citation.cfm?id=2180921.2180938},
volume = {37},
year = {2012}
}
@article{Khan2018,
abstract = {Context Empirical studies are essential in evaluating the effectiveness of Model-based Testing (MBT) research and should be reported properly to ensure their replication and to highlight the strengths and limitations of the MBT techniques being evaluated. Researchers have proposed guidelines detailing what information should be reported when presenting empirical studies and what should be the structure of such primary studies. There is a need to evaluate the reporting quality of the empirical studies in MBT literature. Objective To evaluate the reporting quality of empirical studies in the model based testing domain; identifying where the reported studies fail to follow the proposed guidelines and finding frequently omitted details. As an auxiliary goal we aim to quantify the percentage of empirical studies conducted in industrial context. Method We evaluate the reporting quality and the execution contexts of MBT empirical studies reported in literature. For our study we consider the MBT papers published in top ten software engineering journals over the last eighteen years. We evaluate the published primary studies using the empirical study reporting guidelines. Results We found 87 empirical in MBT that met our selection criteria. Initial results showed that the existing guidelines were not only too strict (for example they demand presence of specific sections rather than simply having the details present in the paper), they also did not adequately cover MBT specific details. Therefore, we propose modified the guidelines for reporting empirical studies in MBT and re-evaluated the selected studies. Results show that while only a few empirical studies follow the exact structure proposed by the guidelines, approximately half the papers contain at least 50% of the required details. Most of the papers omit details related to process and analysis leading to presented results. We found a positive trend of improving reporting quality of empirical studies in MBT over the last Eighteen years. Another important finding from the review is that few reported studies were conducted in real industrial context. Conclusions Model based testing community needs to be more aware of the reporting guidelines and more effort should be spent on reporting the necessary details. Furthermore, we found that only few studies that are conducted in industrial context and hence more focus should be given to empirical case studies in real industry context. However, the reporting quality of research papers presenting empirical evaluations is gradually improving.},
author = {Khan, Muhammad Uzair and Iftikhar, Sidra and Iqbal, Muhammad Zohaib and Sherin, Salman},
doi = {10.1016/j.csi.2017.08.002},
issn = {09205489},
journal = {Computer Standards and Interfaces},
keywords = {Empirical study,Model based testing,Reporting guidelines,Reporting quality},
pages = {156--170},
publisher = {Elsevier},
title = {{Empirical studies omit reporting necessary details: A systematic literature review of reporting quality in model based testing}},
volume = {55},
year = {2018}
}
@article{Iqbal2015,
abstract = {Given the challenges of testing at the system level, only a fully automated approach can really scale up to industrial real-time embedded systems (RTES). Our goal is to provide a practical approach to the model-based testing of RTES by allowing system testers, who are often not familiar with the system's design but are application domain experts, to model the system environment in such a way as to enable its black-box test automation. Environment models can support the automation of three tasks: the code generation of an environment simulator to enable testing on the development platform or without involving actual hardware, the selection of test cases, and the evaluation of their expected results (oracles). From a practical standpoint—and such considerations are crucial for industrial adoption—environment modeling should be based on modeling standards (1) that are at an adequate level of abstraction, (2) that software engineers are familiar with, and (3) that are well supported by commercial or open source tools. In this paper, we propose a precise environment modeling methodology fitting these requirements and discuss how these models can be used to generate environment simulators. The environment models are expressed using UML/MARTE and OCL, which are international standards for real-time systems and constraint modeling. The presented techniques are evaluated on a set of three artificial problems and on two industrial RTES.},
author = {Iqbal, Muhammad Zohaib and Arcuri, Andrea and Briand, Lionel},
doi = {10.1007/s10270-013-0328-6},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Automated testing,Environment modeling,Environment simulation,Model-based testing,Real-time embedded systems,Search based software engineering},
number = {1},
pages = {483--524},
publisher = {Springer},
title = {{Environment modeling and simulation for automated testing of soft real-time embedded software}},
volume = {14},
year = {2015}
}
@article{Gario2018,
abstract = {This paper proposes an approach for testing of safety-critical systems. It is based on a behavioral and a fault model. The two models are analyzed for compatibility, and necessary changes are identified to make them compatible. Then, transformation rules are used to transform the fault model into the same model type as the behavioral model. Integration rules define how to combine them. This approach results in an integrated model which then can be used to generate tests using a variety of testing criteria. The paper illustrates this general framework using a CEFSM for the behavioral model and a fault tree for the fault model. We apply the technique to an aerospace launch system. We also investigate the scalability of the approach and compare its efficiency with integrating a state chart and a fault tree.},
author = {Gario, Ahmed and Andrews, Anneliese and Hagerman, Seana},
doi = {10.1007/s11219-015-9283-5},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Behavioral model,CEFSM,FTA,Fault model,Finite-state machine,Integration,Safety-critical,Testing},
month = {mar},
number = {1},
pages = {3--48},
publisher = {Springer New York LLC},
title = {{Fail-safe testing of safety-critical systems: a case study and efficiency analysis}},
volume = {26},
year = {2018}
}
@article{Qian2018,
abstract = {Fault injection has already been used to access the dependability of web services. However, most of the existing work focuses on how to inject faults. Problems such as where to inject faults and what faults should be injected still have not been systematically studied in literature, especially for the testing of performance related issues in composite web services. This paper presents an approach that defines coverage criteria to guide fault injection testing of performance related issues in composite web services. We generate fault injection configurations that follows the defined test criteria for systematic fault injection. The configurations specify where to inject faults and what faults should be injected, and the injected faults (e.g. message delays) are generated according to the characteristics of each individual sub-service in order to make the faults more realistic. With the fault injection configurations, the fault injection process can be automatically conducted and the performance of a composite service can be effectively evaluated.},
author = {Qian, Ju and Wu, Han and Chen, Hao and Li, Changjian and Li, Weiwei},
doi = {10.23940/ijpe.18.06.p23.13141323},
issn = {09731318},
journal = {International Journal of Performability Engineering},
keywords = {Fault injection,Performance,Web service},
number = {6},
pages = {1314--1323},
title = {{Fault injection for performance testing of composite web services}},
volume = {14},
year = {2018}
}
@article{Timo2019,
abstract = {Test generation based on one-by-one analysis of potential implementations in fault models is challenging; it is indeed impossible or inefficient to enumerate each and every implementation, even when a fault model defines a finite but a significant number of implementations. We propose an approach for fault model and constraint solving-based testing from a particular type of extended finite state machines called a symbolic input finite state machine (SIFSM). Transitions in SIFSMs are labeled with symbolic inputs, which are predicates on input variables having possibly infinite domains. Its implementations, mutants, are also represented by SIFSMs. The generated tests are complete in a given fault domain which is a set of mutants specified with a so-called mutation machine. We define a well-formed mutation SIFSM for describing various types of faults. Given a mutation SIFSM, we develop methods for evaluating the adequacy of a test suite and generating complete tests. Experimental results with the prototype tool we have developed indicate that the approach is applicable to industrial-like systems.},
author = {Timo, Omer Nguena and Petrenko, Alexandre and Ramesh, S.},
doi = {10.1007/s11219-019-9440-3},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Conformance testing,Constraint solving,Extended FSM,Fault model-based test generation,Mutation testing},
number = {2},
pages = {501--527},
publisher = {Springer},
title = {{Fault model-driven testing from FSM with symbolic inputs}},
volume = {27},
year = {2019}
}
@article{Peleska2019,
abstract = {In this paper, new contributions for model-based testing using Communicating Sequential Processes (CSP)are presented. For a finite non-terminating CSP process representing the reference model, finite test suites for checking the conformance relations traces and failures refinement are presented, and their completeness (that is, capability to uncover conformity violations)is proven. The fault domains for which complete failure detection can be guaranteed are specified by means of normalised transition graphs representing the failures semantics of finite-state CSP processes. While complete test suites for CSP processes have been previously investigated by several authors, a sufficient condition for their finiteness is presented here for the first time. Moreover, it is shown that the test suites are optimal in two aspects: (a)the maximal length of test traces cannot be further reduced, and (b)the nondeterministic behaviour cannot be tested with smaller or fewer sets of events, without losing the test suite's completeness property.},
author = {Peleska, Jan and ling Huang, Wen and Cavalcanti, Ana},
doi = {10.1016/j.scico.2019.04.004},
issn = {01676423},
journal = {Science of Computer Programming},
keywords = {CSP,Complete test suites,Failures refinement,Model-based testing,Traces refinement},
pages = {1--23},
publisher = {Elsevier},
title = {{Finite complete suites for CSP refinement testing}},
volume = {179},
year = {2019}
}
@article{Brucker2015,
abstract = {Firewalls are an important means to secure critical ICT infrastructures. As configurable off-the-shelf products, the effectiveness of a firewall crucially depends on both the correctness of the implementation itself as well as the correct configuration. While testing the implementation can be done once by the manufacturer, the configuration needs to be tested for each application individually. This is particularly challenging as the configuration, implementing a firewall policy, is inherently complex, hard to understand, administrated by different stakeholders and thus difficult to validate. This paper presents a formal model of both stateless and stateful firewalls (packet filters), including NAT, to which a specification-based conformance test case generation approach is applied. Furthermore, a verified optimisation technique for this approach is presented: starting from a formal model for stateless firewalls, a collection of semantics-preserving policy transformation rules and an algorithm that optimizes the specification with respect of the number of test cases required for path coverage of the model are derived. We extend an existing approach that integrates verification and testing, that is, tests and proofs to support conformance testing of network policies. The presented approach is supported by a test framework that allows to test actual firewalls using the test cases generated on the basis of the formal model. Finally, a report on several larger case studies is presented.},
author = {Brucker, Achim D. and Br{\"{u}}gger, Lukas and Wolff, Burkhart},
doi = {10.1002/stvr.1544},
issn = {10991689},
journal = {Software Testing Verification and Reliability},
keywords = {Conformance testing,Firewall,HOL-Testgen,Model-based testing,Security configuration testing,Security testing,Specification-based testing,Test and proof,Testing cloud infrastructure,Transformation for testability},
number = {1},
pages = {34--71},
publisher = {Wiley Online Library},
title = {{Formal firewall conformance testing: An application of test and proof techniques}},
volume = {25},
year = {2015}
}
@article{Nguyen2014,
abstract = {Most of today's software applications feature a graphical user interface (GUI) front-end. System testing of these applications requires that test cases, modeled as sequences of GUI events, be generated and executed on the software. We term GUI testing as the process of testing a software application through its GUI. Researchers and practitioners agree that one must employ a variety of techniques (e.g., model-based, capture/replay, manually scripted) for effective GUI testing. Yet, the tools available today for GUI testing are limited in the techniques they support. In this paper, we describe an innovative tool called GUITAR that supports a wide variety of GUI testing techniques. The innovation lies in the architecture of GUITAR, which uses plug-ins to support flexibility and extensibility. Software developers and quality assurance engineers may use this architecture to create new toolchains, new workflows based on the toolchains, and plug in a variety of measurement tools to conduct GUI testing. We demonstrate these features of GUITAR via several carefully crafted case studies. {\textcopyright} Springer Science+Business Media New York 2013.},
author = {Nguyen, Bao N. and Robbins, Bryan and Banerjee, Ishan and Memon, Atif},
doi = {10.1007/s10515-013-0128-9},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {GUI testing,Test automation,Test generation},
number = {1},
pages = {65--105},
publisher = {Springer},
title = {{GUITAR: An innovative tool for automated testing of GUI-driven software}},
volume = {21},
year = {2014}
}
@article{Hierons2017,
abstract = {We present a complete framework to formally test systems with distributed ports where some choices are probabilistically quantified while other choices are non-deterministic. We define different implementation relations, that is, relations that state what it means for a system to be a valid implementation of a specification. We also study how these relate. In order to define these implementation relations we use probabilistic schedulers, a more powerful version, including probabilistic choices, of a notion of scheduler introduced in our previous work. Probabilistic schedulers, when applied to either a specification or an implementation, resolve all the possible non-determinism, so that we can compare purely probabilistic systems.},
author = {Hierons, Robert M. and N{\'{u}}{\~{n}}ez, Manuel},
doi = {10.1016/j.jss.2017.03.011},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Distributed systems,Model-based testing,Probabilistic systems},
pages = {319--335},
publisher = {Elsevier},
title = {{Implementation relations and probabilistic schedulers in the distributed test architecture}},
volume = {132},
year = {2017}
}
@article{Kourtesis2010,
abstract = {Organisations wishing to engage in industrial collaborative networks will typically seek some guarantees concerning the reliability of their prospective partners before committing to cooperation. Evaluating reliability can encompass several aspects, but one of the most crucial things to consider from a cooperation perspective is whether the software systems that support the business processes of some collaborator actually behave as expected. For organisations that rely on a service-oriented computing infrastructure, this amounts to checking whether the functionality of the respective services is conformant to a given behavioural specification. Today's state of the art lacks standardised methods for creating behavioural specifications of Web services, and also lacks tools for automating the process of behavioural conformance checking through testing. This paper presents a concrete method for creating formal specifications of Web service behaviour and utilising them within service registries for automated testing of service implementations in order to verify and certify their conformance.},
author = {Kourtesis, D. and Ramollari, E. and Dranidis, D. and Paraskakis, I.},
doi = {10.1080/09537280903441922},
issn = {09537287},
journal = {Production Planning and Control},
keywords = {Behavioural conformance,Registry,Testing,Web services},
number = {2},
pages = {130--144},
publisher = {Taylor & Francis},
title = {{Increased reliability in SOA environments through registry-based conformance testing of Web services}},
volume = {21},
year = {2010}
}
@article{Tsai2016,
abstract = {Testing-as-a-Service (TaaS) is a software testing service in a cloud that can leverage the computation power provided by the cloud. Specifically, a TaaS can be scaled to large and dynamic workloads, executed in a distributed environment with hundreds of thousands of processors, and these processors may support concurrent and distributed test execution and analysis. This paper proposes a TaaS system based on Adaptive Reasoning (AR) and Test Algebra (TA) for Combinatorial Testing (CT). AR performs testing and identifies faulty interactions, and TA eliminates related configurations from testing and there can be carried out concurrently. By combining these two, it is possible to perform large CT that were not possible before. Specifically, we performed experiments with 250 components with 2.83*1087 6-way interactions with about 21.1×1015 configurations, and this may be the largest CT experimentation as 2014. 98.6% of configurations have been eliminated out of total number of configurations.},
author = {Tsai, Wei Tek and Qi, Guanqiu},
doi = {10.1016/j.simpat.2016.08.003},
issn = {1569190X},
journal = {Simulation Modelling Practice and Theory},
keywords = {Adaptive reasoning,Combinatorial testing,Concurrent testing,TaaS,Test algebra},
pages = {108--124},
publisher = {Elsevier},
title = {{Integrated fault detection and test algebra for combinatorial testing in TaaS (Testing-as-a-Service)}},
volume = {68},
year = {2016}
}
@article{Felderer2014a,
abstract = {Risk-based testing has a high potential to improve the software development and test process as it helps to optimize the allocation of resources and provides decision support for the management. But for many organizations, its integration into an existing test process is a challenging task. In this article, we provide a comprehensive overview of existing work and present a generic testing methodology enhancing an established test process to address risks. On this basis, we develop a procedure on how risk-based testing can be introduced in a test process and derive a stage model for its integration. We then evaluate our approach for introducing risk-based testing by means of an industrial study and discuss benefits, prerequisites and challenges to introduce it. Potential benefits of risk-based testing identified in the studied project are faster detection of defects resulting in an earlier release, a more reliable release quality statement as well as the involved test-process optimization. As necessary prerequisites for risk-based testing, we identified an inhomogeneous distribution of risks associated with the various parts of the tested software system as well as consolidated technical and business views on it. Finally, the identified challenges of introducing risk-based testing are reliable risk assessment in the context of complex systems, the availability of experts for risk assessment as well as established tool supports for test management.},
author = {Felderer, Michael and Ramler, Rudolf},
doi = {10.1007/s11219-013-9226-y},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Risk-based testing,Software risk management,Software testing,System testing,Test management,Test-process improvement},
number = {3},
pages = {543--575},
publisher = {Springer},
title = {{Integrating risk-based testing in industrial test processes}},
volume = {22},
year = {2014}
}
@article{Foidl2019,
abstract = {This summary refers to the paper 'Integrating software quality models into risk-based testing' [FF18]. The paper was published as an article in the Software Quality Journal. It shows for the first time how to integrate software quality models into risk-based testing.},
author = {Foidl, Harald and Felderer, Michael},
doi = {10.18420/se2019-54},
isbn = {9783885796862},
issn = {16175468},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
keywords = {Risk-based testing,Software quality,Software quality models,Software risk management,Software testing},
number = {2},
pages = {173--174},
publisher = {Springer},
title = {{Integrating software quality models into risk-based testing}},
volume = {P-292},
year = {2019}
}
@article{Aichernig2015,
abstract = {This article presents the techniques and results of a novel model-based test case generation approach that automatically derives test cases from UML state machines. The main contribution of this article is the fully automated fault-based test case generation technique together with two empirical case studies derived from industrial use cases. Also, an in-depth evaluation of different fault-based test case generation strategies on each of the case studies is given and a comparison with plain random testing is conducted. The test case generation methodology supports a wide range of UML constructs and is grounded on the formal semantics of Back's action systems and the well-known input-output conformance relation. Mutation operators are employed on the level of the specification to insert faults and generate test cases that will reveal the faults inserted. The effectiveness of this approach is shown and it is discussed how to gain a more expressive test suite by combining cheap but undirected random test case generation with the more expensive but directed mutation-based technique. Finally, an extensive and critical discussion of the lessons learnt is given as well as a future outlook on the general usefulness and practicability of mutation-based test case generation.},
author = {Aichernig, Bernhard K. and Brandl, Harald and J{\"{o}}bstl, Elisabeth and Krenn, Willibald and Schlick, Rupert and Tiran, Stefan},
doi = {10.1002/stvr.1522},
issn = {10991689},
journal = {Software Testing Verification and Reliability},
keywords = {UML,Unified Modeling Language,action systems,ioco,model-based testing,mutation testing,random testing,test case generation},
number = {8},
pages = {716--748},
publisher = {Wiley Online Library},
title = {{Killing strategies for model-based mutation testing}},
volume = {25},
year = {2015}
}
@article{Segura2018,
abstract = {Web Application Programming Interfaces (APIs) allow systems to interact with each other over the network. Modern Web APIs often adhere to the REST architectural style, being referred to as RESTful Web APIs. RESTful Web APIs are decomposed into multiple resources (e.g., a video in the YouTube API) that clients can manipulate through HTTP interactions. Testing Web APIs is critical but challenging due to the difficulty to assess the correctness of API responses, i.e., the oracle problem. Metamorphic testing alleviates the oracle problem by exploiting relations (so-called metamorphic relations) among multiple executions of the program under test. In this paper, we present a metamorphic testing approach for the detection of faults in RESTful Web APIs. We first propose six abstract relations that capture the shape of many of the metamorphic relations found in RESTful Web APIs, we call these Metamorphic Relation Output Patterns (MROPs). Each MROP can then be instantiated into one or more concrete metamorphic relations. The approach was evaluated using both automatically seeded and real faults in six subject Web APIs. Among other results, we identified 60 metamorphic relations (instances of the proposed MROPs) in the Web APIs of Spotify and YouTube. Each metamorphic relation was implemented using both random and manual test data, running over 4.7K automated tests. As a result, 11 issues were detected (3 in Spotify and 8 in YouTube), 10 of them confirmed by the API developers or reproduced by other users, supporting the effectiveness of the approach.},
author = {Segura, Sergio and Parejo, Jose A. and Troya, Javier and Ruiz-Cortes, Antonio},
doi = {10.1109/TSE.2017.2764464},
issn = {19393520},
journal = {IEEE Transactions on Software Engineering},
keywords = {Metamorphic testing,REST,RESTful Web services,Web API},
number = {11},
pages = {1083--1099},
publisher = {IEEE},
title = {{Metamorphic testing of RESTful Web APIs}},
volume = {44},
year = {2018}
}
@article{DeFrancesco2015,
abstract = {The increasing adoption of the service-oriented architecture approach makes software quality more challenging since traditional approaches for software testing are inadequate for SOA-based applications. The MIDAS platform is an integrated platform for SOA testing automation, designed and architected according to the SOA computing paradigm, and deployed on a public cloud infrastructure to tackle the variability in the computational resources necessary for testing SOA applications. It is available to end users as a testing as a service on a self-provisioning, pay-per-use, elastic basis. The cloud-based software architecture envisioned for the MIDAS platform and the strategies adopted for both its development, and its deployment on the target cloud infrastructure are here discussed, together with the solution designed and implemented in order to monitor the usage of MIDAS services and resources. Also, the strategy adopted to provide the MIDAS platform with the management of elastic resources is outlined.},
author = {{De Francesco}, Alberto and {Di Napoli}, Claudia and Giordano, Maurizio and Ottaviano, Giuseppe and Perego, Raffaele and Tonellotto, Nicola},
doi = {10.1504/IJHPCN.2015.071254},
issn = {17400570},
journal = {International Journal of High Performance Computing and Networking},
keywords = {Amazon AWS,Cloud computing,SOA,Service-oriented architecture,Software testing},
number = {3},
pages = {285--300},
publisher = {Inderscience Publishers (IEL)},
title = {{Midas: A cloud platform for SOA testing as a service}},
volume = {8},
year = {2015}
}
@inproceedings{Sadhukhan2011,
abstract = {The use of a model to describe the behavior of a system is a proven and major advantage to test development teams. Models can be utilized in many ways throughout the product life-cycle, including: improved quality of specifications, code generation, reliability analysis, and test generation. This paper will focus on the testing benefits and review some of the historical challenges that prevented model based testing and present the solutions that overcame these challenges. In addition benefits of a model based approach are reviewed in the context of two real applications, a call processing feature and a UI of a workflow oriented database system.},
author = {Sadhukhan, Bhaswati},
booktitle = {Software quality week conference},
pages = {1--5},
title = {{Model Based Testing Practices}},
year = {2011}
}
@article{Enoiu2020,
abstract = {Nowadays, embedded systems are increasingly complex, meaning that traditional testing methods are costly to use and infeasible to directly apply due to the complex interactions between hardware and software. Modern embedded systems are also demanded to function based on low-energy computing. Hence, testing the energy usage is increasingly important. Artifacts produced during the development of embedded systems, such as architectural descriptions, are beneficial abstractions of the system's complex structure and behavior. Electronic Architecture and Software Tools Architecture Description Language (EAST-ADL) is one such example of a domain-specific architectural language targeting the automotive industry. In this paper, we propose a method for testing design models using EAST-ADL architecture mutations. We show how fault-based testing can be used to generate, execute and select tests using energy-aware mutants—syntactic changes in the architectural description, used to mimic naturally occurring energy faults. Our goal is to improve testing of complex embedded systems by moving the testing bulk from the actual systems to models of their behaviors and non-functional requirements. We combine statistical model-checking, increasingly used in quality assurance of embedded systems, with EAST-ADL architectural models and mutation testing to drive the search for faults. We show the results of applying this method on an industrial-sized system developed by Volvo GTT. The results indicate that model testing of EAST-ADL architectural models can reduce testing complexity by bringing early and cost-effective automation.},
author = {Enoiu, Eduard Paul and Seceleanu, Cristina},
doi = {10.3390/designs4010005},
issn = {24119660},
journal = {Designs},
keywords = {EAST-ADL,Energy consumption,Model testing,Mutation testing},
number = {1},
pages = {1--18},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Model testing of complex embedded systems using EAST-ADL and energy-aware mutations}},
volume = {4},
year = {2020}
}
@article{Ali2010,
author = {Ali, S and Hemmati, H and Holt, Ne and Arisholm, E and Briand, L.},
journal = {Simula Research Laboratory, Technical Report (2010-01)},
keywords = {automatic test-case generation,configurability,development,extensibility,model transformation,model-based testing tool,model-driven,state-based system,uml state machine},
pages = {1--28},
publisher = {Citeseer},
title = {{Model Transformations as a Strategy to Automate Model-Based Testing-A Tool and Industrial Case Studies}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.999&rep=rep1&type=pdf},
year = {2010}
}
@article{Karlsson2021,
abstract = {Automatic testing of mobile applications has been a well-researched area in recent years. However, testing in industry is still a very manual practice, as research results have not been fully transferred and adopted. Considering mobile applications, manual testing has the additional burden of adequate testing posed by a large number of available devices and different configurations, as well as the maintenance and setup of such devices.In this paper, we propose and evaluate the use of a model-based test generation approach, where generated tests are executed on a set of cloud-hosted real mobile devices. By using a model-based approach we generate dynamic, less brittle, and implementation simple test cases. The test execution on multiple real devices with different configurations increase the confidence in the implementation of the system under test. Our evaluation shows that the used approach produces a high coverage of the parts of the application related to user interactions. Nevertheless, the inclusion of external services in test generation is required in order to additionally increase the coverage of the complete application. Furthermore, we present the lessons learned while transferring and implementing this approach in an industrial context and applying it to the real product.},
archivePrefix = {arXiv},
arxivId = {2008.08859},
author = {Karlsson, Stefan and Causevic, Adnan and Sundmark, Daniel and Larsson, Marten},
doi = {10.1109/ICSTW52544.2021.00033},
eprint = {2008.08859},
isbn = {9781665444569},
issn = {23318422},
journal = {Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2021},
keywords = {Automated testing,Model-based testing,Test-case generation},
pages = {130--137},
title = {{Model-based automated testing of mobile applications: An industrial case study}},
year = {2021}
}
@article{Padgham2013,
abstract = {Software testing remains the most widely used approach to verification in industry today, consuming between 30-50 percent of the entire development cost. Test input selection for intelligent agents presents a problem due to the very fact that the agents are intended to operate robustly under conditions which developers did not consider and would therefore be unlikely to test. Using methods to automatically generate and execute tests is one way to provide coverage of many conditions without significantly increasing cost. However, one problem using automatic generation and execution of tests is the oracle problem: How can we automatically decide if observed program behavior is correct with respect to its specification? In this paper, we present a model-based oracle generation method for unit testing belief-desire-intention agents. We develop a fault model based on the features of the core units to capture the types of faults that may be encountered and define how to automatically generate a partial, passive oracle from the agent design models. We evaluate both the fault model and the oracle generation by testing 14 agent systems. Over 400 issues were raised, and these were analyzed to ascertain whether they represented genuine faults or were false positives. We found that over 70 percent of issues raised were indicative of problems in either the design or the code. Of the 19 checks performed by our oracle, faults were found by all but 5 of these checks. We also found that 8 out the 11 fault types identified in our fault model exhibited at least one fault. The evaluation indicates that the fault model is a productive conceptualization of the problems to be expected in agent unit testing and that the oracle is able to find a substantial number of such faults with relatively small overhead in terms of false positives. {\textcopyright} 1976-2012 IEEE.},
author = {Padgham, Lin and Zhang, Zhiyong and Thangarajah, John and Miller, Tim},
doi = {10.1109/TSE.2013.10},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {BDI agents,Test oracles,unit testing},
number = {9},
pages = {1230--1244},
publisher = {IEEE},
title = {{Model-based test oracle generation for automated unit testing of agent systems}},
volume = {39},
year = {2013}
}
@article{Villalobos-Arias2019,
abstract = {Context: Model-based testing is one of the most studied approaches by secondary studies in the area of software testing. Aggregating knowledge from secondary studies on model- based testing can be useful for both academia and industry. Objective: The goal of this study is to characterize secondary studies in model-based testing, in terms of the areas, tools and challenges they have investigated. Method: We conducted a tertiary study following the guidelines for systematic mapping studies. Our mapping included 22 secondary studies, of which 12 were literature surveys and 10 systematic reviews, over the period 1996–2016. Results: A hierarchy of model-based testing areas and subareas was built based on existing taxonomies as well as data that emerged from the secondary studies themselves. This hierarchy was then used to classify studies, tools, challenges and their tendencies in a unified classification scheme. We found that the two most studied areas are UML models and transition-based notations, both being modeling paradigms. Regarding tendencies of areas in time, we found two areas with constant activity through time, namely, test objectives and model specification. With respect to tools, we only found five studies that compared and classified model-based testing tools. These tools have been classified into common dimensions that mainly refer to the model type and phases of the model-based testing process they support. We reclassified all the tools into the hierarchy of model-based testing areas we proposed, and found that most tools were reported within the modeling paradigm area. With regard to tendencies of tools, we found that tools for testing the functional behavior of software have prevailed over time. Another finding was the shift from tools that support the generation of abstract tests to those that support the generation of executable tests. For analyzing challenges, we used six categories that emerged from the data (based on a grounded analysis): efficacy, availability, complexity, professional skills, investment, cost & effort, and evaluation & empirical evidence. We found that most challenges were related to availability. Besides, we too classified challenges according to our hierarchy of model-based testing areas, and found that most challenges fell in the model specification area. With respect to tendencies in challenges, we found they have moved from complexity of the approaches to the lack of approaches for specific software domains. Conclusions: Only a few systematic reviews on model-based testing could be found, therefore some areas still lack secondary studies, particularly, test execution aspects, language types, model dynamics, as well as some modeling paradigms and generation methods. We thus encourage the community to perform further systematic reviews and mapping studies, following known protocols and reporting procedures, in order to increase the quality and quantity of empirical studies in model-based testing.},
author = {Villalobos-Arias, Leonardo and Quesada-L{\'{o}}pez, Christan and Martinez, Alexandra and Jenkins, Marcelo},
doi = {10.19153/cleiej.22.1.3},
issn = {0717-5000},
journal = {CLEI Electronic Journal},
number = {1},
pages = {1--3},
publisher = {Centro Latino Americano de Estudios en Informatica},
title = {{Model-based testing areas, tools and challenges: A tertiary study}},
volume = {22},
year = {2019}
}
@misc{Krichen2020,
author = {Krichen, Moez},
publisher = {ReDCAD Laboratory},
title = {{Contributions to Model-Based Testing of Dynamic and Distributed Real-Time Systems}},
year = {2020}
}
@inproceedings{Pinheiro2010,
abstract = {Service-Oriented Architecture is a well-known architectural style that promotes many benefits among enterprise systems. In the last years, an alternative architectural style, so-called REST, has been proposed and widely adopted to design services' capabilities as resources. However, when it comes to verify these services, many challenges arise and hinder the process of testing. This paper proposes a model-based approach to test RESTful Web services using the UML protocol state machine as the formal behavioral model. A tool was developed to support the approach by automatically generating test cases for state and transition coverage criteria. An example is presented to illustrate the practical application of the approach.},
author = {Pinheiro, PVP and Endo, AT and Simao, Adenilso},
booktitle = {Sjc.Unifesp.Br},
number = {Icmc},
pages = {1--10},
publisher = {Citeseer},
title = {{Model-Based Testing of RESTful Web Services Using UML Protocol State Machines}},
year = {2010}
}
@misc{Petrenko2012,
abstract = {Model-based testing is focused on testing techniques which rely on the use of models. The diversity of systems and software to be tested implies the need for research on a variety of models and methods for test automation. We briefly review this research area and introduce several papers selected from the 22nd International Conference on Testing Software and Systems (ICTSS). {\textcopyright} 2012 Springer-Verlag.},
author = {Petrenko, Alexandre and Simao, Adenilso and Maldonado, Jos{\'{e}} Carlos},
booktitle = {International Journal on Software Tools for Technology Transfer},
doi = {10.1007/s10009-012-0240-3},
isbn = {1433-2787},
issn = {14332779},
keywords = {Model-based testing,Software testing},
number = {4},
pages = {383--386},
publisher = {Springer},
title = {{Model-based testing of software and systems: Recent advances and challenges}},
volume = {14},
year = {2012}
}
@incollection{Huang2018,
abstract = {Model-based testing in its most advanced form allows for automated test case identification, test data calculation, and test procedure generation from reference models describing the expected behaviour of the system under test. If the underlying algorithms for test case identification operate only on the syntactic representation of test models, however, the resulting test strength depends on the syntactic representation as well. This observation is true, even if syntactically differing models are behaviourally equivalent. In this paper, we present a systematic approach to elaborating test case selection strategies that only depend on the behavioural semantics of test models, but are invariant under syntactic transformations preserving the semantics. The benefits of these strategies are discussed, and practical generation algorithms are presented.},
author = {ling Huang, Wen and Peleska, Jan},
booktitle = {International Journal on Software Tools for Technology Transfer},
doi = {10.1007/s10009-017-0479-9},
issn = {14332787},
keywords = {Complete testing theories,Equivalence class partition testing,Languages,Model-based testing},
number = {4},
pages = {441--465},
publisher = {Springer},
title = {{Model-based testing strategies and their (in)dependence on syntactic model representations}},
volume = {20},
year = {2018}
}
@article{An2013,
abstract = {The Object Management Group's (OMG) Data Distribution Service (DDS) provides many configurable policies which determine end-to-end quality of service (QoS) of applications. It is challenging to predict the system's performance in terms of latencies, throughput, and resource usage because diverse combinations of QoS configurations influence QoS of applications in different ways. To overcome this problem, design-time formal methods have been applied with mixed success, but lack of sufficient accuracy in prediction, tool support, and understanding of formalism has prevented wider adoption of the formal techniques. A promising approach to address this challenge is to emulate system behavior and gather data on the QoS parameters of interest by experimentation. To realize this approach, which is preferred over formal methods due to their limitations in accurately predicting QoS, we have developed a model-based automatic performance testing framework with generative capabilities to reduce manual efforts in generating a large number of relevant QoS configurations that can be deployed and tested on a cloud platform. This paper describes our initial efforts in developing and using this technology. {\textcopyright} 2013 ACM.},
author = {An, Kyoungho and Kuroda, Takayuki and Gokhale, Aniroddha and Tambe, Sumant and Sorbini, Andrea},
doi = {10.1145/2517208.2517216},
isbn = {9781450323734},
issn = {0362-1340},
journal = {SPLASH Indianapolis 2013; GPCE 2013 - Proceedings of the 12th International Conference on Generative Programming: Concepts and Experiences},
keywords = {generative programming,model-driven engineering,performance testing,publish/subscribe},
number = {3},
pages = {179--182},
publisher = {ACM New York, NY, USA},
title = {{Model-driven generative framework for automated OMG DDS performance testing in the cloud}},
volume = {49},
year = {2013}
}
@article{Tao2017,
abstract = {With the rapid advance of mobile computing, cloud computing and wireless network, there is a significant increasing number of mobile subscriptions. This brings new business requirements and demands in mobile testing service, and causes new issues and challenges. In this paper, informative discussions about cloud-based mobile testing-as-a-service (mobile TaaS) are offered, including the essential concepts, focuses, test process, and the expected testing infrastructures. To address the need of infrastructure level service for mobile TaaS, this paper presents a developed system known as MTaaS to provide an infrastructure-as-a-service (IaaS) for mobile testing, in order to indicate the feasibility and effectiveness of cloud-based mobile testing service. In addition, the paper presents a comparison among cloud-based mobile TaaS approaches and several best practices in industry are discussed. Finally, the primary issues, challenges, and needs existed in current mobile TaaS are analyzed.},
author = {Tao, Chuanqi and Gao, Jerry},
doi = {10.1016/j.jss.2016.11.016},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Cloud-based infrastructure -as-a-service,Mobile application testing,Mobile testing as a service},
pages = {39--55},
publisher = {Elsevier},
title = {{On building a cloud-based mobile testing infrastructure service system}},
volume = {124},
year = {2017}
}
@article{Cartaxo2011,
abstract = {Test case selection in model-based testing is discussed focusing on the use of a similarity function. Automatically generated test suites usually have redundant test cases. The reason is that test generation algorithms are usually based on structural coverage criteria that are applied exhaustively. These criteria may not be helpful to detect redundant test cases as well as the suites are usually impractical due to the huge number of test cases that can be generated. Both problems are addressed by applying a similarity function. The idea is to keep in the suite the less similar test cases according to a goal that is defined in terms of the intended size of the test suite. The strategy presented is compared with random selection by considering transition-based and fault-based coverage. The results show that, in most of the cases, similarity-based selection can be more effective than random selection when applied to automatically generated test suites. Copyright {\textcopyright} 2009 John Wiley & Sons, Ltd.},
author = {Cartaxo, Emanuela G. and MacHado, Patr{\'{i}}cia D.L. and Neto, Francisco G.Oliveira},
doi = {10.1002/stvr.413},
issn = {09600833},
journal = {Software Testing Verification and Reliability},
keywords = {LTS,model-based testing,test case selection},
number = {2},
pages = {75--100},
publisher = {Wiley Online Library},
title = {{On the use of a similarity function for test case selection in the context of model-based testing}},
volume = {21},
year = {2011}
}
@article{DalalanaBertoglio2017,
abstract = {Several studies regarding security testing for corporate environments, networks, and systems were developed in the past years. Therefore, to understand how methodologies and tools for security testing have evolved is an important task. One of the reasons for this evolution is due to penetration test, also known as Pentest. The main objective of this work is to provide an overview on Pentest, showing its application scenarios, models, methodologies, and tools from published papers. Thereby, this work may help researchers and people that work with security to understand the aspects and existing solutions related to Pentest. A systematic mapping study was conducted, with an initial gathering of 1145 papers, represented by 1090 distinct papers that have been evaluated. At the end, 54 primary studies were selected to be analyzed in a quantitative and qualitative way. As a result, we classified the tools and models that are used on Pentest. We also show the main scenarios in which these tools and methodologies are applied to. Finally, we present some open issues and research opportunities on Pentest.},
author = {{Dalalana Bertoglio}, Daniel and Zorzo, Avelino Francisco},
doi = {10.1186/s13173-017-0051-1},
issn = {16784804},
journal = {Journal of the Brazilian Computer Society},
keywords = {Penetration test,Security testing,Systematic mapping study},
number = {1},
pages = {1--16},
publisher = {SpringerOpen},
title = {{Overview and open issues on penetration test}},
volume = {23},
year = {2017}
}
@article{Letychevskyi2015,
abstract = {Model-based testing (MBT) of software systems is considered and well-known MBT methods and tools are reviewed. These methods make it possible to automate procedures performed by software test engineers and significantly increase the quality of testing software systems. The MBT technology, its components, and main tools are described. A list of problems arising in using MBT is compiled. A symbolic approach that makes it possible to solve some of existing MBT problems and the Generic Trace Generator based on the symbolic approach are presented. This tool was developed at the V. M. Glushkov Institute of Cybernetics. The symbolic approach is extended for the first time as a result of addition of the test execution stage that considerably improves the MBT technology.},
author = {Letychevskyi, O. O.},
doi = {10.1007/s10559-015-9761-6},
issn = {15738337},
journal = {Cybernetics and Systems Analysis},
keywords = {basic protocol,execution of tests,model-based testing,symbolic modeling,user scenario,verification},
number = {5},
pages = {692--703},
publisher = {Springer},
title = {{Paradigms of Model-Based and Symbolic Testing of Software Systems}},
volume = {51},
year = {2015}
}
@article{Leotta2018,
abstract = {Test automation tools are widely adopted for testing complex Web applications. Three generations of tools exist: first, based on screen coordinates; second, based on DOM–based commands; and third, based on visual image recognition. In our previous work, we proposed Pesto, a tool able to migrate second-generation Selenium WebDriver test suites towards third-generation Sikuli ones. In this work, we extend Pesto to manage Web elements having (1) complex visual interactions and (2) multiple visual appearances. Pesto relies on aspect-oriented programming, computer vision, and code transformations. Our new improved tool has been evaluated on two Web test suites developed by an independent tester. Experimental results show that Pesto manages and transforms correctly test suites with Web elements having complex visual interactions and multistate elements. By using Pesto, the migration of existing DOM–based test suites to the visual approach requires a low manual effort, since our approach proved to be very accurate.},
author = {Leotta, Maurizio and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
doi = {10.1002/stvr.1665},
issn = {10991689},
journal = {Software Testing Verification and Reliability},
keywords = {DOM-based testing,Selenium WebDriver,Sikuli,Web testing,test automation,visual testing},
number = {4},
pages = {e1665},
publisher = {Wiley Online Library},
title = {{Pesto: Automated migration of DOM-based Web tests towards the visual approach}},
volume = {28},
year = {2018}
}
@article{Barcelona2017,
abstract = {In this paper, we present the experience in the usage of MIDAS, an integrated framework for Service Oriented Architecture (SOA) testing automation that is available as Software as a Service (SaaS) in a cloud infrastructure, to test a GS1 Logistics Interoperability Model (GS1 LIM) compliant service architecture for the logistics domain. Activities performed, results achieved and the evaluation of success factors and key performance indicators (KPIs) are detailed as well as other insights: (1) 25 % of companies would pay for model-based testing (MBT), (2) GS1 LIM should be certifiable, and (3) companies identify as a major barrier how to calculate the MBT return on investment (ROI).},
author = {Barcelona, M. A. and Garc{\'{i}}a-Borgo{\~{n}}{\'{o}}n, L. and L{\'{o}}pez-Nicol{\'{a}}s, G.},
doi = {10.1007/s10009-016-0430-5},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Case study,Logistics domain,Model based testing,Testing automation},
month = {jun},
number = {3},
pages = {325--339},
publisher = {Springer Verlag},
title = {{Practical experiences in the usage of MIDAS in the logistics domain}},
volume = {19},
year = {2017}
}
@article{Han,
abstract = {Distributed systems are hard to build. Existing validation methods fall under proof-based verification of the high-level description and black box testing of the implementation. We introduce the use of P, a DSL for writing specifications which compile into executable production code, to build a distributed system. We develop the Raft protocol in P and extend P to interface the generated implementation with a key-value store application for proof of concept.},
author = {Han, Eliot and Sun, Andrew and Yeo, Alex and Zhang, James},
title = {{PRaft: Building Provably Safe Distributed Systems using Model Checking}}
}
@article{Andersen2021,
abstract = {Distributed systems are hard to get right, model, test, debug, and teach. Their textbook definitions, typically given in a form of replicated state machines, are concise, yet prone to introducing programming errors if na{\"{i}}vely translated into runnable implementations. In this work, we present Distributed Protocol Combinators (DPC), a declarative programming framework that aims to bridge the gap between specifications and runnable implementations of distributed systems, and facilitate their modeling, testing, and execution. DPC builds on the ideas from the state-of-the art logics for compositional systems verification. The contribution of DPC is a novel family of program-level primitives, which facilitates construction of larger distributed systems from smaller components, streamlining the usage of the most common asynchronous message-passing communication patterns, and providing machinery for testing and user-friendly dynamic verification of systems. This paper describes the main ideas behind the design of the framework and presents its implementation in Haskell. We introduce DPC through a series of characteristic examples and showcase it on a number of distributed protocols from the literature. This paper extends our preceeding conference publication (Andersen & Sergey, 2019a) with an exploration of randomized testing for protocols and their implementations, and an additional case study demonstrating bounded model checking of protocols.},
author = {Andersen, Kristoffer Just Arndal and Sergey, I. L.Y.A.},
doi = {10.1017/S095679682000026X},
issn = {14697653},
journal = {Journal of Functional Programming},
title = {{Protocol combinators for modeling, testing, and execution of distributed systems}},
year = {2021}
}
@article{Pietrantuono2020,
abstract = {We address the problem of operational reliability assessment through testing of software services delivered on-demand such as Web Services. Software reliability assessment is typically done for a specific operational profile: the profile is needed in testing to select or generate test cases (operational testing) in a way statistically similar to the anticipated use of software in operation; the observations of success/failure of test executions are used to predict software reliability in actual operation. It is well known that unless the profile is accurate, software reliability predictions obtained via operational testing cannot be trusted. We present a new way of dealing with the uncertainty in the operational profile adopting a two-stage Bayesian inference for reliability assessment. The technique relies on the availability of information about partitions of the input space. The approach is demonstrated on contrived examples and on a case study of real Web Services. We discuss the usefulness of the approach in dealing with two important practical problems: i) the true profile in operation differs from the one used in testing, ii) the profile in operation is changing continuously.},
author = {Pietrantuono, Roberto and Popov, Peter and Russo, Stefano},
doi = {10.1016/j.ress.2020.107193},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Bayes methods,Service-based software,Software reliability,Software testing,Web service},
pages = {107193},
publisher = {Elsevier},
title = {{Reliability assessment of service-based software under operational profile uncertainty}},
volume = {204},
year = {2020}
}
@article{Augusto2020,
abstract = {Continuous integration practice mandates to continuously introduce incremental changes into code, but doing so may introduce new faults too. These faults could be detected automatically through regression testing, but this practice becomes prohibitive as the cost of executing the tests grows. This problem is preponderant in end-to-end testing where the whole system is requested for test execution. However, some of these test cases could be executed with fewer resources (e.g., memory, web services, computation, Cloud instances, among others), by deploying only the subsystems needed by each test. This paper is focused on the optimization of the resources employed in end-to-end testing by means of a resource-aware test orchestration technique in the context of continuous integration practices in the Cloud. The RETORCH approach proposes a novel way to identify the resources required by end-to-end test cases and to use this information to group together those tests requiring equivalent resources. Besides, the approach proposes to deploy the grouped tests in isolated and elastic environments, so that their execution can be scheduled in parallel on several machines. RETORCH is exemplified with a real-world application, and its performance evaluation shows promising savings in terms of resource usage and time.},
author = {Augusto, Cristian and Mor{\'{a}}n, Jes{\'{u}}s and Bertolino, Antonia and de la Riva, Claudio and Tuya, Javier},
doi = {10.1007/s11219-020-09505-2},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Continuous integration,Continuous testing,End-to-end testing,Software testing,Test orchestration,Testing in the Cloud},
number = {3},
pages = {1147--1171},
publisher = {Springer},
title = {{RETORCH: an approach for resource-aware orchestration of end-to-end test cases}},
volume = {28},
year = {2020}
}
@article{Bastidas2019,
abstract = {In recent years, organizations begin to incorporate software quality practices to ensure that the product meets the requirements for the user or client. Risk-based tests are a type of test that helps to identify risks from the beginning of development, incorporating metrics as the case may be, or simply giving a priority to the risks, so that it also allows creating test cases according to that prioritization and then be executed during the development of the product. This type of tests helps to detect faults at early moments of the development process, which would increase the chances of a greater degree of quality in the software product.},
author = {Bastidas, Mar{\'{i}}a and Pardo, C{\'{e}}sar and Ardila, Carlos},
isbn = {9781510887534},
journal = {XIV Jornadas Iberoamericanas de Ingenieria de Software e Ingenieria del Conocimiento, JIISIC 2019},
keywords = {Risk-Based Testing,Software Testing,Systematic Mapping},
pages = {107--120},
title = {{Risk-based testing: Preliminary findings obtained from a systematic mapping study of the literature}},
year = {2019}
}
@article{Gutierrez2013,
abstract = {The definition of protocols between agents is not enough for guaranteeing the absence of undesirable communication in organizations and the presence of desirable ones in large multi-agent systems (MASs). This is a consequence of the complex system nature of MASs, which cause sophisticated behaviors to arise out of a multiplicity of relatively simple interactions among the independent agents composing them. With this motivation, this paper presents an approach for testing communication in MAS architectures. In this approach, designers are not only recommended to specify the desired communication protocols, but also the undesired patterns and organization structures in the agents' communications, allowing designers to define robust communication structures. For this purpose, this work presents (1) a language to define such patterns; (2) a set of already defined desired and undesired patterns which usually appear in general MASs; (3) a tool that allows developers to automatically detect these patterns in logs of MAS executions; and (4) a guideline that takes developers through the testing of the communications in MASs. The current approach is experienced with a case study, and the results show that the application of the current approach and the suppression of detected undesired patterns improve the effectiveness and efficiency of the corresponding MAS. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Guti{\'{e}}rrez, Celia and Garc{\'{i}}a-Magari{\~{n}}o, Iv{\'{a}}n and Serrano, Emilio and Bot{\'{i}}a, Juan A.},
doi = {10.1016/j.engappai.2013.06.006},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Agent-oriented software engineering,Communication,Debugging,Interaction,Multi-agent system,Testing},
number = {9},
pages = {2093--2104},
publisher = {Elsevier},
title = {{Robust design of multi-agent system interactions: A testing approach based on pattern matching}},
volume = {26},
year = {2013}
}
@article{Lei2010,
abstract = {This paper defines robustness based on a formal semantics of component system, and proposes a testing framework to detect robustness problems. The approach is implemented in a tool named RoTesCo (robustness testing for components). It traverses the state machine of the component under testing to generate paths, which cover all the transitions. Firstly, the call sequences following the paths drive the component to different states. Secondly, it feeds method calls with invalid parameters or inopportune calls to the component. The test oracle is automated by distinguishing different types of exceptions. RoTesCo is evaluated on a benchmark of real-world components from widely-used open source projects and it has produced encouraging results. {\textcopyright} by Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
author = {Lei, Bin and Wang, Lin Zhang and Bu, Lei and Li, Xuan Dong},
doi = {10.3724/SP.J.1001.2010.03544},
issn = {10009825},
journal = {Ruan Jian Xue Bao/Journal of Software},
keywords = {Component,Invalid input,Robustness,Software testing,State machine},
number = {5},
pages = {930--941},
publisher = {Elsevier},
title = {{Robustness testing for components based on state machine model}},
volume = {21},
year = {2010}
}
@article{Shah2016,
abstract = {Embedded software is at the core of current and future telecommunication, automotive, multimedia, and industrial automation systems. The success of practically any industrial application depends on the embedded software system's dependability, and one method to verify the dependability of a system is testing its robustness. The motivation behind this paper is to provide a knowledge base of the state of the practice in robustness testing of embedded software systems and to compare this to the state of the art. We have gathered the information on the state of the practice in robustness testing from seven different industrial domains (telecommunication, automotive, multimedia, critical infrastructure, aerospace, consumer products, and banking) by conducting 13 semi-structured interviews. We investigate the different aspects of robustness testing, such as the general view of robustness, relation to requirements engineering and design, test execution, failures, and tools. We highlight knowledge from the state of the practice of robustness testing of embedded software systems. We found different robustness testing practices that have not been previously described. This paper shows that the state of the practice, when it comes to robustness testing, differs between organizations and is quite different from the state of the art described in the scientific literature. For example, methods commonly described in the literature (e.g., the fuzzy approach) are not used in the organizations we studied. Instead, the interviewees described several ad hoc approaches that take specific scenarios into account (e.g., power failure or overload). Other differences we found concern the classification of robustness failures, the hypothesized root causes of robustness failures, and the types of tools used for robustness testing. This paper is a first step in capturing the state of the practice of robustness testing of embedded software systems. The results can be used by both researchers and practitioners. Researchers can use our findings to understand the gap between the state of the art and the state of the practice and develop their studies to fill this gap. Practitioners can also learn from this knowledge base regarding how they can improve their practice and acquire other practices.},
author = {Shah, Syed Muhammad Ali and Sundmark, Daniel and Lindstr{\"{o}}m, Birgitta and Andler, Sten F.},
doi = {10.1109/ACCESS.2016.2544951},
issn = {21693536},
journal = {IEEE Access},
keywords = {Testing,embedded systems,interviews,robustness,state of the art,state of the practice,survey},
pages = {1859--1871},
publisher = {IEEE},
title = {{Robustness Testing of Embedded Software Systems: An Industrial Interview Study}},
volume = {4},
year = {2016}
}
@article{Derakhshanfar2021,
abstract = {This is an extended abstract of the article: Pouria Derakhshanfar, Xavier Devroey, Gilles Perrouin, Andy Zaidman and Arie van Deursen. 2019. Search-based crash reproduction using behavioural model seeding. In: Software Testing, Verification and Reliability (May 2020). http://doi.org/10.1002/stvr.1733.},
archivePrefix = {arXiv},
arxivId = {1912.04606},
author = {Derakhshanfar, Pouria and Devroey, Xavier and Perrouin, Gilles and Zaidman, Andy and Deursen, Arie Van},
doi = {10.1109/ICST49551.2021.00039},
eprint = {1912.04606},
isbn = {9781728168364},
issn = {23318422},
journal = {Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation, ICST 2021},
keywords = {crash reproduction,model seeding,search-based software testing,seed learning},
number = {3},
pages = {281},
publisher = {Wiley Online Library},
title = {{Summary of Search-based Crash Reproduction using Behavioral Model Seeding}},
volume = {30},
year = {2021}
}
@article{Mani2010,
abstract = {Multi-Agent Systems (MAS) have been extensively used in the automation of manufacturing systems. However, similar to other distributed systems, autonomous agents' interaction in the Automated Manufacturing Systems (AMS) can potentially lead to runtime behavioral failures including deadlocks. Deadlocks can cause major financial consequences by negatively affecting the production cost and time. Although the deadlock monitoring techniques can prevent the harmful effects of deadlocks at runtime, but the testing techniques are able to detect design faults during the system design and development stages that can potentially lead to deadlock at runtime. In this paper, we propose a search based testing technique for deadlock detection in multi-agent manufacturing system based on the MAS design models. MAS design artifacts, constructed using Multi-agent Software Engineering (MaSE) methodology, are used for extracting test requirements for deadlock detection. As the case study, the proposed technique is applied to a multi-agent manufacturing system for verifying its effectiveness. A MAS simulator has been developed to simulate multi-agent manufacturing system behavior under test and the proposed testing technique has been implemented in a test requirement generator tool which creates test requirements based on the given design models. {\textcopyright} 2010 World Scientific Publishing Company.},
author = {Mani, Nariman and Garousi, Vahid and Far, Behrouz H.},
doi = {10.1142/S0218213010000261},
issn = {02182130},
journal = {International Journal on Artificial Intelligence Tools},
keywords = {Automated Manufacturing Systems (AMS),Multi-Agent Systems (MAS),Multi-agent Software Engineering (MaSE),deadlock,software testing},
number = {4},
pages = {417--437},
publisher = {World Scientific},
title = {{Search-based testing of multi-agent manufacturing systems for deadlocks based on models}},
volume = {19},
year = {2010}
}
@article{Jin2020,
abstract = {Resource allocation is essential for cloud-based load testing. The existing techniques use coarse-grained resource allocation methods with an entire virtual machine occupied by a single test task for cloud-based load testing. The idle resources in a virtual machine are unable to be used by other load testing tasks. This may result in uneconomical use of test resources and increase test costs. To optimize the use of test resources, this paper presents a shared-mode resource allocation method for cloud-based load testing. The method shares client-side virtual machine resources among load testing tasks. It takes minimizing resource redundancy, test execution cost, and network communication cost as optimization objectives of resource allocation, with the assurance of enough test resources as a basic constraint. We introduce a multi-objective optimization algorithm to create an optimized resource allocation plan for load testing tasks within a time window. The experiments show that the proposed method can reduce resource demands for load testing and thereby save the test costs.},
author = {Jin, Wenming and Qian, Ju and Yan, Shuoyan},
doi = {10.1109/ACCESS.2020.3020863},
issn = {21693536},
journal = {IEEE Access},
keywords = {Cloud testing,load testing,multi-objective optimization,test resource allocation},
pages = {161894--161907},
publisher = {IEEE},
title = {{Shared-Mode Resource Allocation for Cloud-Based Load Testing}},
volume = {8},
year = {2020}
}
@article{Azzouzi2010,
abstract = {The Multi Agent Systems are a paradigm of the most promising technology in the development of distributed software systems. They include the mechanisms and functions required to support interaction, communication and coordination between the various components of such a system. In the context of distributed test the activity of the test is performed by a set of parallel testers called PTCs (Parallel Test Components). The difficulty is in writing communication procedures of coordination and cooperation between the PTCS. In this context, we combine in this paper adaptable mobile agent with multi-agent systems technology to enhance the distributed test. The objective of this work is to eventually have a platform for compliance testing of distributed applications.},
author = {Azzouzi, Salma and Benattou, Mohammed and El, My and Charaf, Hassan and Abouchabaka, Jaafar},
issn = {1694-0814},
journal = {IJCSI International Journal of Computer Science Issues},
keywords = {Actor,Distributed Testing,Mobile Actor,Mobile Agent,SMA},
number = {5},
pages = {231},
publisher = {Citeseer},
title = {{SMA and Mobile Agents Actors for Distributed Testing}},
url = {www.IJCSI.org},
volume = {7},
year = {2010}
}
@article{Katherine2012,
abstract = {Software Testing is a challenging activity for many software engineering projects and it is one of the five main technical activity areas of the software engineering lifecycle that still poses substantial challenges. Testing software requires enough resources and budget to complete it successfully. But most of the organizations face the challenges to provide enough resources to test their software in distributed environment, with different loading level. This leads to severe problem when the software deployed into different client environment and varying user load. Cloud computing is a one of the emerging technology which opens new door for software testing. This paper investigates the software testing in cloud platform which includes cloud testing models, recent research work, commercial tools and research issues.},
author = {Katherine, a Vanitha and Alagarsamy, K},
journal = {International journal of Computer Applications},
keywords = {cloud testing,software testing,testing tools},
number = {6},
pages = {21--25},
publisher = {Published by Foundation of Computer Science},
title = {{Software Testing in Cloud Platform : A Survey}},
volume = {46},
year = {2012}
}
@article{Carver2018,
abstract = {A test oracle for a concurrent program is a method for checking whether an observed behavior of the program is consistent with the program's specification. Abstract specification models for message-passing concurrent programs are often expressed as, or can be translated into, a labeled transition system (LTS). Stateful techniques for generating test oracles from LTS specification models are often limited by the state explosion problem. In this paper, we present a stateless technique for generating global and local test oracles from LTS specification models. A global test oracle uses tests generated from a global LTS model of the complete system to verify a global implementation relation between the model of the system and its implementation. Global test oracles, however, may require too many test sequences to be executed by the implementation. A local test oracle verifies local implementation relations between individual component models and their implementation threads. Local tests are executed against individual threads, without testing the system as a whole. Verifying the local implementation relations implies that a corresponding global implementation relation holds between the complete system model and its implementation. Empirical results indicate that using local test oracles can significantly reduce the number of executed test sequences.},
author = {Carver, R. and Lei, Yu},
doi = {10.1016/j.jss.2017.11.026},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Concurrent programs,Model-based testing,Stateless search,Test oracle,Test sequences},
pages = {237--265},
publisher = {Elsevier},
title = {{Stateless techniques for generating global and local test oracles for message-passing concurrent programs}},
volume = {136},
year = {2018}
}
@article{Bernardino2017,
abstract = {Every year several contributions to the model-based testing (MBT) field are published. Therefore, to follow the evolution and trends of several tools and models available is difficult. Moreover, since the variety of models and tools that became available in recent years, choosing an approach to support the MBT process is a challenging activity. The main objective of this study is to provide an overview on MBT tools and models used by those tools. Furthermore, the authors' study can help academic researchers and companies to understand the topics involving MBT. Therefore, a systematic mapping study was conducted in which 1197 distinct papers were evaluated. At the end, 87 primary studies were selected to be analysed in a quantitative and qualitative way. As a result, they classified the tools and models that are currently used to support MBT. Moreover, they identified 70 MBT tools, as well as different domains in which MBT is already applied to. Therefore, there are some evidence that MBT continues to be a broad and 'alive' research field since every year a significant number of papers presenting different kinds of contributions are published.},
author = {Bernardino, Maicon and Rodrigues, Elder M. and Zorzo, Avelino F. and Marchezan, Luciano},
doi = {10.1049/iet-sen.2015.0154},
issn = {17518806},
journal = {IET Software},
number = {4},
pages = {141--155},
publisher = {IET},
title = {{Systematic mapping study on MBT: Tools and models}},
volume = {11},
year = {2017}
}
@inproceedings{Muniz2015,
abstract = {Model-based testing (MBT) is an approach that takes software specification as the base for the formal model creation and, from it, enables the test case extraction. Depending on the type of model, an MBT tool can support functional and statistical tests. However, there are few tools that support both testing techniques. Moreover, the ones that support them offer a limited number of coverage criteria. This paper presents TCG, a tool for the generation and selection of functional and statistical test cases. It provides 8 classic generation techniques and 5 selection heuristics, including a novel one called minimum probability of path.},
author = {Muniz, Laryssa Lima and Netto, Ubiratan S.C. and Maia, Paulo Henrique M.},
booktitle = {ICEIS 2015 - 17th International Conference on Enterprise Information Systems, Proceedings},
doi = {10.5220/0005398604040411},
isbn = {9789897580963},
keywords = {Functional testing,Model-based testing,Statistical testing,Tool},
pages = {404--411},
title = {{TCG: A model-based testing tool for functional and statistical testing}},
volume = {2},
year = {2015}
}
@article{Bozkurt2013,
abstract = {Service-oriented architecture (SOA) is gaining momentum as an emerging distributed system architecture for business-to-business collaborations. This momentum can be observed in both industry and academic research. SOA presents new challenges and opportunities for testing and verification, leading to an upsurge in research. This paper surveys the previous work undertaken on testing and verification of service-centric systems, which in total are 177 papers, showing the strengths and weaknesses of current strategies and testing tools and identifying issues for future work. Copyright {\textcopyright} 2012 John Wiley & Sons, Ltd.},
author = {Bozkurt, Mustafa and Harman, Mark and Hassoun, Youssef},
doi = {10.1002/stvr.1470},
issn = {09600833},
journal = {Software Testing Verification and Reliability},
number = {4},
pages = {261--313},
publisher = {Wiley Online Library},
title = {{Testing and verification in service-oriented architecture: A survey}},
volume = {23},
year = {2013}
}
@article{Mishra2016,
abstract = {Testing as a Service (TaaS) [1] is gaining acceptance in software engineering industry for it finds outsourcing of testing a viable option for both, adhering to software production timeline and making testing cost-effective while not making compromise in product quality. In consequence, few companies have started offering tools automating TaaS process. This chapter highlights on this upcoming concept and presents a generic model explaining the steps involved in TaaS. It also explains use of cloud technology for TaaS implementation. This aims at effective utilization of resources while delivering a testing service. In addition, a pricing model for test outsourcing is proposed.},
author = {Mishra, Pankhuri and Tripathi, Neeraj},
doi = {10.1007/978-981-10-1415-4_7},
isbn = {9789811014154},
issn = {0218-1940},
journal = {Trends in Software Testing},
keywords = {Benefits of taas,Industry case study in taas,Industry leaders in taas,Outsourcing,Pricing test service,Taas,Taas accelerators,Taas architecture,Taas consumer,Taas cost model,Taas demo,Taas disadvantages,Taas enablers,Taas example,Taas framework,Taas infrastructure,Taas limitations,Taas on cloud,Taas provider,Taas working model,Test optimization,Test outsourcing,Test service,Testing as a service,Traditional testing to taas},
number = {01},
pages = {149--176},
publisher = {World Scientific},
title = {{Testing as a service}},
volume = {26},
year = {2016}
}
@article{Nascimento2020,
abstract = {Multiagent Systems (MASs) have multiple different characteristics, such as autonomy, and asynchronous and social features, which make these systems difficult to understand. Thus, there is a lack of procedures guaranteeing that multiagent systems once implemented would behave as desired. Determining the reliability of such systems is further complicated by the fact that current agent-based approaches may also involve non-deterministic characteristics, such as learning, self-adaptation and self-organization (SASO). Nonetheless, there is a gap in the literature regarding the testing of systems with these features. This paper presents an approach based on metadata and the publish-subscribe paradigm to develop test applications that address the process of failure diagnosis in a self-organizing MAS. The novelty of the proposed approach involves its ability to test self-organizing MAS systems in the context of local and global behavior. To illustrate the use of this approach, we developed a self-organizing MAS system based on the Internet of Things (IoT), which simulates a set of smart street lights, and we performed functional ad-hoc tests. The street lights need to interact with each other in order to achieve the global goals of reducing energy consumption and maintaining the maximum value of visual comfort in illuminated areas. To achieve these global behaviors, the street lights develop local behaviors automatically through a self-organizing process based on machine-learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1904.01736},
author = {Nascimento, Nathalia and Alencar, Paulo and Lucena, Carlos and Cowan, Donald},
doi = {10.1109/ACCESS.2020.3036668},
eprint = {1904.01736},
issn = {21693536},
journal = {IEEE Access},
keywords = {Failure diagnosis,Internet of Things (IoT),Machine learning,Metadata-oriented testing,Multiagent system,Neuroevolution,Publish-subscribe,Self-organizing},
pages = {204256--204267},
publisher = {IEEE},
title = {{A metadata-driven approach for testing self-organizing multiagent systems}},
volume = {8},
year = {2020}
}
@article{Suffian2014,
author = {Suffian, Muhammad Dhiauddin Mohamed and Fahrurazi, Fairul Rizal and Ibrahim, Suhaimi},
issn = {2289-2842},
journal = {International Journal of Software Engineering and Technology The},
keywords = {cloud,open source,performance testing,response},
number = {2},
pages = {19--25},
title = {{The Design and Execution of Performance Testing Strategy for Cloud-based System}},
volume = {1},
year = {2014}
}
@article{Saeed2016,
abstract = {Context Model-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. MBT gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of SBTs for MBT because the generated test cases are optimal and have low computational cost. However, successful, future SBTs for MBT applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. Objective The objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of SBTs for MBT and present the limitations of the current literature to direct future research. Method We conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. Results The results indicate that the majority of the existing applications of SBTs for MBT focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the SBTs for complex systems in MBT. Conclusion This extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation.},
author = {Saeed, Aneesa and {Ab Hamid}, Siti Hafizah and Mustafa, Mumtaz Begum},
doi = {10.1016/j.asoc.2016.08.030},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Model-based testing,Search-based techniques,Software testing,Systematic literature review,Taxonomy,Test case generation},
pages = {1094--1117},
publisher = {Elsevier},
title = {{The experimental applications of search-based techniques for model-based testing: Taxonomy and systematic literature review}},
volume = {49},
year = {2016}
}
@article{Bernardi2011,
abstract = {Software systems that do not meet their timing constraints can cause risks. In this work, we propose a comprehensive method for assessing the risk of timing failure by evaluating the software design. We show how to apply best practises in software engineering and well-known Time Petri Net (TPN) modeling and analysis techniques, and we demonstrate the effectiveness of the method with reference to a case study in the domain of real-time embedded systems. The method customizes the Australian standard risk management process, where the system context is the UML-based software specification, enriched with standard MARTE profile annotations to capture nonfunctional system properties. During the risk analysis, a TPN is derived, via model transformation, from the software design specification and TPN bound techniques are applied to estimate the probability of timing failure. TPN bound techniques are also exploited, within the risk evaluation and treatment steps, to identify the risk causes in the software design. {\textcopyright} 2006 IEEE.},
author = {Bernardi, Simona and Campos, Javier and Merseguer, Jos{\'{e}}},
doi = {10.1109/TII.2010.2098415},
issn = {15513203},
journal = {IEEE Transactions on Industrial Informatics},
keywords = {MARTE profile,risk assessment,time Petri net (TPN) bound techniques,unified modeling language (UML)},
number = {1},
pages = {90--104},
publisher = {IEEE},
title = {{Timing-failure risk assessment of UML design using time petri net bound techniques}},
volume = {7},
year = {2011}
}
@inproceedings{Nikiforova2020,
abstract = {The main idea of the solution is to improve testing methodology of information systems (IS) by using data quality models. The idea of the approach is as follows: (a) first, a description of the data to be processed by IS and the data quality requirements used for the development of the test are created, (b) then, an automated test of the system on the generated tests is performed. Thus, the traditional software testing is complemented with new features – automated compliance checks of data to be entered and stored in the database. The generation of tests for all possible data quality conditions creates a complete set of tests that check the operation of the IS on all possible data quality conditions. Since this paper describes the first steps that are taken moving towards the proposed idea, it aims to (a) define the aim of the initiated research and (b) to choose the main components and to propose their combination resulting in the architecture of the idea to be implemented.},
author = {Nikiforova, Anastasija and Bicevskis, Janis},
booktitle = {ICEIS 2020 - Proceedings of the 22nd International Conference on Enterprise Information Systems},
doi = {10.5220/0009459703220329},
isbn = {9789897584237},
keywords = {Data Object,Executable Models,Functional Testing,Information System,Model-based Testing},
pages = {322--329},
title = {{Towards a business process model-based testing of information systems functionality}},
volume = {2},
year = {2020}
}
@article{Aichernig2011,
abstract = {This paper presents a novel model-based test case generation approach that automatically derives test cases from UML state machines. UML is given a two-layered formal semantics by (1) mapping UML class diagrams and state charts to Back's Action Systems, (2) by interpreting these action systems as labeled transition systems. The first semantics provides a formal framework to capture the object-oriented machinery: classes, objects, inheritance, transitions, time-outs, signals, nested and parallel regions. The second mapping represents the tester's view on the interface in terms of input and output actions. Tretman's input-output conformance relation (ioco) forms the basis of our fault models. Mutation analysis on the models is used to generate test cases. A car alarm system serves as a running example},
author = {Aichernig, Bernhard K and Brandl, Harald and J{\"{o}}bstl, Elisabeth and Krenn, Willibald},
issn = {0163-5948},
journal = {SIGSOFT Softw. Eng. Notes},
number = {1},
pages = {1--8},
publisher = {ACM New York, NY, USA},
title = {{UML in Action: A Two-layered Interpretation for Testing}},
url = {http://doi.acm.org/10.1145/1921532.1921559},
volume = {36},
year = {2011}
}
@article{Mansour2011,
abstract = {In software maintenance, a system has to be regression tested after modifying it. The goal of regression testing is to ensure that modifications have not adversely affected the system. Regression test selection determines a subset of test cases, from the initial test suite, which concentrates on the parts of the system affected by the modification. Previous techniques have been mainly code-based and several of them have addressed procedural programs. When working with large and complex object-oriented systems, source code-based regression testing is usually costly. This paper proposes a programming- language-independent technique for regression test selection for object-oriented software based on Unified Modeling Language (UML 2.0) design diagrams. These diagrams are: the newly introduced interaction overview diagram, class diagrams, and sequence diagrams. We assume a test suite that contains both unit and system test cases. Based on the software changes reflected in the class and the interaction overview diagrams, our proposed technique selects test cases in phases. In the first phase, we select both unit and system test cases that directly traverse the changed methods and their calling methods. For the second phase, we present algorithms for detecting system level changes in the interaction overview diagram. If the change is at the action level, which is represented by a sequence diagram, only the test cases that execute changed methods will be selected. We apply our proposed technique to a few object-oriented subject applications and evaluate its precision and inclusiveness in addition to the number of selected tests; the results demonstrate the advantages of the technique. {\textcopyright} 2010 John Wiley & Sons, Ltd.},
author = {Mansour, Nashat and Takkoush, Husam and Nehme, Ali},
doi = {10.1002/smr.508},
issn = {1532060X},
journal = {Journal of Software Maintenance and Evolution},
keywords = {UML,design-level testing,object-oriented regression testing,regression test selection,software maintenance},
number = {1},
pages = {51--68},
publisher = {Wiley Online Library},
title = {{UML-based regression testing for OO software}},
volume = {23},
year = {2011}
}
@article{Esparcia-Alcazar2018,
abstract = {Traversal-based automated software testing involves testing an application via its graphical user interface (GUI) and thereby taking the user's point of view and executing actions in a human-like manner. These actions are decided on the fly, as the software under test (SUT) is being run, as opposed to being set up in the form of a sequence prior to the testing, a sequence that is then used to exercise the SUT. In practice, random choice is commonly used to decide which action to execute at each state (a procedure commonly referred to as monkey testing), but a number of alternative mechanisms have also been proposed in the literature. Here we propose using genetic programming (GP) to evolve such an action selection strategy, defined as a list of IF-THEN rules. Genetic programming has proved to be suited for evolving all sorts of programs, and rules in particular, provided adequate primitives (functions and terminals) are defined. These primitives must aim to extract the most relevant information from the SUT and the dynamics of the testing process. We introduce a number of such primitives suited to the problem at hand and evaluate their usefulness based on various metrics. We carry out experiments and compare the results with those obtained by random selection and also by Q-learning, a reinforcement learning technique. Three applications are used as Software Under Test (SUT) in the experiments. The analysis shows the potential of GP to evolve action selection strategies.},
author = {Esparcia-Alc{\'{a}}zar, Anna I. and Almenar, Francisco and Vos, Tanja E.J. and Rueda, Urko},
doi = {10.1007/s12293-018-0263-8},
issn = {18659292},
journal = {Memetic Computing},
keywords = {Action selection for testing,Automated software testing via the GUI,Genetic programming,Testing metrics},
number = {3},
pages = {257--265},
publisher = {Springer},
title = {{Using genetic programming to evolve action selection rules in traversal-based automated software testing: results obtained with the TESTAR tool}},
volume = {10},
year = {2018}
}
@article{Ibias2019,
abstract = {Context:Testing is the main validation technique used to increase the reliability of software systems. The effectiveness of testing can be strongly reduced by Failed Error Propagation. This situation happens when the System Under Test executes a faulty statement, the state of the system is affected by this fault, but the expected output is observed. Squeeziness is an information theoretic measure designed to quantify the likelihood of Failed Error Propagation and previous work has shown that Squeeziness correlates strongly with Failed Error Propagation in white-box scenarios. Despite its usefulness, this measure, in its current formulation, cannot be used in a black-box scenario where we do not have access to the source code of the components. Objective:The main goal of this paper is to adapt Squeeziness to a black-box scenario and evaluate whether it can be used to estimate the likelihood that a component of a software system introduces Failed Error Propagation. Method: First, we defined our black-box scenario. Specifically, we considered the Failed Error Propagation that a component introduces when it receives its input from another component. We were interested in this since such fault masking makes it more difficult to find faults in the previous component when testing. Second, we defined our notion of Squeeziness in this framework. Finally, we carried out experiments in order to evaluate our measure. Results: Our experiments showed a strong correlation between the likelihood of Failed Error Propagation and Squeeziness. Conclusion: We can conclude that our new notion of Squeeziness can be used as a measure that estimates the probability of Failed Error Propagation being introduced by a component. As a result, it has the potential to be used as a measure of testability, allowing testers to assess how easy it is to test either the whole system or a single component. We considered a simple model (Finite State Machines)but the notions and results can be extended/adapted to deal with more complex state-based models, in particular, those containing data.},
author = {Ibias, Alfredo and Hierons, Robert M. and N{\'{u}}{\~{n}}ez, Manuel},
doi = {10.1016/j.infsof.2019.04.012},
issn = {09505849},
journal = {Information and Software Technology},
pages = {132--147},
publisher = {Elsevier},
title = {{Using Squeeziness to test component-based systems defined as Finite State Machines}},
volume = {112},
year = {2019}
}
@article{DiAlesio2018,
abstract = {Real-time embedded systems (RTESs) operating in safety-critical domains have to satisfy strict performance requirements in terms of task deadlines, response time, and CPU usage. Two of the main factors affecting the satisfaction of these requirements are the configuration parameters regulating how the system interacts with hardware devices, and the external events triggering the system tasks. In particular, it is necessary to carefully tune the parameters in order to ensure a satisfactory trade-off between responsiveness and usage of computational resources, and also to stress test the system with worst-case inputs likely to violate the requirements. Performance tuning and stress testing are usually manual, time-consuming, and error-prone processes, because the system parameters and input values range in a large domain, and their impact over performance is hard to predict without executing the system. In this paper, we provide an approach, based on UML/MARTE, to support the generation of system configurations predicted to achieve a satisfactory trade-off between response time and CPU usage, and stress test cases that push the system tasks to violate their deadlines. First, we devise a conceptual model that specifies the abstractions required for analyzing task deadlines, response time, and CPU usage, and provide a mapping between these abstractions and UML/MARTE. Then, we prune the UML/MARTE metamodel to only contain a purpose-specific subset of entities needed to support performance tuning and stress testing. The pruned version is a supertype of UML/MARTE, which ensures that all instances of the pruned metamodel are also instances of UML/MARTE. Finally, we cast the generation of configurations and stress test cases as two constrained optimization problems (COPs) over our conceptual model. The input data for these COPs in automatically generated via a model-to-text (M2T) transformation from models specified in the pruned UML/MARTE metamodel to the Optimization Programming Language. We validate our approach in a safety-critical RTES from the maritime and energy domain, showing that (1) our conceptual model can be applied in an industrial setting with reasonable effort, and (2) the optimization problems effectively identify configurations predicted to minimize response time and CPU usage, and stress test cases that maximize deadline misses. Based on our experience, we highlight challenges and potential issues to be aware of when using UML/MARTE to support performance tuning and stress testing in an industrial context.},
author = {{Di Alesio}, Stefano and Sen, Sagar},
doi = {10.1007/s10270-017-0585-x},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Constrained optimization,Performance tuning,Real-time systems,Safety-critical systems,Stress testing,UML/MARTE},
number = {2},
pages = {479--508},
publisher = {Springer},
title = {{Using UML/MARTE to support performance tuning and stress testing in real-time systems}},
volume = {17},
year = {2018}
}
@article{Qi2016,
abstract = {Reachability testing is an important approach to testing concurrent programs. It generates and exercises synchronization sequences automatically and on-the-fly without saving any test history. Existing reachability testing can be classified into exhaustive and t-way testing. Exhaustive testing is impractical in many cases while t-way testing may decrease the capability of fault detection in some cases. In this paper, we present a variable strength reachability testing strategy, which adopts the dynamic framework of reachability testing and uses a variable strength combinatorial strategy. Different parameter groups are provided with different covering strength. Variable strength testing covers no t-way combinations but the necessary combinations of parameters having mutual interactions in a concurrent program. It is more reasonable than t-way testing because uniform interactions between parameters do not often exist in concurrent systems. We propose a merging algorithmthat implements the variable strength combinatorial testing strategy and conduct our experiment on several concurrent programs. The experimental results indicate that our variable strength reachability testing reaches a good tradeoff between the effectiveness and efficiency. It can keep the same capability of fault detection as exhaustive reachability testing while substantially reducing the number of synchronization sequences and decreasing the execution time in most cases.},
author = {Qi, Xiaofang and He, Jun and Wang, Peng and Zhou, Huayang},
doi = {10.1007/s11704-016-5096-3},
issn = {20952236},
journal = {Frontiers of Computer Science},
keywords = {concurrency testing,reachability testing,software testing,variable strength combinatorial testing},
month = {aug},
number = {4},
pages = {631--643},
publisher = {Higher Education Press},
title = {{Variable strength combinatorial testing of concurrent programs}},
volume = {10},
year = {2016}
}
@article{Khattak2015,
author = {Khattak, Abid Saeed and Khiyal, Malik Sikander Hayat and Rizvi, Sanam Shahla},
journal = {IJCSNS International Journal of Computer Science and Network Security},
keywords = {agent-based,e-vomas,ict,information and communication,model,software,v,verification and validation},
number = {3},
pages = {29--35},
title = {{Verification and Validation of agent-based model using E- VOMAS approach}},
volume = {15},
year = {2015}
}
@article{Krac2018,
abstract = {Test oracles differentiate between the correct and incorrect system behavior. Hence, test oracle automation is essential to achieve overall test automation. Otherwise, testers have to manually check the system behavior for all test cases. A common test oracle automation approach for testing systems with visual output is based on exact matching between a snapshot of the observed output and a previously taken reference image. However, images can be subject to scaling and translation variations. These variations lead to a high number of false positives, where an error is reported due to a mismatch between the compared images although an error does not exist. To address this problem, we introduce an automated test oracle, named VISOR, that employs a fast image processing pipeline. This pipeline includes a series of image filters that align the compared images and remove noise to eliminate differences caused by scaling and translation. We evaluated our approach in the context of an industrial case study for regression testing of Digital TVs. Results show that VISOR can avoid 90% of false positive cases after training the system for 4 h. Following this one-time training, VISOR can compare thousands of image pairs within seconds on a laptop computer.},
author = {Kıra{\c{c}}, M. Furkan and Aktemur, Barış and S{\"{o}}zer, Hasan},
doi = {10.1016/j.jss.2017.06.023},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Black-box testing,Computer vision,Image processing,Test automation,Test oracle},
pages = {1339--1351},
publisher = {Elsevier},
title = {{VISOR: A fast image processing pipeline with scaling and translation invariance for test oracle automation of visual output systems}},
volume = {136},
year = {2018}
}
@article{Garousi2020,
abstract = {Context: Visual GUI testing (VGT) is referred to as the latest generation GUI-based testing. It is a tool-driven technique, which uses image recognition for interacting with and asserting the behavior of the system under test. Motivated by the industrial need of a large Turkish software and systems company providing solutions in the areas of defense and IT sector, an action-research project was recently initiated to implement VGT in several teams and projects in the company. Objective: To address the above needs, we planned and carried out an empirical investigation with the goal of assessing VGT using two tools (Sikuli and JAutomate). The purpose was to determine a suitable approach and tool for VGT of a given project (software product) in the company, increase the know-how in the company's test teams. Method: Using an action-research case-study design, we investigated the use of VGT in the studied organization. Specifically, using the two selected VGT tools, we conducted a quantitative and a qualitative evaluation of VGT. Results: By assessing the list of Challenges, Problems and Limitations (CPL), proposed in previous work, in the context of our empirical study, we found that test-tool- and SUT-related CPLs were quite comparable to a previous empirical study, e.g., the synchronization between SUT and test tools were not always robust and there were failures in test tools' image recognition features. When assessing the types of test maintenance activities, when executing the automated test cases on next versions of the SUTs, for the case of the two test tools, we found that about half of the test cases (59.1% and 47.8%) failed in the next version. Conclusion: By our results, we confirm some of the previously-reported issues when conducting VGT. Further, we highlight some additional challenges in test maintenance when using VGT.},
archivePrefix = {arXiv},
arxivId = {2005.09303},
author = {Garousi, Vahid and Afzal, Wasif and {\c{C}}ağlar, Adem and Işık, İhsan Berk and Baydan, Berker and {\c{C}}aylak, Se{\c{c}}kin and Boyraz, Ahmet Zeki and Yola{\c{c}}an, Burak and Herkiloğlu, Kadir},
eprint = {2005.09303},
issn = {23318422},
journal = {arXiv},
keywords = {Automated testing tools,Empirical evaluation,Graphical User Interface (GUI) Testing,Industrial case study,Visual GUI testing},
title = {{Visual GUI testing in practice: An extended industrial case study}},
url = {http://arxiv.org/abs/2005.09303},
year = {2020}
}
@article{Ghani2019,
abstract = {These days continual demands on loosely coupled systems have web service gives basic necessities to deliver resolution that are adaptable and sufficient to be work at runtime for maintaining the high quality of the system. One of the basic techniques to evaluate the quality of such systems is through testing. Due to the rapid popularization of web service, which is progressing and continuously increasing, testing of web service has become a basic necessity to maintain high quality of web service. The testing of the performance of Web service based applications is attracting extensive attention. In order to evaluate the performance of web services, it is essential to evaluate the QoS (Quality of Service) attributes such as interoperability, reusability, auditability, maintainability, accuracy and performance to improve the quality of service. The purpose of this study is to introduce the systematic literature review of web services testing techniques to evaluate the QoS attributes to make the testing technique better. With the intention of better testing quality in web services, this systematic literature review intends to evaluate what QoS parameters are necessary to provide better quality assurance. The focus of systematic literature is also to make sure that quality of testing can be encouraged for the present and future. Consequently, the main attention and motivation of the study is to provide an overview of recent research efforts of web service testing techniques from the research community. Each testing technique in web services has identified apparent standards, benefits, and restrictions. This systemic literature review provides a different testing resolution to industry to decide which testing technique is the most efficient and effective with the testing assignment agenda with available resources. As for the significance, it can be said that web service testing technique are still broadly open for improvements.},
author = {Ghani, Israr and Wan-Kadir, Wan M.N. and Mustafa, Ahmad},
doi = {10.14569/ijacsa.2019.0100858},
issn = {21565570},
journal = {International Journal of Advanced Computer Science and Applications},
keywords = {Quality assurance,Web service component testing,Web service testing,Web service testing techniques},
number = {8},
pages = {443--458},
title = {{Web service testing techniques: A systematic literature review}},
volume = {10},
year = {2019}
}
@article{Ye2013,
abstract = {Whitening the testing of service-oriented applications can provide service consumers confidence on how well an application has been tested. However, to protect business interests of service providers and to prevent information leakage, the implementation details of services are usually invisible to service consumers. This makes it challenging to determine the test coverage of a service composition as a whole and design test cases effectively. To address this problem, we propose an approach to whiten the testing of service compositions based on events exposed by services. By deriving event interfaces to explore only necessary test coverage information from service implementations, our approach allows service consumers to determine test coverage based on selected events exposed by services at runtime without releasing the service implementation details. We also develop an approach to design test cases effectively based on event interfaces concerning both effectiveness and information leakage. The experimental results show that our approach outperforms existing testing approaches for service compositions with up to 49 percent more test coverage and an up to 24 percent higher fault-detection rate. Moreover, our solution can trade off effectiveness, efficiency, and information leakage for test case generation. {\textcopyright} 2013 IEEE.},
author = {Ye, Chunyang and Jacobsen, Hans Arno},
doi = {10.1109/TSE.2013.20},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Web service composition,event interface,events,white-box testing},
number = {10},
pages = {1444--1465},
publisher = {IEEE},
title = {{Whitening SOA testing via event exposure}},
volume = {39},
year = {2013}
}
@article{Chen2014,
abstract = {The testing of Web services is an essential aspect of their quality assurance, however, because this testing often involves injecting only one mutant at one time, some vulnerability faults cannot be detected. To address this, the current paper presents a set of mutation operators that can be combined and defines the corresponding combinatorial strategies based on data perturbation and combinatorial testing. Based on this, multiple mutants can be injected at one time to help uncover interactive faults. To improve testing efficiency and effectiveness, a combinatorial testing approach focusing on Web service vulnerability is proposed: Firstly, initial test data are generated with perturbation techniques based on Web Services Description Language documents and Simple Object Access Protocol messages. Then, a combinatorial testing cases generation (CTCG) algorithm is used to generate the final combinatorial test data according to the proposed strategies. Furthermore, for some special Web services in which there is only one parameter or one method in service interface, a fuzzy mutation approach algorithm, as a complementary approach to CTCG, is also proposed. Finally, some testing experiments are conducted to verify the effectiveness of the proposed approaches in an integrated testing platform. The experiments show that proposed approaches are both feasible and effective: They can find more vulnerability faults than the traditional approaches. {\textcopyright} 2013 Springer-Verlag London.},
author = {Chen, Jinfu and Li, Qing and Mao, Chengying and Towey, Dave and Zhan, Yongzhao and Wang, Huanhuan},
doi = {10.1007/s11761-013-0139-1},
issn = {18632394},
journal = {Service Oriented Computing and Applications},
keywords = {Combinatorial testing,Mutation operator,SOAP message mutation,Vulnerability testing,Web services testing},
number = {1},
pages = {1--13},
title = {{A Web services vulnerability testing approach based on combinatorial mutation and SOAP message mutation}},
volume = {8},
year = {2014}
}
@article{Castro2015,
abstract = {Software testing is a very delicate aspect of software development, since designing good test sets is a non-trivial task. In this article, we describe a testing technique for testing business rules using property-based testing and the property-based automatic testing tool QuickCheck. Systematic, effective, and efficient testing of business rules increases the confidence on the validation of business concepts and domain rules which are specifically critical to data consistency. The approach is presented on the basis of small but representative examples in order to facilitate the readers' understanding, but it has been successfully evaluated in a number of different industrial examples, demonstrating that it generalises to much larger systems and is, thus, broadly applicable.},
author = {Castro, Laura M.},
doi = {10.1007/s10844-014-0335-2},
issn = {15737675},
journal = {Journal of Intelligent Information Systems},
keywords = {Business rules,Data integrity,Model-based testing,QuickCheck,Software verification},
month = {jun},
number = {3},
pages = {355--380},
publisher = {Kluwer Academic Publishers},
title = {{Advanced management of data integrity: property-based testing for business rules}},
volume = {44},
year = {2015}
}
@article{Bakar2018,
abstract = {Agent systems are distributed systems consist of agents that autonomously interact to each other in an environment to perform tasks and achieve goals. Performing verification is important to ensure correctness of agent properties and to detect faults. The objective of this review is to identify research gap and future research direction of agent systems verification. In this study, the surveys of existing techniques for checking agent properties and detecting faults during design, development and runtime phases of agent system life-cycle are presented. Search terms with relevant keywords were used to identify primary studies that relate to the topic of discussion. Next, the studies were classified based on the used techniques and the addressed properties. 231 primary studies were identified during the search process. From these studies, 49% were implemented for verification of agent systems during design, 27% during development and 25% during runtime. Model checking or model-based verification techniques are the highest proposed techniques (44%) followed by the testing and debugging during development (17%). The properties that are largely addressed by the selected studies are temporal properties (19%) and epistemic properties (9%). At the end of the review, the research gap and the future research direction are presented.},
author = {Bakar, Najwa Abu and Selamat, Ali},
doi = {10.1007/s10489-017-1112-z},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {Agent systems,Debugging,Fault management,Multi-agent systems,Testing,Verification,Violations detection},
month = {may},
number = {5},
pages = {1251--1274},
publisher = {Springer New York LLC},
title = {{Agent systems verification : systematic literature review and mapping}},
volume = {48},
year = {2018}
}
@article{Arcuri2018,
abstract = {What is the impact of software engineering research on current practices in industry? In this paper, I report on my direct experience as a PhD/post-doc working in software engineering research projects, and then spending the following five years as an engineer in two different companies (the first one being the same I worked in collaboration with during my post-doc). Given a background in software engineering research, what cutting-edge techniques and tools from academia did I use in my daily work when developing and testing the systems of these companies? Regarding validation and verification (my main area of research), the answer is rather short: as far as I can tell, only FindBugs. In this paper, I report on why this was the case, and discuss all the challenging, complex open problems we face in industry and which somehow are “neglected” in the academic circles. In particular, I will first discuss what actual tools I could use in my daily work, such as JaCoCo and Selenium. Then, I will discuss the main open problems I faced, particularly related to environment simulators, unit and web testing. After that, popular topics in academia are presented, such as UML, regression and mutation testing. Their lack of impact on the type of projects I worked on in industry is then discussed. Finally, from this industrial experience, I provide my opinions about how this situation can be improved, in particular related to how academics are evaluated, and advocate for a greater involvement into open-source projects.},
archivePrefix = {arXiv},
arxivId = {1901.03865},
author = {Arcuri, Andrea},
doi = {10.1007/s10664-017-9570-9},
eprint = {1901.03865},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Applied research,Impact,Industry,Practice,Technology transfer},
month = {aug},
number = {4},
pages = {1959--1981},
publisher = {Springer New York LLC},
title = {{An experience report on applying software testing academic results in industry: we need usable automated test generation}},
volume = {23},
year = {2018}
}
@article{10.1145/2501654.2501663,
abstract = {Web-based malware is a growing threat to today's Internet security. Attacks of this type are prevalent and lead to serious security consequences. Millions of malicious URLs are used as distribution channels to propagate malware all over theWeb. After being infected, victim systems fall in the control of attackers, who can utilize them for various cyber crimes such as stealing credentials, spamming, and distributed denial-ofservice attacks. Moreover, it has been observed that traditional security technologies such as firewalls and intrusion detection systems have only limited capability to mitigate this new problem. In this article, we survey the state-of-the-art research regarding the analysis of-and defense against-Web-based malware attacks. First, we study the attack model, the root cause, and the vulnerabilities that enable these attacks. Second, we analyze the status quo of the Web-based malware problem. Third, three categories of defense mechanisms are discussed in detail: (1) building honeypots with virtual machines or signature-based detection system to discover existing threats; (2) using code analysis and testing techniques to identify the vulnerabilities of Web applications; and (3) constructing reputation-based blacklists or smart sandbox systems to protect end-users from attacks. We show that these three categories of approaches form an extensive solution space to theWeb-based malware problem. Finally, we compare the surveyed approaches and discuss possible future research directions. {\textcopyright} 2013 ACM.},
address = {New York, NY, USA},
author = {Chang, Jian and Venkatasubramanian, Krishna K. and West, Andrew G. and Lee, Insup},
doi = {10.1145/2501654.2501663},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {Web-based malware},
number = {4},
publisher = {Association for Computing Machinery},
title = {{Analyzing and defending against web-based malware}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/2501654.2501663},
volume = {45},
year = {2013}
}
@article{Iqbal2015a,
abstract = {Modeling and Analysis of Real-Time and Embedded Systems (MARTE) is a Unified Modeling Language (UML) profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In the last 5 years, we have applied UML/MARTE to three distinct industrial problems in three industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experience of solving these problems by applying UML/MARTE on four industrial case studies. We highlight the challenges we faced with respect to the industrial adoption of MARTE. Based on our combined experience, we derive a framework to guide practitioners for future applications of UML/MARTE in an industrial context. The framework provides a set of detailed guidelines that help reduce the gap between the modeling notations and real-world industrial application needs.},
author = {Iqbal, Muhammad Zohaib and Ali, Shaukat and Yue, Tao and Briand, Lionel},
doi = {10.1007/s10270-014-0405-5},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Architecture Modeling,Industrial Case Studies,MARTE,Model-based Testing,Real-Time Embedded Systems,UML},
month = {oct},
number = {4},
pages = {1367--1385},
publisher = {Springer Verlag},
title = {{Applying UML/MARTE on industrial projects: challenges, experiences, and guidelines}},
volume = {14},
year = {2015}
}
@article{ISI:000320217900005,
abstract = {Context: Testing distributed service-oriented applications (SOAs) is more challenging than testing monolithic applications since these applications have complex interactions between participant services. Test engineers can observe test results only through a front service that handles request messages sent by test engineers. Message exchanges between participant services are hidden behind the front service and cannot be easily observed or controlled through the front service. For this reason, testing SOAs suffer from limited observability and controllability problem. Objective: This paper proposes a new test method that is architecture-based and exploits interaction architecture of a SOA. The proposed test method alleviates the limited observability and controllability problem by employing test architecture, thereby facilitating test execution and analysis through monitoring and controlling message exchanges. Method: Our proposed method derives an interaction architecture from the specification of a SOA. Test architectures can be designed from the derived interaction architecture by extending it with additional test elements. At the same time, architecture-neutral test scenarios are automatically generated from the test model that is constructed from the specification. Our method combines test architecture information with the test scenarios to obtain architecture-enabled test scenarios under the selected test architectures. Finally, architecture-enabled test execution and analysis are conducted in the real network environment. Results: The efficacy of the proposed method is demonstrated with an industrial case study, which shows that it is practical and effective for testing SOAs. Even though our method increases an additional test generation effort owing to test architecture, it is counterbalanced by higher fault detection rate and faster fault locating time. Conclusion: The main benefit of our approach is that using test architecture it enhances testability of SOA by increasing observability and controllability through monitoring and controlling message exchanges. Our architecture-based test method enables test engineers to detect faults efficiently and also reduce fault locating time significantly. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Keum, Changsup and Kang, Sungwon and Kim, Myungchul},
doi = {10.1016/j.infsof.2013.01.002},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Architecture-based testing,Service composition,Service-oriented application testing,Test architecture},
month = {jul},
number = {7},
pages = {1212--1223},
title = {{Architecture-based testing of service-oriented applications in distributed systems}},
volume = {55},
year = {2013}
}
@article{Vain2016297,
abstract = {Low-latency systems where reaction time is primary success factor and design consideration, are serious challenge to existing integration and system level testing techniques. Modern cyber physical systems have grown to the scale of global geographic distribution and latency requirements are measured in nanoseconds. While existing tools support prescribed input profiles they seldom provide enough reactivity to run the tests with simultaneous and interdependent input profiles at remote front ends. Additional complexities emerge due to severe timing constraints the tests have to meet when test navigation decision time ranges near the message propagation time. Sufficient timing conditions for remote online testing have been proposed in remote $\delta$-testing method recently. We extend the $\delta$-testing by deploying testers on fully distributed test architecture. This approach reduces the test reaction time by almost a factor of two. We validate the method on a distributed oil pumping SCADA system case study.},
annote = {cited By 2},
author = {Vain, J{\"{u}}ri and Halling, Evelin and Kanter, Gert and Anier, Aivo and Pal, Deepak},
doi = {10.3233/978-1-61499-714-6-297},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
keywords = {Distributed systems,Low-latency systems,Model-based testing},
pages = {297--310},
title = {{Automatic distribution of local testers for testing distributed systems}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030703059&doi=10.3233%2F978-1-61499-714-6-297&partnerID=40&md5=84387b7d2b59d52288045486de30a3a9},
volume = {291},
year = {2016}
}
@article{Hillah2017,
abstract = {This paper presents the approach to functional test automation of services (black-box testing) and service architectures (grey-box testing) that has been developed within the MIDAS project and is accessible on the MIDAS SaaS. In particular, the algorithms and techniques adopted for addressing input and oracle generation, dynamic scheduling, and session planning issues supporting service functional test automation are illustrated. More specifically, the paper details: (i) the test input generation based on formal methods and temporal logic specifications, (ii) the test oracle generation based on service formal specifications, (iii) the dynamic scheduling of test cases based on probabilistic graphical reasoning, and (iv) the reactive, evidence-based planning of test sessions with on-the-fly generation of new test cases. Finally, the utilisation of the MIDAS prototype for the functional test of operational services and service architectures in the healthcare industry is reported and assessed. A planned evolution of the technology deals with the testing and troubleshooting of distributed systems that integrate connected objects.},
author = {Hillah, Lom Messan and Maesano, Ariele Paolo and {De Rosa}, Fabio and Kordon, Fabrice and Wuillemin, Pierre Henri and Fontanelli, Riccardo and Bona, Sergio Di and Guerri, Davide and Maesano, Libero},
doi = {10.1007/s10009-016-0440-3},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Model-based test generation,Service testing,Test automation,Test planning,Test prioritisation,Test scheduling},
number = {3},
pages = {281--308},
title = {{Automation and intelligent scheduling of distributed system functional testing: Model-based functional testing in practice}},
url = {https://doi.org/10.1007/s10009-016-0440-3},
volume = {19},
year = {2017}
}
@article{Herbold2017,
abstract = {Usage-based testing focuses quality assurance on highly used parts of the software. The basis for this are usage profiles based on which test cases are generated. There are two fundamental approaches in usage-based testing for deriving usage profiles: either the system under test (SUT) is observed during its operation and from the obtained usage data a usage profile is automatically inferred, or a usage profile is modeled by hand within a model-based testing (MBT) approach. In this article, we propose a third and combined approach, where we automatically infer a usage profile and create a test data repository from usage data. Then, we create representations of the generated tests and test data in the test model from an MBT approach. The test model enables us to generate executable Testing and Test Control Notation version 3 (TTCN-3) and thereby allows us to automate the test execution. Together with industrial partners, we adopted this approach in two pilot studies. Our findings show that usage-based testing can be applied in practice and greatly helps with the automation of tests. Moreover, we found that even if usage-based testing is not of interest, the incorporation of usage data can ease the application of MBT.},
author = {Herbold, Steffen and Harms, Patrick and Grabowski, Jens},
doi = {10.1007/s10009-016-0437-y},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Model-based testing,TTCN-3,Usage monitoring,Usage-based testing,Web service testing},
month = {jun},
number = {3},
pages = {309--324},
publisher = {Springer Verlag},
title = {{Combining usage-based and model-based testing for service-oriented architectures in the industrial practice}},
volume = {19},
year = {2017}
}
@article{Huang2016,
abstract = {In this article, we present a model-based black-box equivalence partition testing strategy, together with a formal proof of its completeness properties. The results apply to reactive systems with large, possibly infinite input data types and finite internal and output data ranges that may be enumerated with acceptable effort. The investigation is performed on a semantic level and applies to all concrete test models whose behavioural semantics can be encoded as a variant of state transition systems. Test suite construction is performed in relation to a given fault model F for which a finite black-box test suite can be constructed which is complete with respect to F. It is shown how the test suite generation can be effectively implemented by model-based testing tools, using propositional representations of behavioural model semantics and constraint solvers. A SysML model of the ceiling speed monitoring function of the European Train Control System is presented as a case study, to explain theory application to a concrete modelling formalism.},
author = {ling Huang, Wen and Peleska, Jan},
doi = {10.1007/s10009-014-0356-8},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Complete test strategies,Equivalence class partition testing,Model-based testing,State transition systems,SysML state machines},
month = {jun},
number = {3},
pages = {265--283},
publisher = {Springer Verlag},
title = {{Complete model-based equivalence class testing}},
volume = {18},
year = {2016}
}
@article{10.1145/3276529,
abstract = {A real-world distributed system is rarely implemented as a standalone monolithic system. Instead, it is composed of multiple independent interacting components that together ensure the desired system-level specification. One can scale systematic testing to large, industrial-scale implementations by decomposing the system-level testing problem into a collection of simpler component-level testing problems.This paper proposes techniques for compositional programming and testing of distributed systems with two central contributions: (1) We propose a module system based on the theory of compositional trace refinement for dynamic systems consisting of asynchronously-communicating state machines, where state machines can be dynamically created, and communication topology of the existing state machines can change at runtime; (2) We present ModP, a programming system that implements our module system to enable compositional reasoning (assume-guarantee) of distributed systems.We demonstrate the efficacy of our framework by building two practical fault-tolerant distributed systems, a transaction-commit service and a replicated hash-table. ModP helps implement these systems modularly and validate them via compositional testing. We empirically demonstrate that the abstraction-based compositional reasoning approach helps amplify the coverage during testing and scale it to real-world distributed systems. The distributed services built using ModP achieve performance comparable to open-source equivalents.},
address = {New York, NY, USA},
author = {Desai, Ankush and Phanishayee, Amar and Qadeer, Shaz and Seshia, Sanjit A.},
doi = {10.1145/3276529},
issn = {2475-1421},
journal = {Proceedings of the ACM on Programming Languages},
keywords = {actors,compositional verification,distributed systems,domain-specific language,event-driven programming,module system,systematic testing},
number = {OOPSLA},
pages = {1--30},
publisher = {Association for Computing Machinery},
title = {{Compositional programming and testing of dynamic distributed systems}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/3276529},
volume = {2},
year = {2018}
}
@article{Jaghoori2017,
abstract = {We present an extension of the actor model with real-time, including deadlines associated with messages, and explicit application-level scheduling policies, e.g.,“earliest deadline first” which can be associated with individual actors. Schedulability analysis in this setting amounts to checking whether, given a scheduling policy for each actor, every task is processed within its designated deadline. To check schedulability, we introduce a compositional automata-theoretic approach, based on maximal use of model checking combined with testing. Behavioral interfaces define what an actor expects from the environment, and the deadlines for messages given these assumptions. We use model checking to verify that actors match their behavioral interfaces. We extend timed automata refinement with the notion of deadlines and use it to define compatibility of actor environments with the behavioral interfaces. Model checking of compatibility is computationally hard, so we propose a special testing process. We show that the analyses are decidable and automate the process using the Uppaal model checker.},
author = {Jaghoori, Mohammad Mahdi and de Boer, Frank and Longuet, Delphine and Chothia, Tom and Sirjani, Marjan},
doi = {10.1007/s00236-015-0254-x},
issn = {14320525},
journal = {Acta Informatica},
month = {jun},
number = {4},
pages = {343--378},
publisher = {Springer Verlag},
title = {{Compositional schedulability analysis of real-time actor-based systems}},
volume = {54},
year = {2017}
}
@article{David2012,
abstract = {We present a specification theory for timed systems implemented in the Ecdar tool. We illustrate the operations of the specification theory on a running example, showing the models and verification checks. To demonstrate the power of the compositional verification, we perform an in depth case study of a leader election protocol; Modeling it in Ecdar as Timed input/output automata Specifications and performing both monolithic and compositional verification of two interesting properties on it. We compare the execution time of the compositional to the classical verification showing a huge difference in favor of compositional verification. {\textcopyright} 2012 Springer-Verlag.},
author = {David, Alexandre and Larsen, Kim G. and Legay, Axel and M{\o}ller, Mikael H. and Nyman, Ulrik and Ravn, Anders P. and Skou, Arne and Wasowski, Andrzej},
doi = {10.1007/s10009-012-0237-y},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Compositional verification,Leader election protocol,Real-time systems,Timed input/output automata},
number = {6},
pages = {703--720},
title = {{Compositional verification of real-time systems using Ecdar}},
volume = {14},
year = {2012}
}
@article{Kim2012,
abstract = {In today's information society, flash memory has become a virtually indispensable component, particularly for mobile devices. In order for mobile devices to operate successfully, it is essential that flash memory be controlled correctly through flash storage platform software such as the file system, flash translation layer, and low-level device drivers. However, as is typical for embedded software, conventional testing methods often fail to detect hidden flaws in the software due to the difficulty of creating effective test cases. As a different approach, model checking techniques guarantee a complete analysis, but only on a limited scale. In this paper, we describe an empirical study wherein a concolic testing method is applied to the multi-sector read operation for flash storage platform software. This method combines a concrete dynamic execution and a symbolic execution to automatically generate test cases for full path coverage. Through the experiments, we analyze the advantages and weaknesses of the concolic testing approach on the flash storage platform software. {\textcopyright} 2011 BCS.},
author = {Kim, Moonzoo and Kim, Yunho and Choi, Yunja},
doi = {10.1007/s00165-011-0200-9},
issn = {09345043},
journal = {Formal Aspects of Computing},
keywords = {And embedded software verification,Concolic testing,Flash memory,Unit analysis},
month = {may},
number = {3},
pages = {355--374},
title = {{Concolic testing of the multi-sector read operation for flash storage platform software}},
volume = {24},
year = {2012}
}
@article{Melo2018,
abstract = {Concurrent software testing is a challenging activity due to factors that are not present in sequential programs, such as communication, synchronization, and non-determinism, and that directly affect the testing process. When we consider multithreaded programs, new challenges for the testing activity are imposed. In the context of structural testing, an important problem raised is how to deal with the coverage of shared variables in order to establish the association between def-use of shared variables. This paper presents results related to the structural testing of multithreaded programs, including testing criteria for coverage testing, a supporting tool, called ValiPthread testing tool and results of an experimental study. This study was conducted to evaluate the cost, effectiveness, and strength of the testing criteria. Also, the study evaluates the contribution of these testing criteria to test specific aspects of multithreaded programs. The experimental results show evidence that the testing criteria present lower cost and higher effectiveness when revealing some kinds of defects, such as deadlock and critical region block. Also, compared to sequential testing criteria, the proposed criteria show that it is important to establish specific coverage testing for multithreaded programs.},
author = {Melo, Silvana Morita and de Souza, Simone do Rocio Senger and Sarmanho, Felipe Santos and de Souza, Paulo Sergio Lopes},
doi = {10.1007/s11219-017-9376-4},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Coverage criteria,Experimental evaluation,Multithreaded programs,PThreads,Shared memory,Structural testing},
month = {sep},
number = {3},
pages = {921--959},
publisher = {Springer New York LLC},
title = {{Contributions for the structural testing of multithreaded programs: coverage criteria, testing tool, and experimental evaluation}},
volume = {26},
year = {2018}
}
@article{Antunes2017,
abstract = {This paper proposes a generic approach for designing vulnerability testing tools for web services, which includes the definition of the testing procedure and the tool components. Based on the proposed approach, we present the design of three innovative testing tools that implement three complementary techniques (improved penetration testing, attack signatures and interface monitoring, and runtime anomaly detection) for detecting injection vulnerabilities, thus offering an extensive support for different scenarios. A case study has been designed to demonstrate the tools for the particular case of SQL Injection vulnerabilities. The experimental evaluation demonstrates that the tools can effectively be used in different scenarios and that they outperform well-known commercial tools by achieving higher detection coverage and lower false-positive rates.},
author = {Antunes, Nuno and Vieira, Marco},
doi = {10.1007/s10207-016-0334-0},
issn = {16155270},
journal = {International Journal of Information Security},
keywords = {Security testing,Software vulnerabilities,Vulnerability detection,Web services},
month = {aug},
number = {4},
pages = {435--457},
publisher = {Springer Verlag},
title = {{Designing vulnerability testing tools for web services: approach, components, and tools}},
volume = {16},
year = {2017}
}
@article{Vanus2014,
abstract = {This article describes the development of a visualization application software used to control operational and technical functions in the Smart Home system or Smart Home Care system via the wireless xComfort control system. Graphic visualization of a home electrical control system gives the user unprecedented comfort when controlling home systems. The user is able to obtain the information necessary to optimise the management of operational and technical functions in the building as well as information about energy consumption. Selected definitions of requirements for the visualization system, online access via the Internet, control via USB interface, and control requirements executed via mobile phone are the reasons why these technical elements were selected. This article describes their mutual relations, functions and connections within the system. At the end of this article we propose a method to test the reliability of the created software application as well as the wireless xComfort system under different conditions which stimulate different implementation methods applicable to a real building/apartment unit. Measurement results can be used for the actual installation process and for optimal implementation of the active elements of the wireless system.},
author = {Vanus, Jan and Kucera, Pavel and Martinek, Radek and Koziorek, Jiri},
doi = {10.1186/s13673-014-0019-5},
issn = {21921962},
journal = {Human-centric Computing and Information Sciences},
keywords = {Control,Smart home,Smart home care,Testing,Visualization,Wireless system},
number = {1},
publisher = {Springer Berlin Heidelberg},
title = {{Development and testing of a visualization application software, implemented with wireless control system in smart home care}},
volume = {4},
year = {2014}
}
@article{Chauvel2015,
abstract = {Various services are now available in the Cloud, ranging from turnkey databases and application servers to high-level services such as continuous integration or source version control. To stand out of this diversity, robustness of service compositions is an important selling argument, but which remains difficult to understand and estimate as it does not only depend on services but also on the underlying platform and infrastructure. Yet, choosing a specific service composition may fail to deliver the expected robustness, but reverting early choices may jeopardise the success of any Cloud project. Inspired by existing models used in Biology to quantify the robustness of ecosystems, we show how to tailor them to obtain early indicators of robustness for cloud-based deployments. This technique helps identify weakest parts in the overall architecture and in turn mitigates the risk of having to revert key architectural choices. We illustrate our approach by comparing the robustness of four alternative deployments of the SensApp application, which includes a MongoDB database, four REST services and a graphical web-front end.},
author = {Chauvel, Franck and Song, Hui and Ferry, Nicolas and Fleurey, Franck},
doi = {10.1186/s13677-015-0043-7},
issn = {2192113X},
journal = {Journal of Cloud Computing},
keywords = {Cloud-based systems,Failure sequences,Robustness indicators,Robustness metric,Sensitive components,Software deployment,Systems architectures},
month = {dec},
number = {1},
publisher = {Springer Verlag},
title = {{Evaluating robustness of cloud-based systems}},
volume = {4},
year = {2015}
}
@article{Belli2010,
abstract = {A service-oriented architecture (SOA) for web applications is often implemented using web services (WSs) and consists of different operations the executions of which are perceived as events. The order and time-appropriateness of occurrences of these events play a vital role for the proper functioning of a real-time SOA. This paper presents an event-based approach to modeling and testing of functional behavior of WSs by event sequence graphs (ESG). Nodes of an ESG represent events, e.g., "request" or "response", and arcs give the sequence of these events. For representing parameter values, e.g., for time-out of operation calls, ESG are augmented by decision tables. A case study carried out on a commercial web system with SOA validates the approach and analyzes its characteristic issues. The novelty of the approach stems from (i) its simplicity and lucidity in representing complex real-time web applications based on WSs in SOA, and (ii) its modeling that considers also testing and thus enables a comfortable fault management leading to a holistic view. {\textcopyright} 2010 Springer-Verlag London Limited.},
author = {Belli, Fevzi and Linschulte, Michael},
doi = {10.1007/s11761-010-0056-5},
issn = {18632386},
journal = {Service Oriented Computing and Applications},
keywords = {Event sequence graph,Model-based testing,Real-time,Web services},
number = {1},
pages = {3--15},
title = {{Event-driven modeling and testing of real-time web services}},
volume = {4},
year = {2010}
}
@article{Gouvea2013,
abstract = {In this paper, we report on our experience with the application of validated models to assess performance, reliability, and adaptability of a complex mission critical system that is being developed to dynamically monitor and control the position of an oil-drilling platform. We present real-time modeling results that show that all tasks are schedulable. We performed stochastic analysis of the distribution of task execution time as a function of the number of system interfaces. We report on the variability of task execution times for the expected system configurations. In addition, we have executed a system library for an important task inside the performance model simulator. We report on the measured algorithm convergence as a function of the number of vessel thrusters. We have also studied the system architecture adaptability by comparing the documented system architecture and the implemented source code. We report on the adaptability findings and the recommendations we were able to provide to the system's architect. Finally, we have developed models of hardware and software reliability. We report on hardware and software reliability results based on the evaluation of the system architecture. {\textcopyright} 2012 Springer-Verlag.},
author = {Gouv{\^{e}}a, Daniel Dominguez and Muniz, Cyro de A.Assis D. and Pinto, Gilson A. and Avritzer, Alberto and Le{\~{a}}o, Rosa Maria Meri and {de Souza e Silva}, Edmundo and Diniz, Morganna Carmem and Cortellessa, Vittorio and Berardinelli, Luca and Leite, Julius C.B. and Moss{\'{e}}, Daniel and Cai, Yuanfang and Dalton, Michael and Happe, Lucia and Koziolek, Anne},
doi = {10.1007/s10270-012-0264-x},
issn = {16191366},
journal = {Software and Systems Modeling},
keywords = {Adaptability,Performance,Reliability},
month = {oct},
number = {4},
pages = {765--787},
title = {{Experience with model-based performance, reliability, and adaptability assessment of a complex industrial architecture}},
volume = {12},
year = {2013}
}
@article{SilvaFilho2013,
abstract = {The integration of novel software quality assurance tools into existing development environments must be performed in ways that leverage the benefits of the tools while minimizing their impact on existing software processes. This supports the adoption of new methodologies with minimal interference into core business practices. This paper discusses the design of Tedeso, an extensible and interoperable model-based testing platform developed to facilitate the automatic generation of tests, while supporting the needs of different stakeholders in a diverse and broad organization. We discuss Tedeso key design characteristics, in particular its extensibility and interoperability, provided through the use of a workflow-driven service-oriented architecture, and show how it has enabled and facilitated the adoption of model-based testing techniques in different business units in different sectors within SIEMENS. We also discuss some issues that come from the adoption of service-oriented architectures, showing how they have been managed in our platform. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {{Silva Filho}, Roberto Silveira and Hasling, William M. and Budnik, Christof J. and McKenna, Monica},
doi = {10.1007/s10515-012-0118-3},
issn = {09288910},
journal = {Automated Software Engineering},
keywords = {Model-based testing automation,Tedeso,Workflow-driven service-oriented architectures},
month = {sep},
number = {3},
pages = {299--337},
title = {{Experiences using Tedeso: An extensible and interoperable model-based testing platform}},
volume = {20},
year = {2013}
}
@article{Hubner2019,
abstract = {In this paper, a complete model-based equivalence class testing strategy recently developed by the authors is experimentally evaluated. This black-box strategy applies to deterministic systems with infinite input domains and finite internal state and output domains. It is complete with respect to a given fault model. This means that conforming behaviours will never be rejected, and all non-conforming behaviours inside a given fault domain will be uncovered. We investigate the question how this strategy performs for systems under test whose behaviours lie outside the fault domain. Furthermore, a strategy extension is presented, that is based on randomised data selection from input equivalence classes. While this extension is still complete with respect to the given fault domain, it also promises a higher test strength when applied against members outside this domain. This is confirmed by an experimental evaluation that compares mutation coverage achieved by the original and the extended strategy with the coverage obtained by random testing. For mutation generation, not only typical software errors, but also critical HW/SW integration errors are considered. The latter can be caused by mismatches between hardware and software design, even in the presence of totally correct software.},
author = {H{\"{u}}bner, Felix and ling Huang, Wen and Peleska, Jan},
doi = {10.1007/s10270-017-0595-8},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Adaptive random testing,Equivalence class partition testing,Model-based testing,State transition systems,SysML,SystemC},
month = {feb},
number = {1},
pages = {423--443},
publisher = {Springer Verlag},
title = {{Experimental evaluation of a novel equivalence class partition testing strategy}},
volume = {18},
year = {2019}
}
@article{10.14778/3397230.3397233,
abstract = {Formal modelling is a powerful tool for developing complex systems. At MongoDB, we use TLA+ to model and verify multiple aspects of several systems. Ensuring conformance between a specification and its implementation can add value to any specification; it can avoid transcription errors, prevent bugs as a large organization rapidly develops the specified code, and even keep multiple implementations of the same specification in sync. In this paper, we explore model-based testing as a tool for ensuring specification-implementation conformance. We attempted two case studies: model-based trace-checking (MBTC) in the MongoDB Server's replication protocol and model-based test-case generation (MBTCG) in MongoDB Realm Sync's operational transformation algorithm. We found MBTC to be impractical for testing that the Server conformed to a highly abstract specification. MBTCG was highly successful for Realm Sync, however. We analyze why one technique succeeded and the other failed, and advise future implementers making similar attempts at model-based testing.},
author = {Davis, A. Jesse Jiryu and Hirschhorn, Max and Schvimer, Judah},
doi = {10.14778/3397230.3397233},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
month = {may},
number = {9},
pages = {1346--1358},
publisher = {VLDB Endowment},
title = {{eXtreme modelling in practice}},
url = {https://doi-org.ezproxy.utp.edu.co/10.14778/3397230.3397233},
volume = {13},
year = {2020}
}
@article{Garousi2011101,
abstract = {In a previous article, a stress testing methodology was reported to detect network traffic-related Real-Time (RT) faults in distributed RT systems based on the design UML model of a System Under Test (SUT). The stress methodology, referred to as Test LOcation-driven Stress Testing (TLOST), aimed at increasing the chances of RT failures (violations in RT constraints) associated with a given stress test location (an network or a node under test). As demonstrated and experimented in this article, although TLOST is useful in stress testing different test locations (nodes and network, it does not guarantee to target (test) all RT constraints in an SUT. This is because the durations of message sequences bounded by some RT constraints might never be exercised (covered) by TLOST. A complementary stress test methodology is proposed in this article, which guarantees to target (cover) all RT constraints in an SUT and detect their potential RT faults (if any). Using a case study, this article shows that the new complementary methodology is capable of targeting the RT faults not detected by the previous test methodology. Copyright {\textcopyright} 2009 John Wiley & Sons, Ltd.},
annote = {cited By 4},
author = {Garousi, Vahid},
doi = {10.1002/stvr.418},
issn = {09600833},
journal = {Software Testing Verification and Reliability},
keywords = {UML,distributed systems,fault-driven testing,model-based testing,network traffic,search-based software testing,stress testing},
number = {2},
pages = {101--124},
title = {{Fault-driven stress testing of distributed real-time software based on UML models}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957517071&doi=10.1002%2Fstvr.418&partnerID=40&md5=6930777922508250ee67dfae3976e900},
volume = {21},
year = {2011}
}
@article{El-Fakih2012,
abstract = {We study the problem of deriving a test suite with guaranteed fault coverage from a given finite state machine specification with respect to some given user defined faults. We consider the case when an imple- mentation under test can have more states than its specification while user defined faults are implemented in an arbitrary way. We show that our approach can be used for FSM-based incremental and mutation testing and correspondingly we investigate cases that can be used for reducing length of obtained test suites. In some cases, worst-case length of obtained test suite becomes polynomial. Experiments show significant gains is using our approach in comparison to testing the whole specification. {\textcopyright} Pleiades Publishing, Ltd., 2012.},
author = {El-Fakih, K. A. and Dorofeeva, R. and Yevtushenko, N. V. and Bochmann, G. V.},
doi = {10.1134/S0361768812040019},
issn = {03617688},
journal = {Programming and Computer Software},
keywords = {Conformance testing,Finite state machines,Incremental testing,Model-based testing,Mutation testing},
number = {4},
pages = {201--209},
publisher = {Maik Nauka Publishing / Springer SBM},
title = {{FSM-based testing from user defined faults adapted to incremental and mutation testing 1}},
volume = {38},
year = {2012}
}
@article{Lobato2017,
abstract = {Currently, when a technology is defined (Bluetooth, GSM, UMTS or LTE), it is mandatory to think about an infrastructure with an abstract test suite, codecs, timeframes, etc. that will support the design of a new testing architecture. If, in addition, the testing methodology used is for different purposes, such as protocol conformance, radio conformance, radio performance or network conformance, the result is a redundant effort in the design and development of different test systems, increasing costs, development time and reducing reusability. In this paper it is shown how a framework for formal languages such as TTCN can be realized in a common adapter layer capable of sharing a model for any technology and testing methodology, reducing costs, improving time and avoiding redundant designing efforts.},
author = {Lobato, S. and Poncela, J. and Aamir, M.},
doi = {10.1007/s41870-017-0007-x},
issn = {25112112},
journal = {International Journal of Information Technology (Singapore)},
keywords = {Network conformance testing,Protocol conformance testing,Radio conformance testing,Radio performance testing,TTCN-3,Test architecture,Testing methodology},
month = {mar},
number = {1},
pages = {41--48},
publisher = {Springer Science and Business Media B.V.},
title = {{General framework for testing using formal languages}},
volume = {9},
year = {2017}
}
@article{Paiva2016,
abstract = {Model-based testing overcomes challenges in software testing by generating automated test cases from behavior models, e.g. finite state machine (FSM) and input/output transition system (IOTS). Despite the existence of methods for IOTSs, the problem of selection of test cases is an important and difficult topic. The current methods from IOTSs do not provide the same support offered by the existing theory for FSMs, as complete fault coverage. In this paper, we propose a test generation method for IOTSs based on the W method developed for FSMs. The basic idea is to generate a transition cover set and a characterization set and concatenate them to generate complete test suites for IOTSs in a bounded number of steps. The method generates test suites with complete fault coverage for a given fault domain and is targeted at a class of IOTSs, called mealy IOTS, which accepts inputs only in stable states. Results from a case study show the proposed algorithm can achieve better results than a classical method for IOTSs.},
author = {Paiva, Sofia Costa and Simao, Adenilso},
doi = {10.1007/s00165-015-0350-2},
issn = {1433299X},
journal = {Formal Aspects of Computing},
keywords = {Complete test suites,Fault domain,Mealy input/output transition systems,Test generation},
month = {mar},
number = {1},
pages = {65--78},
publisher = {Springer-Verlag London Ltd},
title = {{Generation of complete test suites from mealy input/output transition systems}},
volume = {28},
year = {2016}
}
@article{Hierons2012,
abstract = {Some systems interact with their environment at physically distributed interfaces called ports and we separately observe sequences of inputs and outputs at each port. As a result we cannot reconstruct the global sequence that occurred and this reduces our ability to distinguish different systems in testing or in use. In this paper we explore notions of conformance for an input output transition system that has multiple ports, adapting the widely used ioco implementation relation to this situation. We consider two different scenarios. In the first scenario the agents at the different ports are entirely independent. Alternatively, it may be feasible for some external agent to receive information from more than one of the agents at the ports of the system, these local behaviours potentially being brought together and here we require a stronger implementation relation. We define implementation relations for these scenarios and prove that in the case of a single-port system the new implementation relations are equivalent to ioco. In addition, we define what it means for a test case to be controllable and give an algorithm that decides whether this condition holds. We give a test generation algorithm to produce sound and complete test suites. Finally, we study two implementation relations to deal with partially specified systems. {\textcopyright} 2011 Springer-Verlag.},
author = {Hierons, Robert M. and Merayo, Mercedes G. and N{\'{u}}{\~{n}}ez, Manuel},
doi = {10.1007/s00446-011-0149-1},
issn = {01782770},
journal = {Distributed Computing},
keywords = {Formal approaches to testing,Formal methodologies to develop distributed software systems,Systems with distributed ports},
month = {mar},
number = {1},
pages = {35--62},
title = {{Implementation relations and test generation for systems with distributed interfaces}},
volume = {25},
year = {2012}
}
@article{ISI:000442456700002,
abstract = {The objective of this study is to develop a test process approach for microservices. Microservice architecture can reduce the development costs of web systems and shift the accidental complexity from the application to the infrastructure. In this paper, we provide an analysis and description of the microservice test process, as well as an example implementation of the software system, that supports the described process. An example of usage demonstrates the advantages of our solution in the real environment.},
author = {Savchenko, Dmitrii and Radchenko, Gleb and Hynninen, Timo and Taipale, Ossi},
issn = {1313-8251},
journal = {International Journal on Information Technologies & Security},
keywords = {microservices,test techniques,testing,testing service},
number = {3},
pages = {13--24},
title = {{Microservice Test Process: Design and Implementation}},
volume = {10},
year = {2018}
}
@misc{Zambon2011,
abstract = {For today's organisations, having a reliable information system is crucial to safeguard enterprise revenues (think of on-line banking, reservations for e-tickets etc.). Such a system must often offer high guarantees in terms of its availability; in other words, to guarantee business continuity, IT systems can afford very little downtime. Unfortunately, making an assessment of IT availability risks is difficult: incidents affecting the availability of a marginal component of the system may propagate in unexpected ways to other more essential components that functionally depend on them. General-purpose risk assessment (RA) methods do not provide technical solutions to deal with this problem. In this paper we present the qualitative time dependency (QualTD) model and technique, which is meant to be employed together with standard RA methods for the qualitative assessment of availability risks based on the propagation of availability incidents in an IT architecture. The QualTD model is based on our previous quantitative time dependency (TD) model (Zambon et al. in BDIM '07: Second IEEE/IFIP international workshop on business-driven IT management. IEEE Computer Society Press, pp 75-83, 2007), but provides more flexible modelling capabilities for the target of assessment. Furthermore, the previous model required quantitative data which is often too costly to acquire, whereas QualTD applies only qualitative scales, making it more applicable to industrial practice. We validate our model and technique in a real-world case by performing a risk assessment on the authentication and authorisation system of a large multinational company and by evaluating the results with respect to the goals of the stakeholders of the system. We also perform a review of the most popular standard RA methods and discuss which type of method can be combined with our technique. {\textcopyright} 2010 The Author(s).},
author = {Zambon, Emmanuele and Etalle, Sandro and Wieringa, Roel J. and Hartel, Pieter},
booktitle = {Software and Systems Modeling},
doi = {10.1007/s10270-010-0166-8},
issn = {16191366},
keywords = {Availability,Information risk management,Information security,Risk assessment,System modelling},
month = {oct},
number = {4},
pages = {553--580},
title = {{Model-based qualitative risk assessment for availability of IT infrastructures}},
volume = {10},
year = {2011}
}
@article{Herbold2017a,
abstract = {The quality of Web services is an important factor for businesses that advertise or sell their services in the Internet. Failures can directly lead to fewer costumers or security problems. However, the testing of complex Web services that are organized in service-oriented architectures is a difficult and complex problem. Model-based testing (MBT) is one solution to deal with the complexity of the testing. With MBT, testers do not define the tests directly, but rather specify the structure and behavior of the System Under Test using models. Then, a test strategy is used to derive test cases automatically from the models. However, MBT yields a large amount of tests for complex systems which require lots of resources for their execution, thereby limiting its potential. Within this article, we discuss how cloud computing can be used to provide the required resources for scaling up test campaigns with large amounts of test cases derived using MBT.},
author = {Herbold, Steffen and Hoffmann, Andreas},
doi = {10.1007/s10009-017-0449-2},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Cloud computing,Model-based testing,TTCN-3,Testing as a service},
month = {jun},
number = {3},
pages = {271--279},
publisher = {Springer Verlag},
title = {{Model-based testing as a service}},
volume = {19},
year = {2017}
}
@article{PoncedeLeon2016,
abstract = {Model-based testing has mainly focused on models where concurrency is interpreted as interleaving (like the ioco theory for labeled transition systems), which may be too coarse when one wants concurrency to be preserved in the implementation. In order to test such concurrent systems, we choose to use Petri nets as specifications and define a concurrent conformance relation named co-ioco. We present a test generation algorithm based on Petri net unfolding able to build a complete test suite w.r.t our co-ioco conformance relation. In addition, we propose several coverage criteria that allow to select finite prefixes of an unfolding in order to build manageable test suites.},
author = {{Ponce de Le{\'{o}}n}, Hern{\'{a}}n and Haar, Stefan and Longuet, Delphine},
doi = {10.1007/s10009-014-0353-y},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Conformance testing,Coverage criteria,Cut-off events,Event structures,Petri net unfolding,True concurrency},
month = {jun},
number = {3},
pages = {305--318},
publisher = {Springer Verlag},
title = {{Model-based testing for concurrent systems: unfolding-based test selection}},
volume = {18},
year = {2016}
}
@misc{Gurbuz2018,
abstract = {Testing safety-critical systems is crucial since a failure or malfunction may result in death or serious injuries to people, equipment, or environment. An important challenge in testing is the derivation of test cases that can identify the potential faults. Model-based testing adopts models of a system under test and/or its environment to derive test artifacts. This paper aims to provide a systematic mapping study to identify, analyze, and describe the state-of-the-art advances in model-based testing for software safety. The systematic mapping study is conducted as a multi-phase study selection process using the published literature in major software engineering journals and conference proceedings. We reviewed 751 papers and 36 of them have been selected as primary studies to answer our research questions. Based on the analysis of the data extraction process, we discuss the primary trends and approaches and present the identified obstacles. This study shows that model-based testing can provide important benefits for software safety testing. Several solution directions have been identified, but further research is critical for reliable model-based testing approach for safety.},
author = {Gurbuz, Havva Gulay and Tekinerdogan, Bedir},
booktitle = {Software Quality Journal},
doi = {10.1007/s11219-017-9386-2},
issn = {15731367},
keywords = {Model-based testing,Model-driven testing,Software safety,Systematic mapping study},
month = {dec},
number = {4},
pages = {1327--1372},
publisher = {Springer New York LLC},
title = {{Model-based testing for software safety: a systematic mapping study}},
volume = {26},
year = {2018}
}
@article{Ma2019,
abstract = {Self-healing cyber-physical systems (SH-CPSs) detect and recover from faults by themselves at runtime. Testing such systems is challenging due to the complex implementation of self-healing behaviors and their interaction with the physical environment, both of which are uncertain. To this end, we propose an executable model-based approach to test self-healing behaviors under environmental uncertainties. The approach consists of a Modeling Framework of SH-CPSs (MoSH) and an accompanying Test Model Executor (TM-Executor). MoSH provides a set of modeling constructs and a methodology to specify executable test models, which capture expected system behaviors and environmental uncertainties. TM-Executor executes the test models together with the systems under test, to dynamically test their self-healing behaviors under uncertainties. We demonstrated the successful application of MoSH to specify 11 self-healing behaviors and 17 uncertainties for three SH-CPSs. The time spent by TM-Executor to perform testing activities was in the order of milliseconds, though the time spent was strongly correlated with the complexity of test models.},
author = {Ma, Tao and Ali, Shaukat and Yue, Tao},
doi = {10.1007/s10270-018-00703-y},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Cyber-physical systems,Model execution,Model-based testing,Self-healing,Uncertainty},
month = {oct},
number = {5},
pages = {2843--2873},
publisher = {Springer Verlag},
title = {{Modeling foundations for executable model-based testing of self-healing cyber-physical systems}},
volume = {18},
year = {2019}
}
@article{Ali2012,
abstract = {Model-based robustness testing requires precise and complete behavioral, robustness modeling. For example, state machines can be used to model software behavior when hardware (e. g., sensors) breaks down and be fed to a tool to automate test case generation. But robustness behavior is a crosscutting behavior and, if modeled directly, often results in large, complex state machines. These in practice tend to be error prone and difficult to read and understand. As a result, modeling robustness behavior in this way is not scalable for complex industrial systems. To overcome these problems, aspect-oriented modeling (AOM) can be employed to model robustness behavior as aspects in the form of state machines specifically designed to model robustness behavior. In this paper, we present a RobUstness Modeling Methodology (RUMM) that allows modeling robustness behavior as aspects. Our goal is to have a complete and practical methodology that covers all features of state machines and aspect concepts necessary for model-based robustness testing. At the core of RUMM is a UML profile (AspectSM) that allows modeling UML state machine aspects as UML state machines (aspect state machines). Such an approach, relying on a standard and using the target notation as the basis to model the aspects themselves, is expected to make the practical adoption of aspect modeling easier in industrial contexts. We have used AspectSM to model the crosscutting robustness behavior of a videoconferencing system and discuss the benefits of doing so in terms of reduced modeling effort and improved readability. {\textcopyright} 2011 Springer-Verlag.},
author = {Ali, Shaukat and Briand, Lionel C. and Hemmati, Hadi},
doi = {10.1007/s10270-011-0206-z},
issn = {16191366},
journal = {Software and Systems Modeling},
keywords = {Aspect-oriented modeling,Crosscutting behavior,Robustness,Robustness testing,UML profile,UML state machines},
month = {oct},
number = {4},
pages = {633--670},
title = {{Modeling robustness behavior using aspect-oriented modeling to support robustness testing of industrial systems}},
volume = {11},
year = {2012}
}
@article{Alegroth2017,
abstract = {Visual GUI Testing (VGT) is a tool-driven technique for automated GUI-based testing that uses image recognition to interact with and assert the correctness of the behavior of a system through its GUI as it is shown to the user. The technique's applicability, e.g. defect-finding ability, and feasibility, e.g. time to positive return on investment, have been shown through empirical studies in industrial practice. However, there is a lack of studies that evaluate the usefulness and challenges associated with VGT when used long-term (years) in industrial practice. This paper evaluates how VGT was adopted, applied and why it was abandoned at the music streaming application development company, Spotify, after several years of use. A qualitative study with two workshops and five well chosen employees is performed at the company, supported by a survey, which is analyzed with a grounded theory approach to answer the study's three research questions. The interviews provide insights into the challenges, problems and limitations, but also benefits, that Spotify experienced during the adoption and use of VGT. However, due to the technique's drawbacks, VGT has been abandoned for a new technique/framework, simply called the Test interface. The Test interface is considered more robust and flexible for Spotify's needs but has several drawbacks, including that it does not test the actual GUI as shown to the user like VGT does. From the study's results it is concluded that VGT can be used long-term in industrial practice but it requires organizational change as well as engineering best practices to be beneficial. Through synthesis of the study's results, and results from previous work, a set of guidelines are presented that aim to aid practitioners to adopt and use VGT in industrial practice. However, due to the abandonment of the technique, future research is required to analyze in what types of projects the technique is, and is not, long-term viable. To this end, we also present Spotify's Test interface solution for automated GUI-based testing and conclude that it has its own benefits and drawbacks.},
author = {Al{\'{e}}groth, Emil and Feldt, Robert},
doi = {10.1007/s10664-016-9497-6},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Automated testing,Case study,Guidelines,Industrial,Visual GUI testing},
month = {dec},
number = {6},
pages = {2937--2971},
publisher = {Springer New York LLC},
title = {{On the long-term use of visual gui testing in industrial practice: a case study}},
volume = {22},
year = {2017}
}
@article{Hierons2012a,
abstract = {This paper concerns the testing of a system with physically distributed interfaces, called ports, at which it interacts with its environment. We place a tester at each port and the tester at port p observes events at p only. This can lead to controllability problems, where the observations made by the tester at a port p are not sufficient for it to be able to know when to send an input. It is known that there are test objectives, such as executing a particular transition, that cannot be achieved if we restrict attention to test cases that have no controllability problems. This has led to interest in schemes where the testers at the individual ports send coordination messages to one another through an external communications network in order to overcome controllability problems. However, such approaches have largely been studied in the context of testing from a deterministic finite state machine. This paper investigates the use of coordination messages to overcome controllability problems when testing from an input output transition system and gives an algorithm for introducing sufficient messages. It also proves that the problem of minimising the number of coordination messages used is NP-hard. {\textcopyright} 2011 Springer-Verlag.},
author = {Hierons, Robert M.},
doi = {10.1007/s00446-011-0153-5},
issn = {01782770},
journal = {Distributed Computing},
keywords = {Controllability problems,Coordination messages,Distributed testing,Input output transition system},
month = {mar},
number = {1},
pages = {63--81},
title = {{Overcoming controllability problems in distributed testing from an input output transition system}},
volume = {25},
year = {2012}
}
@article{Perrouin2012,
abstract = {Software Product Lines (SPL) are difficult to validate due to combinatorics induced by variability, which in turn leads to combinatorial explosion of the number of derivable products. Exhaustive testing in such a large products space is hardly feasible. Hence, one possible option is to test SPLs by generating test configurations that cover all possible t feature interactions (t-wise). It dramatically reduces the number of test products while ensuring reasonable SPL coverage. In this paper, we report our experience on applying t-wise techniques for SPL with two independent toolsets developed by the authors. One focuses on generality and splits the generation problem according to strategies. The other emphasizes providing efficient generation. To evaluate the respective merits of the approaches, measures such as the number of generated test configurations and the similarity between them are provided. By applying these measures, we were able to derive useful insights for pairwise and t-wise testing of product lines. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {Perrouin, Gilles and Oster, Sebastian and Sen, Sagar and Klein, Jacques and Baudry, Benoit and le Traon, Yves},
doi = {10.1007/s11219-011-9160-9},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Alloy,Model-based engineering and testing,Software product lines,Test generation,t-wise and pairwise},
number = {3-4},
pages = {605--643},
publisher = {Kluwer Academic Publishers},
title = {{Pairwise testing for software product lines: Comparison of two approaches}},
volume = {20},
year = {2012}
}
@article{ISI:000385604600006,
abstract = {To be competitive and flexible, companies engage in collaborations to develop and share their competences in order to cope with the dynamic environment. Collaborative business process evaluation helps to reflect the actual functioning of business process and their performance level. In this perspective, research in assessing collaborative business process performance presents relevant guidelines in order to adapt IT solutions when business requirements evolve. In this paper, we present an analysis and assessment approach for collaborative business processes in the service-oriented architecture in order to maintain their performance in competitive markets. Our approach proposes an evaluation method using execution traces of business process combined with a high-level assessment method using key performance indicators. Our main objectives are to track the execution of collaborative business process and to analyze the performance trajectory of a business process regarding the business performance level. To collect and structure the performance knowledge (execution and measurement), we create an ontological model-based knowledge repository in order to enrich the semantics of an evaluation business process. The precise track of execution data in our approach is able to identify events that disrupt the proper functioning of processes at the runtime. From an industrial case study, we can conclude that our ontological approach can target the performance assessment of collaborative business processes effectively.},
author = {Hachicha, Maroua and Fahad, Muhammad and Moalla, N{\'{e}}jib and Ouzrout, Yacine},
doi = {10.1016/j.datak.2015.12.002},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Assessment,Collaborative business process,Execution traces,Ontology,Process mining},
month = {sep},
number = {SI},
pages = {73--89},
title = {{Performance assessment architecture for collaborative business processes in BPM-SOA-based environment}},
volume = {105},
year = {2016}
}
@article{Aichernig2019,
abstract = {Property-based testing is well suited for web-service applications, which was already shown in various case studies. For example, it has been demonstrated that JSON schemas can be used to automatically derive test case generators for web forms. In this work, we present a test case generation approach for a rule engine-driven web-service application. Business-rule models serve us as input for property-based testing. We parse these models to automatically derive generators for sequences of web-service requests together with their required form data. Property-based testing is mostly applied in the context of functional programming. Here, we define our properties in an object-oriented style in C# and its tool FsCheck. We apply our method to the business-rule models of an industrial web-service application in the automotive domain.},
author = {Aichernig, Bernhard K. and Schumi, Richard},
doi = {10.1007/s10270-017-0647-0},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Business-rule models,FsCheck,Model-based testing,Property-based testing,QuickCheck,Test case generation,Web services},
month = {apr},
number = {2},
pages = {889--911},
publisher = {Springer Verlag},
title = {{Property-based testing of web services by deriving properties from business-rule models}},
volume = {18},
year = {2019}
}
@article{10.1145/2631685,
abstract = {Web service is a widely used implementation technique under the paradigm of Service-Oriented Architecture (SOA). A service-based system is subjected to continuous evolution and regression testing is required to check whether new faults have been introduced. Based on the current scientific work of web service regression testing, this survey aims to identify gaps in current research and suggests some promising areas for further study. To this end, we performed a broad automatic search on publications in the selected electronic databases published from 2000 to 2013. Through our careful review and manual screening, a total of 30 papers have been selected as primary studies for answering our research questions. We presented a qualitative analysis of the findings, including stakeholders, challenges, standards, techniques, and validations employed in these primary studies. Our main results include the following: (1) Service integrator is the key stakeholder that largely impacts how regression testing is performed. (2) Challenges of cost and autonomy issues have been studied heavily. However, more emphasis should be put on the other challenges, such as test timing, dynamics, privacy, quota constraints, and concurrency issues. (3) Orchestration-based services have been largely studied, while little attention has been paid to either choreography-based services or semantic-based services. (4) An appreciable amount of web service regression testing techniques have been proposed, including 48 test case prioritization techniques, 10 test selection techniques, two test suite minimization techniques, and another collaborative technique. (5) Many regression test techniques have not been theoretically proven or experimentally analyzed, which limits their application in large-scale systems. We believe that our survey has identified gaps in current research work and reveals new insights for the future work.},
address = {New York, NY, USA},
author = {Qiu, Dong and Li, Bixin and Ji, Shunhui and Leung, Hareton},
doi = {10.1145/2631685},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Regression testing,Test case prioritization,Test case selection,Test suite minimization,Web service},
number = {2},
publisher = {Association for Computing Machinery},
title = {{Regression testing of web service: A systematic mapping study}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/2631685},
volume = {47},
year = {2014}
}
@article{10.1145/3293455,
abstract = {RESTful APIs are widespread in industry, especially in enterprise applications developed with a microservice architecture. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, because often the tested API is a remote service whose code is not available. In this article, we consider testing from the point of view of the developers, who have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding metrics. However, REST is not a protocol but rather a set of guidelines on how to design resources accessed over HTTP endpoints. For example, there are guidelines on how related resources should be structured with hierarchical URIs and how the different HTTP verbs should be used to represent well-defined actions on those resources. Test-case generation for RESTful APIs that only rely on white-box information of the source code might not be able to identify how to create prerequisite resources needed before being able to test some of the REST endpoints. Smart sampling techniques that exploit the knowledge of best practices in RESTful API design are needed to generate tests with predefined structures to speed up the search. We implemented our technique in a tool called EvoMaster, which is open source. Experiments on five open-source, yet non-trivial, RESTful services show that our novel technique automatically found 80 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such an approach are therefore discussed, such as the handling of SQL databases.},
address = {New York, NY, USA},
author = {Arcuri, Andrea},
doi = {10.1145/3293455},
issn = {15577392},
journal = {ACM Transactions on Software Engineering and Methodology},
keywords = {REST,Software engineering,Testing,Web service},
number = {1},
publisher = {Association for Computing Machinery},
title = {{RESTful API automated test case generation with Evomaster}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/3293455},
volume = {28},
year = {2019}
}
@article{DuBousquet2010,
abstract = {Testing and verification are two activities which have the same objective: to ensure software dependability. In the Java context, the Java Modelling Language (JML) has been proposed as specification language. It can be used both for verification and test. Usually, the JML specification is designed with a specific purpose: test or verification. This article addresses the question of reusability of a JML specification provided for one activity (resp. verification or test) in the other context (resp. test or verification). Two different case studies are considered. {\textcopyright} 2009 Springer Science+Business Media B.V.},
author = {{Du Bousquet}, Lydie and Ledru, Yves and Maury, Olivier and Oriat, Catherine and Lanet, Jean Louis},
doi = {10.1007/s10817-009-9132-y},
issn = {01687433},
journal = {Journal of Automated Reasoning},
keywords = {JML,Java,Software dependability,Testing,Verification},
month = {dec},
number = {4},
pages = {415--435},
title = {{Reusing a JML specification dedicated to verification for testing, and vice-versa: Case studies}},
volume = {45},
year = {2010}
}
@article{Felderer2016,
abstract = {Risk orientation in testing is an important means to balance quality, time-to-market, and cost of software. Especially for small and medium enterprises (SME) under high competitive and economic pressure, risk orientation can help to focus testing activities on critical areas of a software product. Although several risk-based approaches to testing are available, the topic has so far not been investigated in the context of SME, where risks are often associated with business critical issues. This article fills the gap and explores the state of risk orientation in the testing processes of SME. Furthermore, it compares the state of risk-based testing in SME to the situation in large enterprises. The article is based on a multiple case study conducted with five SME. A previous study on risk-based testing in large enterprises is used as reference for investigating the differences between risk orientation in SME and large enterprises. The findings of our study show that a strong business focus, the use of informal risk concepts, as well as the application of risk knowledge to reduce testing cost and time are key differences of risk-based testing in SME compared to large enterprises.},
author = {Felderer, Michael and Ramler, Rudolf},
doi = {10.1007/s11219-015-9289-z},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Multiple case study,Risk-based testing,SME,Small and medium enterprises,Software quality,Software risk management,Software testing,System testing,Test management,Test process improvement},
month = {sep},
number = {3},
pages = {519--548},
publisher = {Springer New York LLC},
title = {{Risk orientation in software testing processes of small and medium enterprises: an exploratory and comparative study}},
volume = {24},
year = {2016}
}
@article{LAHAMI20161,
abstract = {This paper provides a standard-based and resource aware Runtime Testing Framework For Adaptable and Distributed Systems (RTF4ADS). Based on the runtime testing approach, RTF4ADS performs safely and efficiently tests on the final operational environment of such dynamic systems. Indeed, our proposal (1) looks for a minimal set of tests to re-execute written in a standardized notation, (2) assigns the involved test components in execution nodes while respecting resources and connectivity constraints and finally (3) performs distributed testing at runtime while it prevents test processes from interfering with business processes. Implementation details of the proposed research prototype are presented. To demonstrate the validity and the usefulness of RTF4ADS, a case study in the healthcare domain implemented using the Open Services Gateway Initiative (OSGi) platform is illustrated. The experiments that we conducted show a reasonable overhead introduced by RTF4ADS. They also demonstrate the efficiency of this framework in testing dynamic and distributed systems while reducing side effects of this validation technique.},
author = {Lahami, Mariam and Krichen, Moez and Jmaiel, Mohamed},
doi = {10.1016/j.scico.2016.02.002},
issn = {01676423},
journal = {Science of Computer Programming},
keywords = {Distributed test execution,Dynamic structural adaptations,Resource awareness,Runtime testing,TTCN-3},
pages = {1--28},
title = {{Safe and efficient runtime testing framework applied in dynamic and distributed systems}},
url = {http://www.sciencedirect.com/science/article/pii/S0167642316000460},
volume = {122},
year = {2016}
}
@article{Huang2019,
abstract = {In this paper, a novel safety-related variant of complete test suites for finite state machines is introduced. Under certain hypotheses which are similar to the ones used in the well-known W-Method and its improved versions, the new method guarantees to uncover every violation of safety properties from a certain well-defined class, while erroneous behaviour without safety relevance may remain undetected. While the method can be based on any of the known complete strategies for FSM testing, its most effective variant is based on the H-method, and this variant is presented in detail, denoted as the Safety-complete H-Method. It is guaranteed that application of the Safety-complete H-Method always results in less or equally many test cases than when applying the original H-Method. In well-defined situations that can be pre-determined from the reference model, the Safety-complete H-Method leads to a substantial reduction of test cases in comparison to the size of the analogous H test suites. We advocate this new test suite for situations, where exhaustive testing of the complete system is too expensive. In these cases, strong guarantees with respect to fault coverage should only be given for the errors representing safety violations, while it may be considered as acceptable if less critical errors remain undetected.},
author = {ling Huang, Wen and {\"{O}}zoguz, Sadik and Peleska, Jan},
doi = {10.1007/s11219-018-9421-y},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Complete testing theories,Model-based testing,Safety},
month = {jun},
number = {2},
pages = {589--613},
publisher = {Springer New York LLC},
title = {{Safety-complete test suites}},
volume = {27},
year = {2019}
}
@article{Jan2019,
abstract = {Modern web applications often interact with internal web services, which are not directly accessible to users. However, malicious user inputs can be used to exploit security vulnerabilities in web services through the application front-ends. Therefore, testing techniques have been proposed to reveal security flaws in the interactions with back-end web services, e.g., XML Injections (XMLi). Given a potentially malicious message between a web application and web services, search-based techniques have been used to find input data to mislead the web application into sending such a message, possibly compromising the target web service. However, state-of-the-art techniques focus on (search for) one single malicious message at a time. Since, in practice, there can be many different kinds of malicious messages, with only a few of them which can possibly be generated by a given front-end, searching for one single message at a time is ineffective and may not scale. To overcome these limitations, we propose a novel co-evolutionary algorithm (COMIX) that is tailored to our problem and uncover multiple vulnerabilities at the same time. Our experiments show that COMIX outperforms a single-target search approach for XMLi and other multi-target search algorithms originally defined for white-box unit testing.},
author = {Jan, Sadeeq and Panichella, Annibale and Arcuri, Andrea and Briand, Lionel},
doi = {10.1007/s10664-019-09707-8},
file = {::},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Code injection vulnerabilities,Search-based software engineering,Security testing},
month = {dec},
number = {6},
pages = {3696--3729},
publisher = {Springer},
title = {{Search-based multi-vulnerability testing of XML injections in web applications}},
volume = {24},
year = {2019}
}
@article{DosSantos2020,
abstract = {Testing a software system is an important step approach to ensuring quality, safety, and reliability in safety-critical systems (SCS). Several authors have published new approaches to improve the processes of testing safety requirements taking into consideration existing processes that seek to improve techniques and contribute positively with software developers. This article aims to investigate the main approaches to requirements testing, particularly focusing on safety requirements in the context of SCS. We investigated how these approaches have been developed and what contributions they provide to academia and industry. We evaluated the pros and cons of the approaches and how they related to the joint work of requirements engineers and testers. We performed a systematic literature review (SLR), selecting 53 papers published between 1990 and 2018. Our research was conducted according to the guidelines proposed by Kitchenham and Biolchini. The results of this SLR point out to the new research related to the software and safety-critical systems testing. The results show issues in the integration of requirements engineers with the application test team and gaps in the approaches found, particularly in the applications of the techniques in the industry setting. Moreover, several approaches are presented to solve problems and help to prevent future problems. The results of this research point to the main approaches to requirements testing and their use in academia and industry, as well as the advantages and disadvantages. The shortcomings allow us to suggest new research in safety-critical systems in the scope of validation, verification, specification, and testing of safety requirements, as well as to integrate test teams with requirements engineers in order to get better results. Based on the results, we suggest future studies for improvements in the requirements testing techniques to improve the integration of safety requirements and test cases.},
author = {dos Santos, Jemison and Martins, Luiz Eduardo G. and {de Santiago J{\'{u}}nior}, Valdivino A. and Povoa, Lucas Venezian and dos Santos, Luciana Brasil R.},
doi = {10.1007/s00766-019-00325-w},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Requirements testing,Safety requirements,Safety-critical systems},
month = {sep},
number = {3},
pages = {317--337},
publisher = {Springer},
title = {{Software requirements testing approaches: a systematic literature review}},
volume = {25},
year = {2020}
}
@article{Houhamdi2011690,
abstract = {Problem statement: In recent years, Agent-Oriented Software Engineering (AOSE) methodologies are proposed to develop complex distributed systems based upon the agent paradigm. The implementation for such systems has usually the form of Multi-Agent Systems (MAS). Testing of MAS is a challenging task because these systems are often programmed to be autonomous and deliberative and they operate in an open world, which requires context awareness. Approach: We introduce a novel approach for goal-oriented software integration testing. It specifies an integration testing process that complements the goal oriented methodology Tropos and strengthens the mutual relationship between goal analysis and testing. Results: The derived test suites from the system goals can be used to observe emergent properties resulting from agent interactions and make sure that a group of agents and contextual resources work correctly together. Conclusion: This approach defines a structured and comprehensive integration test suite derivation process for engineering software agents by providing a systematic way of deriving test cases from goal analysis. {\textcopyright} 2011 Science Publications.},
annote = {cited By 5},
author = {Houhamdi, Zina and Athamena, Belkacem},
doi = {10.3844/jcssp.2011.690.697},
issn = {15493636},
journal = {Journal of Computer Science},
keywords = {Agent-oriented software engineering (aose),Architectural design,Collaborative goals,Deriving test,Detailed design,Integration testing,Multi-agent systems (MAS),Object-Oriented (OO),Test case generation,Tropos methodology},
number = {5},
pages = {690--697},
title = {{Structured integration test suite generation process for multi-agent system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053089888&doi=10.3844%2Fjcssp.2011.690.697&partnerID=40&md5=1016ba71eaff44bf3882272f2925422a},
volume = {7},
year = {2011}
}
@article{10.1145/2089116.2089122,
abstract = {Message sequence charts (MSCs) are a widely used visual formalism for scenario-based specifications of distributed reactive systems. In its conventional usage, an MSC captures an interaction snippet between concrete objects in the system. This leads to voluminous specifications when the system contains several objects that are behaviorally similar. MSCs also play an important role in the model-based testing of reactive systems, where they may be used for specifying (partial) system behaviors, describing test generation criteria, or representing test cases. However, since the number of processes in a MSC specification are fixed, model-based testing of systems consisting of process classes may involve a significant amount of rework: for example, reconstructing system models, or regenerating test cases for systems differing only in the number of processes of various types. In this article we propose a scenario-based notation, called symbolic message sequence charts (SMSCs), for modeling, simulation, and testing of process classes. SMSCs are a lightweight syntactic and semantic extension of MSCs where, unlike MSCs, a SMSC lifeline can denote some/all objects from a collection. Our extensions give us substantially more modeling power. Moreover, we present an abstract execution semantics for (structured collections of) SMSCs. This allows us to validate MSC-based system models capturing interactions between large, or even unbounded, number of objects. Finally, we describe a SMSC-based testing methodology for process classes, which allows generation of test cases for new object configurations with minimal rework. Since our SMSC extensions are only concerned with MSC lifelines, we believe that they can be integrated into existing standards such as UML 2.0. We illustrate our SMSC-based framework for modeling, simulation, and testing of process classes using a weather-update controller case-study from NASA. {\textcopyright} 2012 ACM.},
address = {New York, NY, USA},
author = {Roychoudhury, Abhik and Goel, Ankit and Sengupta, Bikram},
doi = {10.1145/2089116.2089122},
issn = {1049331X},
journal = {ACM Transactions on Software Engineering and Methodology},
keywords = {Message sequence charts,Symbolic execution,Test generation,Unified modeling language (UML)},
number = {2},
publisher = {Association for Computing Machinery},
title = {{Symbolic message sequence charts}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/2089116.2089122},
volume = {21},
year = {2012}
}
@article{Noroozi2015,
abstract = {We present and compare different notions of conformance testing based on labeled transition systems. We formulate and prove several theorems which enable using synchronous conformance testing techniques such as input–output conformance testing (ioco ) in order to test implementations only accessible through asynchronous communication channels. These theorems define when the synchronous test cases are sufficient for checking all aspects of conformance that are observable by asynchronous interaction with the implementation under test.},
author = {Noroozi, Neda and Khosravi, Ramtin and Mousavi, Mohammad Reza and Willemse, Tim A.C.},
doi = {10.1007/s10270-012-0302-8},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Asynchronous conformance testing,Conformance testing,Internal choice implementation,Queue context,ioco},
number = {1},
pages = {149--172},
publisher = {Springer Verlag},
title = {{Synchrony and asynchrony in conformance testing}},
volume = {14},
year = {2015}
}
@article{Dadeau2019,
abstract = {This article describes a new property- and model-based testing approach using UML/OCL models, driven by temporal property patterns and a tool for assisting the temporal properties formalization. The patterns are expressed in the TOCL language, an adaptation of Dwyer's property patterns to OCL. The patterns are used to formalize temporal requirements without having to learn a complex temporal logics such as LTL or CTL. From these properties, automata are automatically computed. These can be used for two purposes. First, it is possible to evaluate the quality of a test suite by measuring the coverage of a property using its associated automaton. Second, the automaton can be used to drive the test generation in order to produce complementary test cases. To this end, we defined dedicated coverage criteria, targeting specific events of the property, and aiming either at illustrating the expected behaviour of the system, or checking its robustness w.r.t. the property. However, it was observed that the semantics of the property language may be more subtle that it seems. To facilitate the adoption of the language by industrials, we have proposed a tool-supported assistant for property design, aiming to help the validation engineer choosing which constructs faithfully correspond to his intention. This approach has been experimented on several case studies with industrial partners. It has shown its interest for software validation, providing useful information thanks to adequate traceability features.},
author = {Dadeau, Fr{\'{e}}d{\'{e}}ric and Fourneret, Elizabeta and Bouchelaghem, Abir},
doi = {10.1007/s10270-017-0635-4},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Behavioural model,Coverage measure,Property design,Property patterns,Test generation},
month = {apr},
number = {2},
pages = {865--888},
publisher = {Springer Verlag},
title = {{Temporal property patterns for model-based testing from UML/OCL}},
volume = {18},
year = {2019}
}
@article{Azzouzi2012486,
abstract = {The development of distributed testing frameworks is more complex, where the implementation process must consider the mechanisms and functions required to support interaction as long as the communication and the coordination between distributed testing components. The typical reactions of such systems are the generation of errors 'set: time outs, locks, observability, controllability and synchronization problems. The first contribution in this study present a way to control the test execution of distributed testing components by introducing the synchronization messages and we show how the problems of control and synchronization can be solved by the same process. In other side, we show that in practice the distributed testing process must not only check if the exchanged events have been observed, but also the dates when these events have been occurred and then the distributed testing frameworks must consider some timing constraints. {\textcopyright} 2005 - 2012 JATIT & LLS. All rights reserved.},
annote = {cited By 0},
author = {Azzouzi, Salma and Benattou, Mohammed and Charaf, Hassan},
issn = {18173195},
journal = {Journal of Theoretical and Applied Information Technology},
keywords = {Controllability,Distributed testing,Observability,Synchronization,Timing Constraints},
number = {1},
pages = {486--498},
title = {{Test execution control with timing constraints for testing distributed systems}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874551626&partnerID=40&md5=e4a067422c93a1c5ffa40b81dd04a42b},
volume = {46},
year = {2012}
}
@article{Nogueira2014,
abstract = {We present a strategy for the automatic generation of test cases from parametrised use case templates that capture control flow, state, input and output. Our approach allows test scenario selection based on particular traces or states of the model. The templates are internally represented as CSP processes with explicit input and output alphabets, and test generation is expressed as counter-examples of refinement checking, mechanised using the FDR tool. Soundness is addressed through an input-output conformance relation formally defined in the CSP traces model. This purely process algebraic characterisation of testing has some potential advantages, mainly an easy automation of conformance verification and test case generation via model checking, without the need to develop any explicit algorithm. {\textcopyright} 2012 British Computer Society.},
author = {Nogueira, Sidney and Sampaio, Augusto and Mota, Alexandre},
doi = {10.1007/s00165-012-0258-z},
issn = {1433299X},
journal = {Formal Aspects of Computing},
keywords = {CSP,Conformance testing,Natural language,Test generation,Test model,Use cases},
number = {3},
pages = {441--490},
publisher = {Springer-Verlag London Ltd},
title = {{Test generation from state based use case models}},
volume = {26},
year = {2014}
}
@article{Wong2015,
abstract = {We present a range of testing techniques for the Abstract Behavioral Specification (ABS) language and apply them to an industrial case study. ABS is a formal modeling language for highly variable, concurrent, component-based systems. The nature of these systems makes them susceptible to the introduction of subtle bugs that are hard to detect in the presence of steady adaptation. While static analysis techniques are available for an abstract language such as ABS, testing is still indispensable and complements analytic methods. We focus on fully automated testing techniques including black-box and glass-box test generation as well as runtime assertion checking, which are shown to be effective in an industrial setting.},
author = {Wong, Peter Y.H. and Bubel, Richard and de Boer, Frank S. and G{\'{o}}mez-Zamalloa, Miguel and de Gouw, Stijn and H{\"{a}}hnle, Reiner and Meinke, Karl and Sindhu, Muddassar Azam},
doi = {10.1007/s10009-014-0301-x},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Automated testing,Black-box testing,Glass-box testing,Industrial case study,Runtime assertion checking},
number = {1},
pages = {107--119},
publisher = {Springer Verlag},
title = {{Testing abstract behavioral specifications}},
volume = {17},
year = {2015}
}
@article{Offutt2019,
abstract = {Web applications are now used in every aspect of our lives to manage work, provide products and services, read email, and provide entertainment. The software technologies used to build web applications provide features that help designers provide flexible functionality, but that are challenging to model and test. In particular, the network-based request-response model of programming means that web applications are inherently “stateless” and implicitly concurrent. They are stateless because a new network connection is made for each request (for example, when a user clicks a submit button). Thus, the server does not, by default, recognize multiple requests from the same user. Web applications are also concurrent because multiple users can use the same web application at the same time, creating contention for the same resources. Unfortunately, most web application testing does not adequately evaluate these aspects of web applications, leaving many software faults in deployed web applications. Part of this problem is because most traditional software modeling tools (such as UML) do not have built-in support for the stateless and concurrent aspects of web applications. This research project uses a novel model that is based on Petri nets to describe certain aspects of the behavior of web applications. This paper makes several contributions. We present a novel technique to design tests from this model that explicitly tests concurrency in web applications. We present novel coverage criteria that are defined on the Petri net model. We present results from an empirical study of 18 web applications with 343 components and 30,186 lines of code, followed by a case study on a large industrial web application. The tests found significantly more faults than traditional requirements-based tests, with fewer tests.},
author = {Offutt, Jeff and Thummala, Sunitha},
doi = {10.1007/s10270-018-0655-8},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Model-based testing,Petri nets,Test criteria,Web applications},
month = {apr},
number = {2},
pages = {913--936},
publisher = {Springer Verlag},
title = {{Testing concurrent user behavior of synchronous web applications with Petri nets}},
volume = {18},
year = {2019}
}
@article{Andrade2012,
abstract = {Modelling and testing of reactive systemswith interruptions are discussed. These systems are commonly found in portable devices, where interruptions to a running application can be demanded at any time, due to concurrent execution of processes sharing a single resource, such as screen, as well as arrival of calls from network distributed services. Since the possible number of combinations of allowed interruptions is large, proper test case selection activities need to be performed. But, in order to systematically investigate and select test cases, it is fundamental to explicitly model interruption behaviour in a compositional way, avoiding the need for explicit enumeration. This work presents a strategy for testing interruptions in reactive systems that covers modelling for testing of systemswith interruptions, generation and selection of sound test cases. The strategy is supported by the LTS-BT tool. Moreover, a formal model of an environment devoted to execution of test cases with interruptions is presented. Finally, a case study illustrates its applicability in the mobile phone application domain. {\textcopyright} 2011 BCS.},
author = {Andrade, Wilkerson L. and MacHado, Patr{\'{i}}cia D.L.},
doi = {10.1007/s00165-011-0197-0},
issn = {09345043},
journal = {Formal Aspects of Computing},
keywords = {Interruption,Model-based testing,Reactive systems,Test case generation},
month = {may},
number = {3},
pages = {331--353},
title = {{Testing interruptions in reactive systems}},
volume = {24},
year = {2012}
}
@article{ISI:000502661100001,
abstract = {Microservice architectures (MSA) is an emerging software architectural paradigm for service-oriented applications, well-suited for dynamic contexts requiring loosely coupled independent services, frequent software releases and decentralized governance. A key problem in the engineering of MSA applications is the estimate of their reliability, which is difficult to perform prior to release due frequent releases/service upgrades, dynamic service interactions, and changes in the way customers use the applications. This paper presents an in vivo testing method, named EMART, to faithfully assess the reliability of an MSA application in operation. EMART is based on an adaptive sampling strategy, leveraging monitoring data about microservices usage and failure/success of user demands. We present results of evaluation of estimation accuracy, confidence and efficiency, through a set of controlled experiments with publicly available subjects. {\textcopyright} 2019 John Wiley & Sons, Ltd.},
author = {Pietrantuono, Roberto and Russo, Stefano and Guerriero, Antonio},
doi = {10.1002/stvr.1725},
issn = {10991689},
journal = {Software Testing Verification and Reliability},
keywords = {in vivo testing,microservice architecture,software reliability},
month = {mar},
number = {2},
title = {{Testing microservice architectures for operational reliability}},
volume = {30},
year = {2020}
}
@article{Ciancone2014,
abstract = {Model-driven development is gaining importance in software engineering practice. This increasing usage asks for a new generation of testing tools to verify correctness and suitability of model transformations. This paper presents a novel approach to unit testing QVT operational (QVTO) transformations, which overcomes limitations of currently available tools. Our proposal, called MANTra (Model trANsformation Testing), allows software developers to design test cases directly within the QVTO language and verify them without moving from the transformation environment. MANTra is also available as an eclipse feature that can be easily integrated into established development practice. {\textcopyright} 2013 Springer-Verlag London.},
author = {Ciancone, Andrea and Filieri, Antonio and Mirandola, Raffaela},
doi = {10.1007/s11334-013-0208-9},
issn = {16145046},
journal = {Innovations in Systems and Software Engineering},
keywords = {MANTra,MDA,MDE,Model-driven transformation,QVT,QVTO,Testing,Transformation testing},
month = {mar},
number = {1},
pages = {19--32},
title = {{Testing operational transformations in model-driven engineering}},
volume = {10},
year = {2014}
}
@inproceedings{DeAlmeida2010,
abstract = {Peer-to-peer (P2P) offers good solutions for many applications such as large data sharing and collaboration in social networks. Thus, it appears as a powerful paradigm to develop scalable distributed applications, as reflected by the increasing number of emerging projects based on this technology. However, building trustworthy P2P applications is difficult because they must be deployed on a large number of autonomous nodes, which may refuse to answer to some requests and even leave the system unexpectedly. This volatility of nodes is a common behavior in P2P systems and may be interpreted as a fault during tests (i.e., failed node). In this work, we present a framework and a methodology for testing P2P applications. The framework is based on the individual control of nodes, allowing test cases to precisely control the volatility of nodes during their execution. We validated this framework through implementation and experimentation on an open-source P2P system. The experimentation tests the behavior of the system on different conditions of volatility and shows how the tests were able to detect complex implementation problems. {\textcopyright} Springer Science+Business Media, LLC 2009.},
author = {{De Almeida}, Eduardo Cunha and Suny{\'{e}}, Gerson and {Le Traon}, Yves and Valduriez, Patrick},
booktitle = {Empirical Software Engineering},
doi = {10.1007/s10664-009-9124-x},
issn = {13823256},
keywords = {Distributed hash tables (DHT),Experimental procedure,Peer-to-peer systems,Software testing,Testing methodology},
month = {aug},
number = {4},
pages = {346--379},
title = {{Testing peer-to-peer systems}},
volume = {15},
year = {2010}
}
@article{Merayo2011,
abstract = {Stream X-machines have been used to specify real systems where complex data structures. They are a variety of extended finite state machine where a shared memory is used to represent communications between the components of systems. In this paper we introduce an extension of the Stream X-machines formalism in order to specify systems that present temporal requirements. We add time in two different ways. First, we consider that (output) actions take time to be performed. Second, our formalism allows to specify timeouts. Timeouts represent the time a system can wait for the environment to react without changing its internal state. Since timeous affect the set of available actions of the system, a relation focusing on the functional behavior of systems, that is, the actions that they can perform, must explicitly take into account the possible timeouts. In this paper we also propose a formal testing methodology allowing to systematically test a system with respect to a specification. Finally, we introduce a test derivation algorithm. Given a specification, the derived test suite is sound and complete, that is, a system under test successfully passes the test suite if and only if this system conforms to the specification. {\textcopyright} 2009 Springer-Verlag.},
author = {Merayo, Mercedes G. and N{\'{u}}{\~{n}}ez, Manuel and Hierons, Robert M.},
doi = {10.1007/s10270-009-0126-3},
issn = {16191366},
journal = {Software and Systems Modeling},
keywords = {Formal testing,Stream X-machines,Timed systems},
month = {may},
number = {2},
pages = {201--217},
title = {{Testing timed systems modeled by Stream X-machines}},
volume = {10},
year = {2011}
}
@inproceedings{Czutro2010,
abstract = {Efficient utilization of the inherent parallelism of multi-core architectures is a grand challenge in the field of electronic design automation (EDA).OneEDAalgorithm associated with a high computational cost is automatic test pattern generation (ATPG). We present the ATPG tool TIGUAN based on a thread-parallel SAT solver. Due to a tight integration of the SAT engine into the ATPG algorithm and a carefully chosen mix of various optimization techniques, multi-million-gate industrial circuits are handled without aborts. TIGUANsupports both conventional single-stuck-at faults and sophisticated conditional multiple stuck-at faults which allows to generate patterns for non-standard fault models. We demonstrate how TIGUAN can be combined with conventional structural ATPG to extract full benefit of the intrinsic strengths of both approaches. {\textcopyright} Springer Science+Business Media, LLC 2009.},
author = {Czutro, Alexander and Polian, Ilia and Lewis, Matthew and Engelke, Piet and Reddy, Sudhakar M. and Becker, Bernd},
booktitle = {International Journal of Parallel Programming},
doi = {10.1007/s10766-009-0124-7},
issn = {08857458},
keywords = {SAT-based automatic test pattern generation,Thread-parallel SAT},
month = {jun},
number = {3-4},
pages = {185--202},
title = {{Thread-Parallel integrated test pattern generator utilizing satisfiability analysis}},
volume = {38},
year = {2010}
}
@article{Hierons2014,
abstract = {In order to test systems that have physically distributed interfaces, called ports, we might use a distributed approach in which there is a separate tester at each port. If the testers do not synchronise during testing then we cannot always determine the relative order of events observed at different ports and this leads to new notions of correctness that have been described using corresponding implementation relations. We study the situation in which each tester has a local clock and timestamps its observations. If we know nothing about how the local clocks relate then this does not affect the implementation relation while if the local clocks agree exactly then we can reconstruct the sequence of observations made. In practice, however, we are likely to be between these extremes: the local clocks will not agree exactly but we have some information regarding how they can differ. We start by assuming that a local tester interacts synchronously with the corresponding port of the system under test and then extend this to the case where communications can be asynchronous, considering both the first-in-first-out (FIFO) case and the non-FIFO case. The new implementation relations are stronger than implementation relations for distributed testing that do not use timestamps but still reflect the distributed nature of observations. This paper explores these alternatives and derives corresponding implementation relations. {\textcopyright} 2014 Springer-Verlag Berlin Heidelberg.},
author = {Hierons, Robert M. and Merayo, Mercedes G. and N{\'{u}}{\~{n}}ez, Manuel},
doi = {10.1007/s00446-014-0208-5},
issn = {01782770},
journal = {Distributed Computing},
keywords = {Distributed systems,Model based testing,Timed systems},
number = {3},
pages = {181--201},
publisher = {Springer Verlag},
title = {{Timed implementation relations for the distributed test architecture}},
volume = {27},
year = {2014}
}
@article{10.1145/3126510,
abstract = {In order to test the performance and verify the correctness of Cyber-Physical Systems (CPS), the timing constraints on the system behavior must be met. Signal Temporal Logic (STL) can efficiently and succinctly capture the timing constraints of a given system model. However, many timing constraints on CPS are more naturally expressed in terms of events on signals. While it is possible to specify event-based timing constraints in STL, such statements can quickly become long and arcane in even simple systems. Timing constraints for CPS, which can be large and complex systems, are often associated with tolerances, the expression of which can make the timing constraints even more cumbersome using STL. This paper proposes a new logic, Timestamp Temporal Logic (TTL), to provide a definitional extension of STL that more intuitively expresses the timing constraints of distributed CPS. TTL also allows for a more natural expression of timing tolerances. Additionally, this paper outlines a methodology to automatically generate logic code and programs to monitor the expressed timing constraints. Since our TTL monitoring logic evaluates the timing constraints using only the timestamps of the required events on the signal, the TTL monitoring logic has significantly less memory footprint when compared to traditional STL monitoring logic, which stores the signal value at the required sampling frequency. The key contribution of this paper is a scalable approach for online monitoring of the timing constraints. We demonstrate the capabilities of TTL and our methodology for online monitoring of TTL constraints on two case studies: 1) Synchronization and phase control of two generators and, 2) Simultaneous image capture using distributed cameras for 3D image reconstruction.},
address = {New York, NY, USA},
author = {Mehrabian, Mohammadreza and Khayatian, Mohammad and Shrivastava, Aviral and Eidson, John C. and Derler, Patricia and Andrade, Hugo A. and Li-Baboud, Ya Shian and Griffor, Edward and Weiss, Marc and Stanton, Kevin},
doi = {10.1145/3126510},
issn = {15583465},
journal = {ACM Transactions on Embedded Computing Systems},
keywords = {CPS,Cyber-physical systems,Real-time systems,Safety critical systems,Time testing,Timing constraints,Verification},
number = {5s},
publisher = {Association for Computing Machinery},
title = {{Timestamp Temporal Logic (TTL) for testing the timing of Cyber-Physical Systems}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/3126510},
volume = {16},
year = {2017}
}
@article{Metsa2014,
abstract = {Aspect-oriented software testing is emerging as an important alternative to conventional procedural and object-oriented testing techniques. This paper reports experiences from two case studies where aspects were used for the testing of embedded software in the context of an industrial application. In the first study, we used code-level aspects for testing non-functional properties. The methodology we used for deriving test aspect code was based on translating high-level requirements into test objectives, which were then implemented using test aspects in AspectC++. In the second study, we used high-level visual scenario-based models for the test specification, test generation, and aspect-based test execution. To specify scenario-based tests, we used a UML2-compliant variant of live sequence charts. To automatically generate test code from the models, a modified version of the S2A Compiler, outputting AspectC++ code, was used. Finally, to examine the results of the tests, we used the Tracer, a prototype tool for model-based trace visualization and exploration. The results of the two case studies show that aspects offer benefits over conventional techniques in the context of testing embedded software; these benefits are discussed in detail. Finally, towards the end of the paper, we also discuss the lessons learned, including the technological and other barriers to the future successful use of aspects in the testing of embedded software in industry. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Mets{\"{a}}, Jani and Maoz, Shahar and Katara, Mika and Mikkonen, Tommi},
doi = {10.1007/s11219-012-9193-8},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Aspect-oriented programming,Case studies,Embedded software,Software testing},
number = {2},
pages = {185--213},
publisher = {Springer New York LLC},
title = {{Using aspects for testing of embedded software: Experiences from two industrial case studies}},
volume = {22},
year = {2014}
}
@article{Hierons2012b,
abstract = {Formal methods are one of the most important approaches to increasing the confidence in the correctness of software systems. A formal specification can be used as an oracle in testing since one can determine whether an observed behaviour is allowed by the specification. This is an important feature of formal testing: behaviours of the system observed in testing are compared with the specification and ideally this comparison is automated. In this paper we study a formal testing framework to deal with systems that interact with their environment at physically distributed interfaces, called ports, and where choices between different possibilities are probabilistically quantified. Building on previous work, we introduce two families of schedulers to resolve nondeterministic choices among different actions of the system. The first type of schedulers, which we call global schedulers, resolves nondeterministic choices by representing the environment as a single global scheduler. The second type, which we call localised schedulers, models the environment as a set of schedulers with there being one scheduler for each port. We formally define the application of schedulers to systems and provide and study different implementation relations in this setting. {\textcopyright} 2012 BCS.},
author = {Hierons, Robert M. and {\'{N}}ũnez, Manuel},
doi = {10.1007/s00165-012-0244-5},
issn = {09345043},
journal = {Formal Aspects of Computing},
keywords = {Distributed systems,Formal testing,Probabilistic systems,Schedulers},
month = {jul},
number = {4-6},
pages = {679--699},
title = {{Using schedulers to test probabilistic distributed systems}},
volume = {24},
year = {2012}
}
@article{Alegroth2015,
abstract = {In today's software development industry, high-level tests such as Graphical User Interface (GUI) based system and acceptance tests are mostly performed with manual practices that are often costly, tedious and error prone. Test automation has been proposed to solve these problems but most automation techniques approach testing from a lower level of system abstraction. Their suitability for high-level tests has therefore been questioned. High-level test automation techniques such as Record and Replay exist, but studies suggest that these techniques suffer from limitations, e.g. sensitivity to GUI layout or code changes, system implementation dependencies, etc. Visual GUI Testing (VGT) is an emerging technique in industrial practice with perceived higher flexibility and robustness to certain GUI changes than previous high-level (GUI) test automation techniques. The core of VGT is image recognition which is applied to analyze and interact with the bitmap layer of a system's front end. By coupling image recognition with test scripts, VGT tools can emulate end user behavior on almost any GUI-based system, regardless of implementation language, operating system or platform. However, VGT is not without its own challenges, problems and limitations (CPLs) but, like for many other automated test techniques, there is a lack of empirically-based knowledge of these CPLs and how they impact industrial applicability. Crucially, there is also a lack of information on the cost of applying this type of test automation in industry. This manuscript reports an empirical, multi-unit case study performed at two Swedish companies that develop safety-critical software. It studies their transition from manual system test cases into tests automated with VGT. In total, four different test suites that together include more than 300 high-level system test cases were automated for two multi-million lines of code systems. The results show that the transitioned test cases could find defects in the tested systems and that all applicable test cases could be automated. However, during these transition projects a number of hurdles had to be addressed; a total of 58 different CPLs were identified and then categorized into 26 types. We present these CPL types and an analysis of the implications for the transition to and use of VGT in industrial software development practice. In addition, four high-level solutions are presented that were identified during the study, which would address about half of the identified CPLs. Furthermore, collected metrics on cost and return on investment of the VGT transition are reported together with information about the VGT suites' defect finding ability. Nine of the identified defects are reported, 5 of which were unknown to testers with extensive experience from using the manual test suites. The main conclusion from this study is that even though there are many challenges related to the transition and usage of VGT, the technique is still valuable, flexible and considered cost-effective by the industrial practitioners. The presented CPLs also provide decision support in the use and advancement of VGT and potentially other automated testing techniques similar to VGT, e.g. Record and Replay.},
author = {Al{\'{e}}groth, Emil and Feldt, Robert and Ryrholm, Lisa},
doi = {10.1007/s10664-013-9293-5},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Challenges,Development cost,Industrial case study,Problems and Limitations,System and acceptance test automation,Visual GUI Testing},
month = {jun},
number = {3},
pages = {694--744},
publisher = {Kluwer Academic Publishers},
title = {{Visual GUI testing in practice: challenges, problemsand limitations}},
volume = {20},
year = {2015}
}
@article{Wang2020,
abstract = {Considering the different weights of various heterogeneity node importance in power grid and communication network and the possibility of the power grid island as well as the independent operation of the local area network, a more accurate method is proposed to evaluate the fragility of a cyber-physical system. According to the quo of grid construction and the background of cyber-physical fusion, the hierarchical system and distributed system model are established, respectively. Simulation results of conducting random attack and deliberate attack on two systems indicate that the proposed method is correct and better than others and it can identify the balance of the network structure. The distributed system is more robust than the hierarchical system under different weighting factors, while the robustness of the hierarchical system is more sensitive to the weighting factor.},
author = {Wang, Bo and Ma, Hengrui and Wang, Xunting and Deng, Guiping and Yang, Yan and Wan, Shaohua},
doi = {10.1007/s11227-019-03027-w},
issn = {15730484},
journal = {Journal of Supercomputing},
keywords = {Cyber-physical system,Node heterogeneity,Smart grid,Vulnerability assessment},
month = {apr},
number = {4},
pages = {2622--2642},
publisher = {Springer},
title = {{Vulnerability assessment method for cyber-physical system considering node heterogeneity}},
volume = {76},
year = {2020}
}
@article{Wang2010,
abstract = {In this paper, we propose an approach to simulation and validation of Web services choreography described by WS-CDL. Simulation of Web services choreography is important to analyze and test the choreography model written by SOA designers. On the other hand, WS-CDL specification regulates the correct behaviors a WS-CDL document has to obey. Thus, constraints are specified in WS-CDL specification including static, dynamic and implementation ones. We developed a relational calculus to capture those constraints precisely, and a corresponding algorithm for relational analysis is performed with the WS-CDL parser and simulator. Last but not least, a tool called CDLchecker is developed to facilitate designers to simulate and validate WS-CDL documents. {\textcopyright} 2010 Springer-Verlag London Limited.},
author = {Wang, Zheng and Zhou, Lei and Zhao, Yongxin and Ping, Jing and Xiao, Hao and Pu, Geguang and Zhu, Huibiao},
doi = {10.1007/s11761-010-0072-5},
issn = {18632386},
journal = {Service Oriented Computing and Applications},
keywords = {SOA,Simulation,Validation,WS-CDL,Web services},
month = {dec},
number = {4},
pages = {291--305},
title = {{Web services choreography validation}},
volume = {4},
year = {2010}
}
@article{10.1145/3158134,
abstract = {Random testing has proven to be an effective way to catch bugs in distributed systems in the presence of network partition faults. This is surprising, as the space of potentially faulty executions is enormous, and the bugs depend on a subtle interplay between sequences of operations and faults.We provide a theoretical justification of the effectiveness of random testing in this context. First, we show a general construction, using the probabilistic method from combinatorics, that shows that whenever a random test covers a fixed coverage goal with sufficiently high probability, a small randomly-chosen set of tests achieves full coverage with high probability. In particular, we show that our construction can give test sets exponentially smaller than systematic enumeration. Second, based on an empirical study of many bugs found by random testing in production distributed systems, we introduce notions of test coverage relating to network partition faults which are effective in finding bugs. Finally, we show using combinatorial arguments that for these notions of test coverage we introduce, we can find a lower bound on the probability that a random test covers a given goal. Our general construction then explains why random testing tools achieve good coverage---and hence, find bugs---quickly.While we formulate our results in terms of network partition faults, our construction provides a step towards rigorous analysis of random testing algorithms, and can be applicable in other scenarios.},
address = {New York, NY, USA},
author = {Majumdar, Rupak and Niksic, Filip},
doi = {10.1145/3158134},
issn = {2475-1421},
journal = {Proceedings of the ACM on Programming Languages},
keywords = {distributed systems,network partition faults,probabilistic method,random testing},
number = {POPL},
pages = {1--24},
publisher = {Association for Computing Machinery},
title = {{Why is random testing effective for partition tolerance bugs?}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/3158134},
volume = {2},
year = {2018}
}
@article{Daoudagh2020,
abstract = {In the context of access control systems, testing activity is among the most adopted means to assure that sensible information or resources are correctly accessed. In XACML-based access control systems, incoming access requests are transmitted to the policy decision point (PDP) that grants or denies the access based on the defined XACML policies. The criticality of a PDP component requires an intensive testing activity consisting in probing such a component with a set of requests and checking whether its responses grant or deny the requested access as specified in the policy. Existing approaches for improving manual derivation of test requests such as combinatorial ones do not consider policy function semantics and do not provide a verdict oracle. In this paper, we introduce XACMET, a novel approach for systematic generation of XACML requests as well as automated model-based oracle derivation. The main features of XACMET are as follows: (i) it defines a typed graph, called the XAC-Graph, that models the XACML policy evaluation; (ii) it derives a set of test requests via full-path coverage of this graph; (iii) it derives automatically the expected verdict of a specific request execution by executing the corresponding path in such graph; (iv) it allows us to measure coverage assessment of a given test suite. Our validation of the XACMET prototype implementation confirms the effectiveness of the proposed approach.},
author = {Daoudagh, Said and Lonetti, Francesca and Marchetti, Eda},
doi = {10.1007/s11219-019-09470-5},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Access control,Automated oracle derivation,Request generation,Testing},
month = {mar},
number = {1},
pages = {249--282},
publisher = {Springer},
title = {{XACMET: XACML Testing & Modeling: An automated model-based testing solution for access control systems}},
volume = {28},
year = {2020}
}
@article{Carver2010a,
abstract = {We describe the Modern Multithreading (MM) class library. MM is a class library consisting of thread and synchronization classes that provide significant support for testing and debugging multithreaded programs. The synchronization classes implement commonly used synchronization objects such as semaphores, monitors, and asynchronous and synchronous message passing channels, for programs that run on a single computer or on a distributed system. MM uses controlled executions to provide program tracing and replay and to support a number of implementation-based and specification-based testing techniques, including non-deterministic and deterministic testing and several forms of reachability testing. MM is portable and easy to use, and has been implemented in Java and C++, with C++ versions for the POSIX Pthreads library and for the Windows Win32 API. {\textcopyright} Springer-Verlag 2009.},
author = {Carver, Richard H. and Lei, Yu},
doi = {10.1007/s10009-009-0102-9},
issn = {14332779},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Class library,Concurrent programming,Debugging,Testing},
month = {jan},
number = {1},
pages = {69--88},
title = {{A class library for implementing, testing, and debugging concurrent programs}},
volume = {12},
year = {2010}
}
@article{Zeiss2014,
abstract = {With more than 10 years of maturing through industrial use and standardization, the Testing and Test Control Notation (TTCN-3) has become a widely used technology that many businesses depend upon for ensuring their product quality. With the rising number of supporters and tools, the demand for a means to assess the standards compliance of TTCN-3 tools has increased. In this article, we describe the motivation, approach, methodology, and results of the still ongoing project to develop a standardized conformance test suite for TTCN-3 tools. We discuss the challenges involved in creating such a test suite, the way to deal with imposed resource limitations of the project, and where we think the effort is heading. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
author = {Zeiss, Benjamin and Kovacs, Andras and Pakulin, Nikolay and Stanca-Kaposta, Bogdan},
doi = {10.1007/s10009-013-0285-y},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Compiler testing,Conformance testing,Semantics tests,Syntax tests,TTCN-3},
number = {3},
pages = {285--294},
publisher = {Springer Verlag},
title = {{A conformance test suite for TTCN-3 tools: Black-Box functional testing of TTCN-3 syntax and semantics}},
volume = {16},
year = {2014}
}
@article{Rings2014,
abstract = {Interoperability is a prerequisite to allow users to access systems implemented by different vendors seamlessly. A good baseline to achieve interoperability is the implementation of a common set of standards. However, this is often not sufficient as different implementations of a standard are not necessarily interoperable. Therefore, different implementations of systems need to be assessed for interoperability by applying interoperability testing. In this article, we present a generic framework that enables automated interoperability testing with message checks, which assess the compliance of messages exchanged between systems. The goal of this framework is the provision of a basic functionality of interoperability test entities, the definition of a generic interoperability test environment, and guidelines for the specification of automated interoperability tests. The framework also considers aspects related to interoperability testing including verdicts, automation, and limitations of the system under test. Through the application of the framework, interoperability of systems can be assessed, systems can be validated, and standards can be improved. In addition, we present a systematic development process for automated interoperability tests to formalize the development and specification of an interoperability test system. We also consider aspects and critical issues, which are important for the development of a complete interoperability test system. The framework and the process are language and system technology independent. We present their application in a case study that includes interoperability tests for the Internet Protocol Multimedia Subsystem (IMS) using the Testing and Test Control Notation Version 3 (TTCN-3). {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
author = {Rings, Thomas and Poglitsch, Patrick and Schulz, Stephan and Serazio, Luca and Vassiliou-Gioles, Theofanis},
doi = {10.1007/s10009-013-0281-2},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Automation,ETSI,IMS,Interoperability,Standardization,Testing},
number = {3},
pages = {295--313},
publisher = {Springer Verlag},
title = {{A generic interoperability testing framework and a systematic development process for automated interoperability testing}},
volume = {16},
year = {2014}
}
@article{Mohanty201260,
abstract = {Modular approach to application software development has become significantly popular among the enterprises. However, Service-Oriented Architecture (SOA)-based applications represent the majority of modular application software. The major concerns of developers of these applications are oriented around the reliability and fault-free implementation, which necessitate proper testing of the application. Testing of SOA-based application plays a critical role in ensuring a successful deployment of applications. Testing strategies of such applications like unit testing, integration testing and system testing may somehow resemble to that of traditional application software. However, regression testing of SOA-based applications, which can be conducted during the maintenance phase, may present several challenges. To begin with, in this research paper, we attempt to explore a road map to regression testing of SOA-based applications. {\textcopyright} 2005 - 2012 JATIT & LLS.},
annote = {cited By 2},
author = {Mohanty, Rajani Kanta and Pattanayak, Binod Kumar and Puthal, Bhagabat and Mohapatra, Durga Prasad},
issn = {18173195},
journal = {Journal of Theoretical and Applied Information Technology},
keywords = {Regression testing,SOA,Service,Software,Testing},
number = {1},
pages = {60--65},
title = {{A road map to regression testing of serviceoriented architecture (SOA) based applications}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857181886&partnerID=40&md5=aa6e1265580e81710b350b17b106db68},
volume = {36},
year = {2012}
}
@article{Laranjeiro2012,
abstract = {The use of Web services in enterprise applications is quickly increasing. In a Web services environment, providers supply a set of services for consumers. However, although Web services are being used in business-critical environments, there are no practical means to test or compare their robustness to invalid and malicious inputs. In fact, client applications are typically developed with the assumption that the services being used are robust, which is not always the case. Robustness failures in such environments are particularly dangerous, as they may originate vulnerabilities that can be maliciously exploited, with severe consequences for the systems under attack. This paper addresses the problem of robustness testing in Web services environments. The proposed approach is based on a set of robustness tests (including both malicious and non-malicious invalid call parameters) that is used to discover programming and design errors. This approach, useful for both service providers and consumers, is demonstrated by two sets of experiments, showing, respectively, the use of Web services Robustness testing from the consumer and the provider points of view. The experiments comprise the robustness testing of 1,204 Web service operations publicly available in the Internet and of 29 home-implemented services, including two different implementations of the Web services specified by the standard TPC-App performance benchmark. Results show that many Web services are deployed with critical robustness problems and that robustness testing is an effective approach to improve services quality. {\textcopyright} 2012 The Brazilian Computer Society.},
author = {Laranjeiro, Nuno and Vieira, Marco and Madeira, Henrique},
doi = {10.1007/s13174-012-0062-2},
issn = {18674828},
journal = {Journal of Internet Services and Applications},
keywords = {Benchmarking,Reliability and robustness,Testing,Web services},
month = {sep},
number = {2},
pages = {215--232},
title = {{A robustness testing approach for SOAP Web services}},
volume = {3},
year = {2012}
}
@article{Bernardi2018,
abstract = {Software performance engineering is a mature field that offers methods to assess system performance. Process mining is a promising research field applied to gain insight on system processes. The interplay of these two fields opens promising applications in the industry. In this work, we report our experience applying a methodology, based on process mining techniques, for the performance assessment of a commercial data-intensive software application. The methodology has successfully assessed the scalability of future versions of this system. Moreover, it has identified bottlenecks components and replication needs for fulfilling business rules. The system, an integrated port operations management system, has been developed by Prodevelop, a medium-sized software enterprise with high expertise in geospatial technologies. The performance assessment has been carried out by a team composed by practitioners and researchers. Finally, the paper offers a deep discussion on the lessons learned during the experience, that will be useful for practitioners to adopt the methodology and for researcher to find new routes.},
author = {Bernardi, Simona and Dom{\'{i}}nguez, Juan L. and G{\'{o}}mez, Abel and Joubert, Christophe and Merseguer, Jos{\'{e}} and Perez-Palacin, Diego and Requeno, Jos{\'{e}} I. and Romeu, Alberto},
doi = {10.1007/s10664-018-9606-9},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Complex event processing,Process mining,Software performance,Stochastic petri net,Unified modeling language},
month = {dec},
number = {6},
pages = {3394--3441},
publisher = {Springer New York LLC},
title = {{A systematic approach for performance assessment using process mining: An industrial experience report}},
volume = {23},
year = {2018}
}
@article{PAL20191091,
abstract = {Industry 4.0 aims at highly flexible and digitized model of industrial production. This requires vertical integration of different operations in the manufacturing process to promote reconfigurability and better collaboration between distributed components (Modularity & Integrability) and to handle growing complexity in manufacturing systems. On the other hand, the flexibility of reconfiguration leads to new challenges for the testing process (i.e Diagnosability). It has now become a priority to quality assurance team to ensure that reconfiguration did not introduce any new faults, often referred to as Regression Testing. In this paper, we present a testing framework for real-time distributed manufacturing systems to verify reconfiguration correctness and to generate tests for its implementation testing. We present a systematic modeling refinement approach where several refinements (reconfigurations) can be modelled separately from an abstract model and composed with it. We use the reconfiguration model derived by refinements to demonstrate distributed regression testing by partitioning it into a set of distributed communicating testers making model based testing for manufacturing automation systems scalable.},
annote = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
author = {Pal, Deepak and Vain, J{\"{u}}ri},
doi = {10.1016/j.ifacol.2019.11.341},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Distributed Systems,Model Based Testing,Reconfigurable Manufacturing System,Regression Testing},
number = {13},
pages = {1091--1096},
title = {{A systematic approach on modeling refinement and regression testing of real-time distributed systems}},
url = {http://www.sciencedirect.com/science/article/pii/S2405896319313199},
volume = {52},
year = {2019}
}
@article{Shafique2015,
abstract = {Model-based testing (MBT) is about testing a software system using a model of its behaviour. To benefit fully from MBT, automation support is required. The goal of this systematic review is determining the current state of the art of prominent MBT tool support where we focus on tools that rely on state-based models. We automatically searched different source of information including digital libraries and mailing lists dedicated to the topic. Precisely defined criteria are used to compare selected tools and comprise support for test adequacy and coverage criteria, level of automation for various testing activities and support for the construction of test scaffolding. Simple adequacy criteria are supported but not advanced ones; data(-flow) criteria are seldom supported; support for creating test scaffolding varies a great deal. The results of this review should be of interest to a wide range of stakeholders: software companies interested in selecting the most appropriate MBT tool for their needs; organizations willing to invest into creating MBT tool support; researchers interested in setting research directions.},
author = {Shafique, Muhammad and Labiche, Yvan},
doi = {10.1007/s10009-013-0291-0},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Comparison,Criteria,Model-based testing,State-based testing,Systematic review},
number = {1},
pages = {59--76},
publisher = {Springer Verlag},
title = {{A systematic review of state-based test tools}},
volume = {17},
year = {2015}
}
@article{10.1145/3331447,
abstract = {A systematic literature review is presented that surveyed the topic of cloud testing over the period 2012–2017. Cloud testing can refer either to testing cloud-based systems (testing of the cloud) or to leveraging the cloud for testing purposes (testing in the cloud): both approaches (and their combination into testing of the cloud in the cloud) have drawn research interest. An extensive paper search was conducted by both automated query of popular digital libraries and snowballing, which resulted in the final selection of 147 primary studies. Along the survey, a framework has been incrementally derived that classifies cloud testing research among six main areas and their topics. The article includes a detailed analysis of the selected primary studies to identify trends and gaps, as well as an extensive report of the state-of-the-art as it emerges by answering the identified Research Questions. We find that cloud testing is an active research field, although not all topics have received enough attention and conclude by presenting the most relevant open research challenges for each area of the classification framework.},
address = {New York, NY, USA},
author = {Bertolino, Antonia and {De Angelis}, Guglielmo and Gallego, Micael and Garc{\'{i}}a, Boni and Gort{\'{a}}zar, Francisco and Lonetti, Francesca and Marchetti, Eda},
doi = {10.1145/3331447},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Cloud computing,Systematic literature review,Testing},
number = {5},
publisher = {Association for Computing Machinery},
title = {{A systematic review on cloud testing}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/3331447},
volume = {52},
year = {2019}
}
@article{AZZOUZI201523,
abstract = {The development of distributed testing frameworks is more complex, where the implementation process must consider the mechanisms and functions required to support interaction as long as the communication and the coordination between distributed testing components. The typical reactions of such systems are the generation of errors'set: time outs, locks, observability, controllability and synchronization problems. In other side, the distributed testing process must not only check if the output events have been observed, but also the dates when these events have been occurred. In this paper, we show how to cope with these problems by using a distributed testing method including timing constraints. Afterwards, a multi-agent architecture is proposed in the design process to describe the behavior of testing a distributed chat group application on high level of abstraction.},
author = {Azzouzi, Salma and Benattou, Mohammed and Charaf, My El Hassan},
doi = {10.1016/j.csi.2015.01.003},
issn = {09205489},
journal = {Computer Standards and Interfaces},
keywords = {Distributed testing,Multi-agent systems,Timing constraints},
pages = {23--33},
title = {{A temporal agent based approach for testing open distributed systems}},
url = {http://www.sciencedirect.com/science/article/pii/S092054891500015X},
volume = {40},
year = {2015}
}
@article{10.1145/2460383.2460384,
abstract = {The Service-Oriented Architecture (SOA) paradigm is giving rise to a new generation of applications built by dynamically composing loosely coupled autonomous services. Clients (i.e., software agents acting on behalf of human users or service providers) implementing such complex applications typically search and integrate services on the basis of their functional requirements and of their trust in the service suppliers. A major issue in this scenario relates to the definition of an assurance technique allowing clients to select services on the basis of their nonfunctional requirements and increasing their confidence that the selected services will satisfy such requirements. In this article, we first present an assurance solution that focuses on security and supports a test-based security certification scheme for Web services. The certification scheme is driven by the security properties to be certified and relies upon a formal definition of the service model. The evidence supporting a certified property is computed using a model-based testing approach that, starting from the service model, automatically generates the test cases to be used in the service certification. We also define a set of indexes and metrics that evaluate the assurance level and the quality of the certification process. Finally, we present our evaluation toolkit and experimental results obtained applying our certificationsolution to a financial service implementing the Interactive Financial eXchange (IFX) standard. {\textcopyright} 2013 ACM.},
address = {New York, NY, USA},
author = {Anisetti, Marco and Ardagna, Claudio A. and Damiani, Ernesto and Saonara, Francesco},
doi = {10.1145/2460383.2460384},
issn = {15591131},
journal = {ACM Transactions on the Web},
keywords = {Model-based testing,Security certification,Service-Oriented Architecture,Symbolic transition systems,Web services},
month = {may},
number = {2},
publisher = {Association for Computing Machinery},
title = {{A test-based security certification scheme for Web services}},
url = {https://doi-org.ezproxy.utp.edu.co/10.1145/2460383.2460384},
volume = {7},
year = {2013}
}
@article{Faria2016,
abstract = {Novel techniques and a toolset are presented for automatically testing the conformance of software implementations against partial behavioral models constituted by a set of parameterized UML sequence diagrams, describing both external interactions with users or client applications and internal interactions between objects in the system. Test code is automatically generated from the sequence diagrams and executed on the implementation under test, and test results and coverage information are presented back visually in the model. A runtime test library handles internal interaction checking, test stubs, and user interaction testing, taking advantage of aspect-oriented programming techniques. Incremental conformance checking is achieved by first translating sequence diagrams to Extended Petri Nets that combine the characteristics of Colored Petri Nets and Event-Driven Petri Nets.},
author = {Faria, Jo{\~{a}}o Pascoal and Paiva, Ana C.R.},
doi = {10.1007/s10009-014-0354-x},
issn = {14332787},
journal = {International Journal on Software Tools for Technology Transfer},
keywords = {Conformance testing,Petri nets,Sequence diagrams},
month = {jun},
number = {3},
pages = {285--304},
publisher = {Springer Verlag},
title = {{A toolset for conformance testing against UML sequence diagrams based on event-driven colored Petri nets}},
volume = {18},
year = {2016}
}
@article{Ipate2016,
abstract = {One of the great benefits of using a stream X-machine to specify a system is its associated testing method. Under certain design for test conditions, this method produces a test suite that can determine the correctness of the implementation under test (IUT), provided that the basic components of the stream X-machine model have been correctly implemented. However, such an approach implies that each component can be tested in isolation from the rest of the system. This is a limitation that, in practice, can be resolved by developing stubs and drivers. However, this adds complexity to the testing process and, furthermore, these new pieces of software can introduce faults that can invalidate the theoretical results of the aforementioned testing method. This paper extends the approach by allowing component testing to be performed in parallel with integration testing, while still guaranteeing the IUT correctness under the given design for test conditions. It also shows how the integration test suite, produced in previous publications, can be reduced.},
author = {Ipate, Florentin and Dranidis, Dimitris},
doi = {10.1007/s00165-015-0345-z},
issn = {1433299X},
journal = {Formal Aspects of Computing},
keywords = {Extended finite state machines,Formal specifications,Model based testing,Stream X-machines,Test generation},
month = {mar},
number = {1},
publisher = {Springer-Verlag London Ltd},
title = {{A unified integration and component testing approach from deterministic stream X-machine specifications}},
volume = {28},
year = {2016}
}
